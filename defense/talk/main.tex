\documentclass{beamer}
%%%%%% UNOFFICIAL ICL BEAMER TEMPLATE V.0.1 %%%%%%
% This is a basic LaTeX Beamer template that I customised to have the logo of ICL and a background picture. Mind that this is NOT an official ICL template but it may still be useful for informal presentations.
% The official ICL graphical identity resources can be found here: http://www3.imperial.ac.uk/graphicidentity
% Please drop me an e-mail or comment via Twitter @AJunyentFerre if you found this was useful or have any suggestion to improve it.
%%%%%%

\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{xmpmulti}
\usepackage{amsmath}
\usepackage[mathscr]{eucal}
%\usepackage{eucal}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{pgfplots}
\usepackage{highlight}
\usepackage{collcell}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{xstring}
\usepackage{appendixnumberbeamer}
\usepackage[absolute,overlay]{textpos}

\usepgfplotslibrary{groupplots}
\usetikzlibrary{pgfplots.groupplots,arrows.meta,shadows,positioning,angles,quotes}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning}

\usepgfplotslibrary{fillbetween}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}
\renewcommand{\vec}[1]{\mathbf{#1}
}
\newcommand{\colorpos}{green}
\newcommand{\colorneg}{red}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\DeclareMathOperator*{\argmax}{arg\,max}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\definecolor{webgreen}{rgb}{0,.5,0}
\definecolor{webbrown}{rgb}{.6,0,0}
\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{Black}{cmyk}{0, 0, 0, 0}
\definecolor{skymagenta}{rgb}{0.81, 0.44, 0.69}

%%%%%% THE FOLLOWING FILE CONTAINS THE STYLE DEFINITIONS %%%%%%
\input{header.tex}
%%%%%%

%%%%%% TITLE, AUTHOR, DATE DEFINITIONS %%%%%%
\title{Contributions to Deep Transfer Learning}
\subtitle{From Supervised to Reinforcement Learning}
\author{\textbf{Matthia Sabatelli} \\ Montefiore Institute, Department of Electrical Engineering and Computer Science, Universit\'e de Li\`ege, Belgium}


\date{March 30th 2022}
%%%%%%

\setbeamertemplate{footline}[frame number]{}


\begin{document}

\frame{\titlepage} 

%\frame{\frametitle{Presentation outline}\tableofcontents}

%============================================================================

\begin{frame}
	\begin{center}
	\textcolor{skymagenta}{\textbf{PART I}}
	\end{center}
\end{frame}


\begin{frame}
	\section{Part I: Preliminaries}
	
	\tableofcontents[sectionstyle=show/hide,subsectionstyle=show/show/hide]
	
	\subsection{Transfer Learning}

\end{frame}

\begin{frame}{Transfer Learning}
	%\begin{textblock*}{5cm}(7cm,3cm) % {block width} (coords) 
   %Your text here
%\end{textblock*}
	
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/example}
	\end{figure}

\end{frame}


\iffalse
\begin{frame}{Transfer Learning}
	


	\begin{columns}
		\begin{column}{.5\textwidth}
			On the one hand we have the concept of \textcolor{RoyalBlue}{\textbf{source}} $S$, which is needed to build some prior knowledge available for transfer
		
		\bigskip
		\begin{itemize}
			\item	Source Domain $\mathcal{D}_S$
			\item	Source Task $\mathcal{T}_S$
		\end{itemize}

		\end{column}
		
		\begin{column}{.5\textwidth}
			While on the other hand we have the concept of \textcolor{RoyalBlue}{\textbf{target}} $T$, defining the machine learning problem we would like to solve
		
		\bigskip
		\begin{itemize}
			\item Target Domain $\mathcal{D}_T$
			\item Target Task $\mathcal{T}_T$
		\end{itemize}


		\end{column}
		
	\end{columns}



\end{frame}
\fi


\begin{frame}{Transfer Learning}
	
	\bigskip
	Transfer Learning in practice ...
	\bigskip
	\input{./Chapter03/tex/tl_examples_1.tex}

\end{frame}


\begin{frame}{Transfer Learning}
	
	\bigskip
	Transfer Learning in practice ...
	\bigskip
	\input{./Chapter03/tex/tl_examples_2.tex}

\end{frame}

\begin{frame}{Transfer Learning}

	The family of machine learning models that is studied under the lens of transfer learning is that of \textcolor{RoyalBlue}{Convolutional Neural Networks}
	\bigskip
	\begin{itemize}
		\item One of the main architectures when it comes to high-dimensional and spatially organized inputs
		\item Have achieved state-of-the-art results across many different machine learning problems
		\item We focus our analysis on Supervised Learning and Reinforcement Learning
	\end{itemize}


\end{frame}

\begin{frame}{Transfer Learning}
	
	When it comes to Supervised Learning ...
	\bigskip	
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/dogo}
	\end{figure}


\end{frame}

\begin{frame}{Transfer Learning}

	When it comes to Reinforcement Learning ...
	\bigskip

	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/dqn}
		\caption{Image courtesy of Patel et al. (2019)}
	\end{figure}

\end{frame}



\begin{frame}{Transfer Learning}

	Why is Transfer Learning so interesting?

	\begin{itemize}
		\item It allows us to train deep learning models when there is a \textcolor{Maroon}{lack} of training data
		\item Is useful when computational resources are \textcolor{Maroon}{scarce}
		\item Results in \textcolor{RoyalBlue}{better performing} models
		\item Provides a unique tool for \textcolor{RoyalBlue}{better understanding} neural networks  
		\item Having models that generalize and transfer across different domains and tasks is \textcolor{RoyalBlue}{key} for the development of Artificial Intelligence 
	\end{itemize}


\end{frame}

%============================================================================


\begin{frame}
	\begin{center}
		\textcolor{skymagenta}{\textbf{PART II}}
	\end{center}
\end{frame}


\begin{frame}
	\section{Part II: Supervised Learning}
		\tableofcontents[sectionstyle=show/hide,subsectionstyle=show/show/hide]		
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/insight}
	\end{figure}
\end{frame}


\begin{frame}
	\subsection{On the Transferability of Convolutional Neural Networks}

	We start by defining the components of \textcolor{RoyalBlue}{Supervised Learning}:

	\begin{itemize}
		\item An input space $\mathcal{X}$,
		\item An output space $\mathcal{Y}$,
		\item A joint probability distribution $P(X,Y)$
		\item A loss function $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathds{R}$
	\end{itemize}

	\bigskip

	The \textcolor{RoyalBlue}{goal} is to find a function $f:\mathcal{X}\rightarrow\mathcal{Y}$ that minimizes the expected risk:

	\begin{equation*}
		R(f) = \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \big[\ell(y,f(\vec{x}))\big]
	\end{equation*}
\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip

	\begin{figure}
		\includegraphics[width=1.2\textwidth]{figures/1}
	\end{figure}
	 
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip

	\begin{figure}
		\includegraphics[width=1.2\textwidth]{figures/2}
	\end{figure}
	 
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip

	\begin{figure}
		\includegraphics[width=1.2\textwidth]{figures/3}
	\end{figure}
	 
\end{frame}


\iffalse
\begin{frame}{On the Transferability of Convolutional Neural Networks}

	For our first study ...
	\bigskip
	\begin{itemize}
		\item We consider $f$ to come in the form of a \textcolor{RoyalBlue}{Convolutional Neural Network} (CNN)
		\item We define the source domain $\mathcal{D}_S$ to be that of \textcolor{RoyalBlue}{natural images}
		\item We define the target domain $\mathcal{D}_T$ to be that of \textcolor{RoyalBlue}{Digital Heritage}
		\item We assume that labels are available in both the source and target data and that the input spaces ${\cal X}_T$ and ${\cal X}_S$ match: \textcolor{RoyalBlue}{Inductive Transfer Learning}
	\end{itemize}

\end{frame}
\fi

\begin{frame}{On the Transferability of Convolutional Neural Networks}

	\begin{columns}
		\begin{column}{.5\textwidth}
			As source-task $\mathcal{T}_S$ we have a CNN pre-trained on the ImageNet dataset

			\begin{figure}
				\includegraphics[width=0.8\textwidth]{figures/imagenet}
			\end{figure}

		\end{column}
		
		\begin{column}{.5\textwidth}
			We have three target-tasks $\mathcal{T}_T$ defined by the Rijksmuseum collection

			\begin{figure}
				\includegraphics[width=0.85\textwidth]{figures/rijks}
			\end{figure}

		\end{column}
		
	\end{columns}

\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}

	More specifically we aim to classify the artworks of the Rijksmuseum by:
	\bigskip

	\begin{columns}
		\begin{column}{.3\textwidth}
			\textcolor{RoyalBlue}{Material}: $\mathcal{T}_T$ \circled{1}
			\begin{figure}
				\includegraphics[width=0.9\textwidth]{figures/paper}
			\end{figure}
		\end{column}
		
		\begin{column}{.3\textwidth}
			\textcolor{RoyalBlue}{Type}: $\mathcal{T}_T$ \circled{2}
			\begin{figure}
				\includegraphics[width=1\textwidth]{figures/portrait}
			\end{figure}
		\end{column}
			
		\begin{column}{.3\textwidth}
			\textcolor{RoyalBlue}{Artist}: $\mathcal{T}_T$ \circled{3}
			\begin{figure}
				\includegraphics[width=1\textwidth]{figures/rembrandt}
			\end{figure}
		\end{column}
		
	\end{columns}

\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}

	\input{./Tables/Chapter04/table_1.tex}
	\bigskip
	$\Rightarrow$ Experimental setup ...
	\begin{itemize}
		\item We compare the transfer learning performance of four CNN architectures 
		\item Explore three training strategies
	\end{itemize}
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	
	\bigskip

	\begin{itemize}
		\item Let us consider a simplified neural network
	\end{itemize}

	\bigskip

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/mlp.pdf}
	\end{figure}

\end{frame}


\iffalse
\begin{frame}{On the Transferability of Convolutional Neural Networks}
	
	\bigskip

	\begin{itemize}
		\item A \textcolor{RoyalBlue}{randomly} initialized model
	\end{itemize}

	\bigskip

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/random_net.pdf}
	\end{figure}

\end{frame}
\fi


\begin{frame}{On the Transferability of Convolutional Neural Networks}
	
	\bigskip

	\begin{itemize}
		\item The \textcolor{RoyalBlue}{off-the-shelf} approach 
	\end{itemize}

	\bigskip

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/frozen_net.pdf}
	\end{figure}

\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}
	
	\bigskip

	\begin{itemize}
		\item The \textcolor{RoyalBlue}{fine-tuning} approach 
	\end{itemize}

	\bigskip

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/fine_tuning_net.pdf}
	\end{figure}

\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip
	The performance on $\mathcal{T}_T$ \circled{1}
	
	\input{./Chapter04/tex/results_1.tex}
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip
	The performance on $\mathcal{T}_T$ \circled{1}
	
	\input{./Chapter04/tex/results_1_1.tex}
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip
	The performance on $\mathcal{T}_T$ \circled{1}
	
	\input{./Chapter04/tex/results_1_2.tex}
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip
	The performance on $\mathcal{T}_T$ \circled{2} and $\mathcal{T}_T$ \circled{3}

	\input{./Chapter04/tex/results_2.tex}
\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}

	We observe that:
	\bigskip
	
	\begin{itemize}
		\item Both transfer learning approaches have significant \textcolor{RoyalBlue}{benefits} 
		\item The off-the-shelf approach performs \textcolor{RoyalBlue}{well} for  $\mathcal{T}_T$ \circled{1} and  $\mathcal{T}_T$ \circled{2} but \textcolor{Maroon}{fails} for  $\mathcal{T}_T$ \circled{3}
		\item Fine-tuning the networks results in \textcolor{RoyalBlue}{best} performance
	\end{itemize}

\end{frame}

\iffalse
\begin{frame}{On the Transferability of Convolutional Neural Networks}
	
	$\Rightarrow$ After having fine-tuned the ImageNet models on the Rijksmuseum collection we investigate whether these models \textcolor{RoyalBlue}{generalize} to different, smaller, artistic collections

	\bigskip

	\begin{itemize}
		\item We collected a new dataset of heritage objects present in the city of Antwerp
		\item Aim again at classifying their type $\mathcal{T}_T$ \circled{2} 
		\item And the respective artist $\mathcal{T}_T$ \circled{3}
		\item Compare the performance to ImageNet pre-trained models only
	\end{itemize}
	
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}

	\input{./Tables/Chapter04/table_3.tex}

\end{frame}
\fi

\begin{frame}{On the Transferability of Convolutional Neural Networks}

	\bigskip

	To conclude we show that
	
	\bigskip

	\begin{itemize}
		\item Modern Convolutional Neural Networks exhibit \textcolor{RoyalBlue}{strong transfer learning} potential
		\item Even when the source task $\mathcal{T}_S$ (natural images) and the target task are very different $\mathcal{T}_T$ (art classification)
		\item Best performance is obtained after fine-tuning, which however comes at a computational cost
	\end{itemize}

	\bigskip

	The present study only considers the computer vision task of \textcolor{RoyalBlue}{image classification} ...

\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip
	$\Rightarrow$ Which is a \textcolor{Maroon}{limitation} which we have addressed in our follow up work, where we perform a similar study for the computer vision task of \textcolor{RoyalBlue}{object detection}

	\bigskip
	
	For this purpose we have created the MINERVA dataset within the open-source \texttt{Cytomine} platform \footnote{Marée, Raphaël, et al. "Collaborative analysis of multi-gigapixel imaging data using Cytomine." Bioinformatics 32.9 (2016): 1395-1401.}

	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/cytomine_annotations}
	\end{figure}

\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\bigskip
	\input{./Tables/Chapter05/hypernym_distribution.tex}
\end{frame}

\begin{frame}{On the Transferability of Convolutional Neural Networks}
	\begin{figure}
		\includegraphics[width=1\textwidth]{figures/minerva}
	\end{figure}
\end{frame}


\begin{frame}{On the Transferability of Convolutional Neural Networks}
	
	In line with the classification experiments performed on the Rijksmuseum dataset, also when it comes to object detection we show that
	\bigskip
	\begin{itemize}
		\item Pre-trained object detectors \textcolor{RoyalBlue}{can get transferred} across domains 
		\item Fine-tuning these networks results in \textcolor{RoyalBlue}{best} performance
		\item This is the case for \textcolor{RoyalBlue}{different families} of object detection models ranging from YOLO, to networks that use region proposals and selective search \footnote{Claes, Yann. "Deep Learning for the Classification and Detection of Animals in Artworks." (2021).}.
	\end{itemize}

\end{frame}



\begin{frame}{On the Transferability of Lottery Winners}
	\subsection{On the Transferability of Lottery Winners}
\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}

	\begin{center}
		\textcolor{RoyalBlue}{The Lottery Ticket Hypothesis}:
		\textit{"A randomly-initialized dense neural network contains a subnetwork that is initialized such that when trained in isolation it can match the test accuracy of the original network after training for at most the same number of iterations.\footnote{Frankle, Jonathan, and Michael Carbin. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." (2018)}"}
	\end{center}

\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/mlp.pdf}
	\end{figure}

\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/mlp_ticket_3.pdf}
	\end{figure}

\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/random_ticket_1.pdf}
	\end{figure}

\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/random_ticket_2.pdf}
	\end{figure}

\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}

	$\Rightarrow$ How does one \textcolor{RoyalBlue}{find} a winning ticket?
	\bigskip
	
	\begin{itemize}
		\item Randomly initialize a network $f(x;\theta_0)$ where $\theta_0 \sim \mathscr{D}_{\theta}$
		\item We train the network for $j$ iterations
		\item We prune $p\%$ of the parameters in $\theta_j$ creating a mask $m$
		\item Reset the remaining parameters to their values at $\theta_0$, creating a winning ticket $f(x;m \odot \theta_0)$
	\end{itemize}

\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}
	The Lottery Ticket Hypothesis in practice ...

	\input{./Chapter06/tex/lth_example_1.tex}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	The Lottery Ticket Hypothesis in practice ...

	\input{./Chapter06/tex/lth_example_2.tex}
\end{frame}



\begin{frame}{On the Transferability of Lottery Winners}
	The Lottery Ticket Hypothesis in practice ...

	\input{./Chapter06/tex/lth_example_3.tex}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	Why are lottery tickets $f(x;m \odot \theta_0)$ so \textcolor{RoyalBlue}{special}?
	\begin{itemize}
		\item Train faster
		\item Faster Inference
		\item (Sometimes) obtain a better final performance
	\end{itemize}
	\bigskip
	However ... 

	\begin{itemize}
		\item Identifying a winning ticket is \textcolor{Maroon}{computationally expensive} 
		\item We \textcolor{Maroon}{do not} know how and why lottery winners appear throughout learning
	\end{itemize}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	$\Rightarrow$ Therefore we study whether winning tickets $f(x;m \odot \theta_0)$
	found on natural image datasets can get \textcolor{RoyalBlue}{transferred} to the non-natural realm

	\bigskip

	We use three popular Computer Vision datasets as source domains $\mathcal{D}_S$:

	\begin{figure}
 \includegraphics[width=0.8\textwidth]{figures/cifar10}
\end{figure}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	$\Rightarrow$ Therefore we study whether winning tickets $f(x;m \odot \theta_0)$
	found on natural image datasets can get \textcolor{RoyalBlue}{transferred} to the non-natural realm

	\bigskip

	We use three popular Computer Vision datasets as source domains $\mathcal{D}_S$:

	\begin{figure}
 \includegraphics[width=0.8\textwidth]{figures/cifar100}
\end{figure}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}

	$\Rightarrow$ Therefore we study whether winning tickets $f(x;m \odot \theta_0)$
	found on natural image datasets can get \textcolor{RoyalBlue}{transferred} to the non-natural realm

	\bigskip

	We use three popular Computer Vision datasets as source domains $\mathcal{D}_S$

	\begin{figure}
 \includegraphics[width=0.8\textwidth]{figures/fashion_mnist}
\end{figure}
\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}

	And we use \textcolor{RoyalBlue}{seven datasets} of non-natural images as target tasks $\mathcal{T}_T$ coming from the fields of Digital Pathology and Digital Heritage

	\begin{figure}
 		\centering
  		\includegraphics[width=1.8cm,height=\textheight,keepaspectratio]{figures/lba.pdf}%
  		\includegraphics[width=1.8cm,height=\textheight,keepaspectratio]{figures/tissus.pdf}%
  		\includegraphics[width=1.8cm,height=\textheight,keepaspectratio]{figures/mouse_lba.pdf}%
  		\includegraphics[width=1.8cm,height=\textheight,keepaspectratio]{figures/bonemarrow.pdf}%
    		\includegraphics[width=1.8cm,height=\textheight,keepaspectratio]{figures/artist.pdf}%
  		\includegraphics[width=1.8cm,height=\textheight,keepaspectratio]{figures/type.pdf}%
	\end{figure}

	\bigskip
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	\input{./Tables/Chapter06/datasets_overview}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	\begin{tikzpicture}
		\node (table) {\input{./Tables/Chapter06/datasets_overview}};
		\draw [red,ultra thick,rounded corners]
		  ($(table.south west) !.5! (table.north west)$)
		  rectangle 
		  ($(table.south east) !.4! (table.north east)$);
	\end{tikzpicture}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Pathology} data ...
	\input{./Chapter06/tex/tl_study_dp_1.tex}

\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Pathology} data ...
	\input{./Chapter06/tex/tl_study_dp_2.tex}
\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}
		Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Pathology} data ...
\input{./Chapter06/tex/tl_study_dp_3.tex}
\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}
		Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Pathology} data ...
\input{./Chapter06/tex/tl_study_dp_4.tex}
\end{frame}


\begin{frame}{On the Transferability of Lottery Winners}
		Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Pathology} data ...
\input{./Chapter06/tex/tl_study_dp_5.tex}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Pathology} data ...
	\input{./Chapter06/tex/tl_study_dp_6.tex}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	Transferring lottery winners to the \textcolor{RoyalBlue}{Digital Heritage} data ...
	\input{./Chapter06/tex/tl_study_results_dh.tex}
\end{frame}



\begin{frame}{On the Transferability of Lottery Winners}

	Our main findings show that on Digital Pathology data
	\bigskip
	\begin{itemize}
		\item All lottery winners \textcolor{RoyalBlue}{significantly outperfom} unpruned models 
		\item Natural lottery winners contain \textcolor{RoyalBlue}{a generic inductive bias} (to some extent)
		\item Best performance is obtained by identifying a winning ticket directly on the \textcolor{Maroon}{target task} $\mathcal{T}_T$
	\end{itemize}
	\bigskip

	whereas on Digital Heritage data
	\bigskip
	\begin{itemize}
		\item Natural lottery winners transfer \textcolor{RoyalBlue}{much better}
		\item They can even \textcolor{RoyalBlue}{outperform} target task $\mathcal{T}_T$ tickets
	\end{itemize}
\end{frame}

\begin{frame}{On the Transferability of Lottery Winners}
	
	We also provide additional empirical insights into the Lottery Ticket Hypothesis:
	\bigskip
	\begin{itemize}
		\item We show that completely pre-trained winning tickets $f(x;m \odot \theta_i)$ \textcolor{Maroon}{overfit} on the source task $\mathcal{T}_S$ 
		\item The presence of winning tickets \textcolor{Maroon}{does not depend} on the size of the training data
		\item The closer the source task $\mathcal{T}_S$ and the target task $\mathcal{T}_T$ the \textcolor{RoyalBlue}{better} the transferability of $f(x;m \odot \theta_k)$
	\end{itemize}

\end{frame}

%============================================================================

\begin{frame}
	\begin{center}
		\textcolor{skymagenta}{\textbf{PART III}}
	\end{center}
\end{frame}


%============================================================================


\begin{frame}
	\section{Part III: Reinforcement Learning}	
		\tableofcontents[sectionstyle=show/hide,subsectionstyle=show/show/hide]	
	\subsection{A Novel Family of DRL Algorithms}
\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}

	\bigskip
	We now consider a different machine learning paradigm: \textcolor{RoyalBlue}{Reinforcement Learning} (RL), where an agent needs to learn how to interact with its environment

	\begin{figure}[htb!]
		\centering
		\input{../../Images/Chapter02/rl_loop.tex}
 		\label{fig:rl_loop}
	\end{figure}

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	Such interaction is modeled as a \textcolor{RoyalBlue}{Markov Decision Process} (MDP) consisting of the following elements:

\begin{itemize}
	\item A set of possible states $\mathcal{S}$,
	\item A set of possible actions $\mathcal{A}$,
	\item A transition function $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow [0,1]$,
	\item A reward function $\Re:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$,
	\item A discount factor denoted as $\gamma \in [0,1]$.
\end{itemize}

\bigskip

Therefore we can define an MDP as $\mathcal{M}=\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \Re, \gamma\rangle$.

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}	
	\bigskip

	The interaction between the agent and the environment is given by the agent's \textcolor{RoyalBlue}{policy} $\pi$, a probability distribution over $a \in \mathcal{A}(s)$ for each $s \in \mathcal{S}$:

	\begin{equation*}
	\pi(a|s) = \text{Pr}\; \{a_t = a | s_t = s\}, \; \text{for all}\; s \in \mathcal{S}\; \text{and}\; a\ \in \mathcal{A}.
	\end{equation*}

	\bigskip

	The \textcolor{RoyalBlue}{goal} of the agent is to maximize the expected discounted return as:
	\begin{equation*}
		\begin{split}
			G_t & = r_t+\gamma r_{t+1} + \gamma^{2} r_{t+2} + ... \\
	    			& = \sum_{k=0}^{\infty}\gamma^{k} r_{t+k}.
		\end{split}
		\label{eq:discounted_return}
	\end{equation*}

\end{frame}


\begin{frame}{A Novel Family of DRL Algorithms}	
	\bigskip
	In RL some components of the MDP are unknown 
	\begin{center}
		$\mathcal{M}=\langle\mathcal{S}, \mathcal{A}, \textcolor{red}{\mathcal{P}}, \textcolor{red}{\Re}, \gamma\rangle$,
	\end{center}
	
	which in the case of \textcolor{RoyalBlue}{model-free} RL can be overcome by learning either the state-value function

	\begin{align*}
    		V^{\pi}(s)=\mathds{E}\bigg[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k}\bigg| s_t = s, \pi \bigg],
	\end{align*}

	or the state-action value function

	\begin{align*}
     		Q^{\pi}(s,a)=\mathds{E}\bigg[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k} \bigg| s_t = s, a_t=a, \pi\bigg].
	\end{align*}
\end{frame}


\begin{frame}{A Novel Family of DRL Algorithms}
	\begin{itemize}
		\item Model-free algorithms are implemented in a \textcolor{RoyalBlue}{tabular} fashion, meaning that the state values, or state-action values, are stored within tables of sizes $|\mathcal{S}|$ and $|\mathcal{S}\times\mathcal{A}|$ respectively

		\item For many problems we typically seek to learn an \textcolor{RoyalBlue}{approximation} of the value functions
	\end{itemize}
	
	\begin{center}

		$V^{\pi}(s)\approx V^\pi{(s;\theta)}$ and $Q^{\pi}(s,a;\theta)\approx Q(s,a;\theta)$
	\end{center}
	
	\bigskip

	This approximation can be represented by a \textcolor{RoyalBlue}{Convolutional Neural Network}

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	Typical DRL algorithms aim at only learning an approximation of the state-action value function $Q^{\pi}(s,a)\approx Q^{\pi}(s,a;\theta)$

	\begin{itemize}
		\item A policy $\pi$ can only be derived from $Q^{\pi}(s,a)$ 
		\item It is hard to combine RL algorithms with neural networks
		\item Dealing with one value function can be complicated enough
	\end{itemize}

	\bigskip

	However ...

	\begin{itemize}
		\item Training can be very slow 
		\item Algorithms are prone to diverge
		\item Do not correctly estimate $Q^{\pi}(s,a)$
	\end{itemize}

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}	
	\bigskip

	$\Rightarrow$ Therefore we suggest to \textcolor{RoyalBlue}{jointly approximate} the state-value function $V^{\pi}(s;\phi)$ alongside the state-action value function $Q^{\pi}(s,a;\theta)$

	\bigskip

	We can do this by either training a \textcolor{skymagenta}{\textbf{DQV}} agent which learns $V^{\pi}(s;\phi)$ by minimizing
		\begin{multline*}
L(\phi) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \phi^{-}) - V(s_{t}; \phi)\big)^{2}\bigg],
		\end{multline*}			
		and $Q^{\pi}(s,a)$ with:
		\begin{multline*}
    			L(\theta) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \phi^{-}) - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg],
		\end{multline*}

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	... or by training a \textcolor{skymagenta}{\textbf{DQV-Max}} agent that learns $V^{\pi}(s;\phi)$ with
	\begin{multline*}
		L(\phi) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}) - V(s_{t}; \phi)\big)^{2}\bigg],
	\end{multline*}
	and $Q^{\pi}(s,a)$ with:
	\begin{multline*}
    		L(\theta) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \phi) - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg].
	\end{multline*}
\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	\bigskip

	$\Rightarrow$ We compare their performance to algorithms which only learn an approximation of the state-action value function $Q^\pi(s,a)$ \footnote{Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." nature 518.7540 (2015)} \footnote{Van Hasselt, Hado, Arthur Guez, and David Silver. "Deep reinforcement learning with double q-learning." Proceedings of the AAAI conference on artificial intelligence (2016).}

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	\input{./Chapter07/tex/dqv_family_cnn_convergence_results_1.tex}
\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	\input{./Chapter07/tex/dqv_family_cnn_convergence_results_2.tex}
\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	$\Rightarrow$ Jointly approximating two value functions over one results in \textcolor{RoyalBlue}{faster convergence}, but also in \textcolor{RoyalBlue}{accurate} value estimates:

	\input{./Chapter07/tex/overestimation_comparison.tex}

\end{frame}

\begin{frame}{A Novel Family of DRL Algorithms}
	To conclude DQV-Learning and DQV-Max Learning

	\begin{itemize}
		\item Result in \textcolor{RoyalBlue}{faster learning}
		\item Are \textcolor{RoyalBlue}{robust} to the "Deadly Triad of DRL"
		\item Suffer \textcolor{RoyalBlue}{less} from the overestimation bias of the $Q$ function 
		\item \textcolor{RoyalBlue}{Scale well} to the multi-agent setting \footnote{Leroy, Pascal, et al. "QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning." (2021).}
	\end{itemize}

	However ... 

	\begin{itemize}
		\item Memory wise are \textcolor{Maroon}{twice} as expensive as DQN and DDQN
		\item \textcolor{Maroon}{Do not} always result in better policies
	\end{itemize}

\end{frame}


\begin{frame}{On the Transferability of Deep-Q Networks}
	\subsection{On the Transferability of Deep-Q Networks}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}
	
	Let's take a look at the performance of the DQV algorithms more closely ...
	
	\bigskip
	\input{./Tables/Chapter07/dqv_dqv_max_results.tex}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}
	
	Let's take a look at the performance of the DQV algorithms more closely ...
	
	\bigskip
	\begin{tikzpicture}
\node (table) {\input{./Tables/Chapter07/dqv_dqv_max_results.tex}};
\draw [red,ultra thick,rounded corners]
  ($(table.south west) !.55! (table.north west)$)
  rectangle 
  ($(table.south east) !.5! (table.north east)$);
\end{tikzpicture}
	

\end{frame}


\begin{frame}{On the Transferability of Deep-Q Networks}

	Based on these results we have formulated the following research questions:

	\begin{itemize}
		\item Can we \textcolor{RoyalBlue}{improve} the performance of DQV and DQV-Max Learning through Transfer Learning?
		\item Is parametric transfer \textcolor{RoyalBlue}{as effective} in the model-free Reinforcement Learning setting as it is in the Supervised Learning setting?
	\end{itemize}
	
\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}

	$\Rightarrow$ We study the Transfer Learning potential of several types of Deep-Q Networks
	
	\bigskip
	\begin{itemize}
		\item We start by considering \textcolor{RoyalBlue}{two variants} of Deep-Q Networks: the DDQN and DQV-Learning algorithms
		\item The popular \texttt{Atari} games are used as benchmark
		\item We explore what happens when models that are pre-trained on $\mathcal{M}_S$ get \textcolor{RoyalBlue}{fine-tuned} on $\mathcal{M}_T$
	\end{itemize}

\end{frame}


\begin{frame}{On the Transferability of Deep-Q Networks}

	Some examples of source and target tasks ...

	\begin{figure}[ht]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
	\centering
	\includegraphics[width=\textwidth]{./figures/similar_sources}
	\caption{The visually similar \texttt{Ms Pacman} and \texttt{Bank Heist} games.}
	\label{fig:similar_games}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[b]{0.45\linewidth}
	\centering
	\includegraphics[width=\textwidth]{./figures/dissimilar_sources}
	\caption{The highly different \texttt{Crazy Climber} and \texttt{Pong} games.}
	\label{fig:dissimilar_games}
	\end{minipage}
	\end{figure}
	\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}
	\bigskip
	We quantitatively measure the benefits of transferring and fine-tuning a pre-trained model by computing the \textcolor{RoyalBlue}{area ratio metric} \footnote{Lazaric, Alessandro. "Transfer in reinforcement learning: a framework and a survey." Reinforcement Learning. Springer (2012).} $\mathscr{R}$

	\begin{equation*}
		\mathscr{R} = \frac{\text{area of $\mathcal{M}_S$ $-$ area of $\mathcal{M}_T$}}{\text{area of $\mathcal{M}_T$}}
	\label{eq:area_ratio_metric}
	\end{equation*}

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{figures/BankHeist_Enduro}
	\end{figure}


\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}

	\bigskip

	However, we noticed that adopting Transfer Learning strategies performed \textcolor{Maroon}{surprisingly bad}

	\begin{table}[t!]
	{\input{./Tables/Chapter08/dqv_tl_results.tex}}
	\label{tab:dqv_res}
%\end{table}


%\begin{table}[ht!]
        %	\caption{The results obtained when fine-tuning ten different pre-trained DDQN agents (rows) on nine other Atari games (columns). Similarly to Table \ref{tab:dqv_res} the lower the area ratio score, the redder the color of the cell.} %We can again observe that fine-tuning a pre-trained agent is only beneficial in a very limited number of cases and that the DDQN algorithm results in different transfer learning performance than DQV.}
        ~\\
	{\input{./Tables/Chapter08/ddqn_tl_results.tex}}
	\label{tab:ddqn_res}
\end{table}

\end{frame}


\begin{frame}{On the Transferability of Deep-Q Networks}

	Specifically, on the \texttt{Atari} benchmark we have noticed that:
	\bigskip
	\begin{itemize}
		\item Transferring pre-trained Deep-Q Networks mostly results in \textcolor{Maroon}{negative transfer}, even across similar games \texttt{Bank Heist} $\rightarrow$ \texttt{Ms. Pacman}
		\item Transfer across environments is \textcolor{Maroon}{not symmetric}: \texttt{Bank Heist} $\leftrightarrow$ \texttt{Fishing Derby}
		\item Different algorithms result in \textcolor{Maroon}{different} transfer learning performance
	\end{itemize}

	\bigskip

	$\Rightarrow$ Can we \textcolor{RoyalBlue}{explain} these results any further?

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}
	\bigskip
	We have designed a set of novel \textcolor{RoyalBlue}{control tasks} that could help us better understanding the transfer learning potential of Deep-Q Networks 
	\bigskip
	\begin{figure}[ht]
\begin{minipage}{0.5\textwidth}
	\centering
	\includegraphics[width=5cm]{./figures/catch_games}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
	\centering
	\input{./Chapter08/tex/catch_baselines.tex}
\end{minipage}
\label{fig:catch_baselines}
\end{figure}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}

	\bigskip
	$\Rightarrow$ Even on our newly designed problems, we keep observing \textcolor{Maroon}{very poor} transfer learning performance
	\bigskip
\begin{table}[ht!]
	\centering
	%\caption{The area ratio obtained after fine-tuning a pre-trained DQN agent on the different \texttt{Catch} environments. We can see that no matter which source game is used for pre-training, transfer learning surprisingly never results in positive transfer.}
	\input{./Tables/Chapter08/catch_tl_results.tex}
	\label{tab:catch_tl_area_ratio}
\end{table}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}
	
	\bigskip
	We therefore asked ourselves whether Deep-Q Networks are at least able to \textcolor{RoyalBlue}{self-transfer} ...
	\bigskip
	\begin{itemize}
		\item What happens when the source task $\mathcal{M}_S$ and the target task $\mathcal{M}_T$ are identical?
		\item Are there any differences between using an off-the-shelf approach compared to a fine-tuning strategy?
		\item Let's recall that when transferring a pre-trained model only the last layer responsible for estimating $Q(s,a)$ is randomly initialized
	\end{itemize}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}
	
	We notice that positive self-transfer can get obtained as long as the networks \textcolor{Maroon}{do not get fine-tuned} but are used as feature extractors instead

	\begin{table}[ht!]
	\centering
%	\caption{The area ratio scores obtained after performing self-transfer. We can see that if only the last linear layer is trained, then positive transfer is obtained on all \texttt{Catch} environments, whereas if the network is fine-tuned, positive transfer is (in part) only obtained on \texttt{Catch-v2}.}
	\input{./Tables/Chapter08/catch_self_tl_results.tex}
	\label{tab:self_tl_area_ratio}
	\end{table}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}

	\bigskip
	Our \textcolor{RoyalBlue}{conjecture} is that a Deep-Q Network can only solve a problem if it has carefully found a balance between its feature extractor components and the linear layer that estimates $Q(s,a)$

	\bigskip

	\input{./Chapter08/tex/catch_hybrid_self_tl_1.tex}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}

	\bigskip
	Our \textcolor{RoyalBlue}{conjecture} is that a Deep-Q Network can only solve a problem if it has carefully found a balance between its feature extractor components and the linear layer that estimates $Q(s,a)$

	\bigskip

	\input{./Chapter08/tex/catch_hybrid_self_tl_2.tex}

\end{frame}

\begin{frame}{On the Transferability of Deep-Q Networks}

	\bigskip
	Our \textcolor{RoyalBlue}{conjecture} is that a Deep-Q Network can only solve a problem if it has carefully found a balance between its feature extractor components and the linear layer that estimates $Q(s,a)$

	\bigskip

	\input{./Chapter08/tex/catch_hybrid_self_tl.tex}

\end{frame}


\begin{frame}{On the Transferability of Deep-Q Networks}
	
	Takeaway of PART III ...
	\bigskip
	\begin{itemize}
		\item We show that fine-tuning pre-trained models in a model-free Deep Reinforcement Learning context is \textcolor{Maroon}{particularly challenging} 
		\item There is a clear \textcolor{Maroon}{difference} in terms of performance with the results presented in PART II
		\item We provide some \textcolor{RoyalBlue}{initial intuition} about why negative transfer seems to be happening
		\item This remains an \textcolor{RoyalBlue}{open problem} that we believe deserves attention
	\end{itemize}

\end{frame}


\begin{frame}{Conclusion}

	\bigskip
	\begin{center}
		\textcolor{skymagenta}{\textbf{Future Work}}
	\end{center}

	\bigskip
	\begin{itemize}
		\item A Transfer Learning study of Vision Transformers (ViTs) \footnote{Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).}
		\item Consider alternative training approaches such as meta-learning and multi-task learning 
		\item Focus on deep model-based Reinforcement Learning
	\end{itemize}


\end{frame}

\begin{frame}
	\begin{center}
		\textcolor{skymagenta}{\textbf{The End.}}
	\end{center}
\end{frame}

\begin{frame}{Final References}
	\begin{itemize}
		\fontsize{8pt}{7.2}\selectfont
		\item Sabatelli, Matthia, et al. "Deep transfer learning for art classification problems." Proceedings of the European Conference on Computer Vision (ECCV) Workshops. 2018.
		\item Sabatelli, Matthia, et al. "Advances in Digital Music Iconography: Benchmarking the detection of musical instruments in unrestricted, non-photorealistic images from the artistic domain." Digital Humanities Quarterly 15.1 (2021).
		\item Sabatelli, Matthia, Mike Kestemont, and Pierre Geurts. "On the Transferability of Winning Tickets in Non-natural Image Datasets." VISIGRAPP (5: VISAPP). 2021.
		\item Sabatelli, Matthia, et al. "Deep Quality Value (DQV) Learning." Deep Reinforcement Learning Workshop of the 32nd Conference on Neural Information Processing Systems (2018).
		\item Sabatelli, Matthia, et al. "The deep quality-value family of deep reinforcement learning algorithms." 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020.
		\item Sabatelli, Matthia, and Pierre Geurts. "On The Transferability of Deep-Q Networks." Deep Reinforcement Learning Workshop of the 35th Conference on Neural Information Processing Systems. 2021.
	\end{itemize}

\end{frame}

%============================================================================


\appendix

\begin{frame}{Backup}
	\bigskip
	\textit{"Why not transfer fully pre-trained pruned models $f(x,m\odot\theta_i)$ instead of lottery tickets $f(x,m\odot\theta_k)$?"}
	\bigskip
	\input{./Chapter06/tex/full_pretraining_comparison.tex}

\end{frame}

\begin{frame}{Backup}

	\textit{"What is the difference between the DQV-Learning algorithms and the Dueling Architecture?"} \footnote{Wang, Ziyu, et al. "Dueling network architectures for deep reinforcement learning." International conference on machine learning. PMLR, 2016.}

	\bigskip
	
	There's two main \textcolor{RoyalBlue}{differences} among e.g. DQV-Learning and the Dueling Architecture:
	\begin{enumerate}
		\item The Dueling Architecture does not explicitly learn the state-value function $V^{\pi}(s)$
		\item DQV-Learning requires two independent sets of parameters to perform well $\phi$ and $\theta$
	\end{enumerate}

\end{frame}


\begin{frame}{Backup}

	1) In the Dueling Architecture the state-value estimates and the advantage estimates are computed
	\bigskip
	\begin{multline*}
	Q(s,a;\theta^{(1)},\theta^{(2)},\theta^{(3)}) = V\bigl(s;\theta^{(1)},\theta^{(3)}\bigr) + \\
	\bigl(A(s,a;\theta^{(1)},\theta^{(2)}) - \underset{a_{t+1}\in \mathcal{A}}{\max}\: A(s, a_{t+1};\theta^{(1)},\theta^{(2)}) \bigr).
	\label{eq:dueling}
\end{multline*}
	
	 such that it is easier to train the model with DDQN's TD-target
	\bigskip
	\begin{equation*}
    y^{DDQN}_{t} = r_{t} + \gamma \: Q(s_{t+1}, \underset{a\in \mathcal{A}}{\argmax}\: Q(s_{t+1}, a; \theta); \theta^{-}). 
\end{equation*}

\end{frame}


\begin{frame}{Backup}

	1) But the true state-value function $V^{\pi}(s)$ is never explicitly learned, which \textcolor{RoyalBlue}{is the case} for DQV-Learning and DQV-Max Learning as there's a dedicated objective function for learning $\approx V^{\pi}(s;\phi)$ where we either have
	\bigskip
	\begin{multline*}
			L(\phi) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: V(s_{t+1};\phi^{-}) - V(s_{t}; \phi)\big)^{2}\bigg],
		\end{multline*}

	\bigskip
	or
	\begin{multline*}
		L(\phi) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}) - V(s_{t}; \phi)\big)^{2}\bigg],
	\end{multline*}

\end{frame}

\begin{frame}{Backup}

	2) We have also tried to train a DQV-Learning agent by following the Dueling neural architecture 
	\bigskip
	
	\begin{figure}[ht]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
	\centering
	\includegraphics[width=\textwidth]{./figures/DuelingDQV_shared}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[b]{0.45\linewidth}
	\centering
	\includegraphics[width=\textwidth]{./figures/dueling_DQV}
	\end{minipage}
	\end{figure}
	\end{frame}
		
\end{frame}

\begin{frame}{Backup}

	$\Rightarrow$ But the results were \textcolor{Maroon}{never on par} with regular DQV-Learning	
	\bigskip
	\input{./Chapter07/tex/duelling_dqv_results.tex}
	
\end{frame}


\begin{frame}{Backup}

	\bigskip
	
	\textit{"Is there an alternative way of improving the performance of neural networks other than Transfer Learning?"}

	\bigskip
	
	We have experimented with \textcolor{RoyalBlue}{Multi-Task Learning}, which for supervised learning problems showed promising results 
	
	\bigskip
	
	\input{./Appendix/tex/multi_task.tex}
	 

\end{frame}

\begin{frame}{Backup}
	\textit{"Does Transfer Learning never work when it comes to Deep Reinforcement Learning?"}

	\bigskip
	
	We believe that this is mostly the case for model-free Deep Reinforcement Learning only:
	\begin{itemize}
		\item As CNNs need to act as feature extractors as well as function approximators
		\item In fact, in model-based Reinforcement Learning transferring neural architectures works really well \footnote{Sasso, Remo, Matthia Sabatelli, and Marco A. Wiering. "Fractional transfer learning for deep model-based reinforcement learning." arXiv preprint arXiv:2108.06526 (2021).}
	\end{itemize}

\end{frame}


\begin{frame}{Backup}

	\bigskip
	
	\textit{"Does Transfer Learning never work when it comes to Deep Reinforcement Learning?"}

	\bigskip
		\begin{figure}
			\includegraphics[width=0.7\textwidth]{figures/triple}
			\caption{Results from Sasso, Remo, Matthia Sabatelli, and Marco A. Wiering. "Fractional transfer learning for deep model-based reinforcement learning." arXiv preprint arXiv:2108.06526 (2021)}
		\end{figure}
	

\end{frame}

\end{document}
