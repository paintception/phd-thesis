% Appendix X

\chapter*{Appendix}
\label{ch:appendix}


\section{The Deep Quality-Value Learning Algorithms}

In this section we report the pseudocode of the DQV-Learning and DQV-Max Learning algorithms described in Sec. \ref{sec:dqv_family} of Chapter \ref{ch:dqv_family_of_algorithms}. The DQV-Learning algorithm is also used for the transfer learning studies presented in Sec. \ref{sec:empirical_study} of Chapter \ref{ch:dqn_transfer}. 

In Tables \ref{tab:table_1}, \ref{tab:table_2} and \ref{tab:table_3} we also summarize all the hyper-parameters that we have used during the experiments reported in Chapters \ref{ch:dqv_family_of_algorithms} and \ref{ch:dqn_transfer}, ranging from the pre-processing values to the neural architectures. We have followed the typical experimental setup that is usually reported in the DRL literature \cite{mnih2015human, van2015deep, wang2016dueling, castro2018dopamine, sabatelli2020deep} that trains Deep Q-Networks on the Atari 2600 testbed.

\begin{table}[ht!]
\centering
\caption{Hyper-parameters used in all our experiments that use the DQV and DQV-Max algorithms. All hyper-parameters coincide with the ones used by e.g. DQN and DDQN with the only difference being the epsilon greedy parameter $\epsilon$ that is set to 0.5 instead of 1.0. DQV-Learning is known to converge faster than these algorithms \cite{sabatelli2018deepqv}, since it is based on an on-policy learning algorithm \cite{wiering2005qv}. Therefore it requires to explore less than typical off-policy models.}
\begin{tabular}{c | c |  }
Hyperparameter \\
\hline \hline
Atari Arcade Learning Version  & \texttt{Deterministic-v4} \\
Frame-Skipping & True \\ 
Reward Clipping & $[-1,1]$ \\ 
Epsilon Greedy $\epsilon$ & 0.5 \\
Discount Factor $\gamma$ & 0.99 \\ 
Pre-processing scheme & $84\times84\times4$ \\ 
$Q$-optimizer & RMSprop \\ 
$Q$ Learning rate & 0.00025 \\ 
$V$-optimizer & SGD \\ 
$V$ Learning rate & 0.001 \\
Optimizer $\rho$ & 0.95 \\ 
Optimizer $\epsilon$ & 0.01 \\ 
Memory size $S$ & 1M trajectories 
\end{tabular}
\label{tab:table_1}
\end{table}

\begin{table}[ht!]
\centering
\caption{Architecture used by DQV-Learning for estimating the $Q$ function, the n parameter in the last layer of the network changes with respect to the number of actions that is required by each different Atari game. Please note that this architecture corresponds to a typical Q network that is also used by popular algorithms like DQN \cite{mnih2015human}, DDQN \cite{van2015deep} and Rainbow \cite{hessel2018rainbow}}.
\begin{tabular}{c | c | c | }
Layer & Output Shape & Param \\
\hline \hline 
Conv2D & (None, 20, 20, 32) & 8224 \\     
Conv2D & (None, 9, 9, 64) & 32832 \\ 
Conv2D & (None, 7, 7, 64)  & 36928 \\ 
Flatten & (None, 3136) & 0 \\
Dense & (None, 512) & 1606144 \\
Dense & (None, $n$) & 1539 \\
Activations & ReLU & \\

\end{tabular}
\label{tab:table_2}
\end{table}

\begin{table}[ht!]
\centering
\caption{Architecture used for estimating the $V$ function. This architecture corresponds exactly to the one presented in Table \ref{tab:table_2} with the only difference being the last output layer which in this case is set to one.}
\begin{tabular}{c | c | c | }
Layer & Output Shape & Param \\
\hline \hline 
Conv2D & (None, 20, 20, 32) & 8224 \\     
Conv2D & (None, 9, 9, 64) & 32832 \\ 
Conv2D & (None, 7, 7, 64)  & 36928 \\ 
Flatten & (None, 3136) & 0 \\
Dense & (None, 512) & 1606144 \\
Dense & (None, 1) & 1539 \\
Activations & ReLU & \\

\end{tabular}
\label{tab:table_3}
\end{table}


\input{./Pseudocode/dqv_family.tex}


\section{Reinforcement Learning Upside Down}
In this section we report some preliminary experiments that we have performed in the context of Upside-Down Reinforcement Learning and reflect on the potential that this learning paradigm can offer to the reinforcement learning community. 

Despite many successes, it is clear that the combination of reinforcement learning algorithms and deep neural networks can present some significant limitations. Throughout this dissertation we have mainly focused on the aforementioned Deadly Triad phenomenon and on the poor transfer learning properties of Deep-Q Networks which both can limit the development of robust and general agents. Next to these issues, DRL algorithms are characterized by numerous other problems as well (the discussion of which is out of the scope of this thesis) and that all mainly arise because tabular RL algorithms are combined with function approximators \cite{tsitsiklis1997analysis,van2016deep,anschel2017averaged,fujimoto2018addressing,fujimoto2019benchmarking, kumar2019stabilizing, marklundexact}. Although the DRL community has been able to address most of such issues successfully, we believe that the way optimal control problems are currently being solved by the DRL community might need revision. In fact, we argue that most of the current DRL research focuses on introducing solutions that, albeit valuable, are only able to solve very specific and limited problems to a (sometimes very) small extent. As a result, very few DRL practitioners have been critically questioning the long term value of modern DRL algorithms, and the way RL problems are currently being addressed. An exception to this common trend, however, has recently been introduced by \citet{schmidhuber2019reinforcement} who proposed the idea of "Upside-Down Reinforcement Learning" (UDRL). In UDRL the main goal is to solve typical Markovian control problems via classic supervised learning techniques, and to replace common RL concepts such as value function approximation and policy search, through maximum likelihood estimation techniques. In principle this should be done by learning a mapping from states to actions (see \cite{srivastava2019training} for all the mathematical details), which could overcome the need of learning an optimal value function or stochastic policy. We agree with Schmidhuber's ideas, and support the goal of potentially solving optimal control problems via supervised learning techniques. In fact we strongly believe that DRL in its current form is already reducing reinforcement learning problems to supervised learning problems. As an example let us consider the role of the experience replay memory buffer reviewed in Chapter \ref{ch:reinforcement_learning} and adopted by the DQV and DQV-Max Learning algorithms presented in Chapter \ref{ch:dqv_family_of_algorithms}. We have seen that the role of experience replay is that of storing RL trajectories which can in a later moment be sampled and used for minimizing the objective function of a Deep-Q Network. By taking a critical look at this buffer, we can see that it is in large part equivalent to the datasets that are typically used in supervised learning: it needs to store a large amount of RL trajectories which can then be used for modeling the task of learning a value function as a regression problem where $\mathcal{Y}$ is modeled by the space of all TD-errors stored within the buffer. The idea of constructing a dataset of previous trajectories is very far from the ideal RL setting reviewed at the beginning of Chapter \ref{ch:reinforcement_learning}, where we have indeed seen that a RL agent should be able to learn in an online fashion based on the last trajectory only. We therefore believe that the successess obtained by DRL stem from the fact that RL problems have been modeled as supervised learning ones, which as discussed in Sec. \ref{sec:supervised_learning} are well suited for deep neural networks.

We have experimented with some of the UDRL ideas presented in \cite{schmidhuber2019reinforcement,srivastava2019training} and compared the performance of a preliminary version of an UDRL agent to that of a DQN, DDQN and DQV agent on two different benchmarks: the popular control problem \texttt{Cartpole} and the Atari game \texttt{Pong}. We report the results of these very preliminary experiments in Fig. \ref{fig:upside_down_results}. We can see that on the \texttt{Cartpole} benchmark the UDRL learns significantly better than all three model-free DRL algorithms, but is unable to improve its policy over time at all when it comes to the more complicated \texttt{Pong} game. These results show that UDRL has definitely some potential, although furhter research will be required to investigate whether this type of algorithm can scale to more complicated problems. We believe that if such scaling issues will be solved, then UDRL could become a valid alternative to popular DRL algorithms. More specifically we foresee that in the future UDRL will find a successful application in the following three RL areas: \footnote{Please note that these claims are at the present moment not empirically supported in any way, and are made on top of some intuition that has been built after performing the preliminary experiments presented in Fig. \ref{fig:upside_down_results}.}.

\input{./Results/Chapter09/tex/dqv_mlp_results.tex}

\begin{itemize}
	\item \textcolor{RoyalBlue}{Transfer Learning}: as neural networks will not have to jointly serve as optimal value function approximators and feature extractors, we believe that UDRL agents should in principle be more suitable for transfer learning. In fact such models will not have to go through the training dynamics that we have identified in Chapter \ref{ch:dqn_transfer}, and will in essence be more similar to the type of models that we have successfully transferred throughout the second part of this dissertation (as they will be trained according to supervised learning principles).  
	\item \textcolor{RoyalBlue}{Batch Reinforcement Learning}: a common problem of current DRL algorithms is that they suffer from a phenomenon known as "extrapolation error" \cite{fujimoto2019benchmarking}. When such agents are trained in a batch setting, they fail in improving their policy because the trajectories contained within the experience replay memory buffer have been collected by a different agent. This can pose numerous practical issues as the process of collecting RL trajectories can sometimes be particular expensive, therefore limiting the potential interaction between the agent and its environment. As UDRL technically overcomes the need of learning an optimal value function, it follows that such agents should not suffer from the extrapolation bias and could therefore generalize well in a batch RL set-up.   
	\item \textcolor{RoyalBlue}{Interpretable Reinforcement Learning}: the main training principle of UDRL agents of maximum likelihood estimation opens the door to the use of supervised learning algorithms other than deep neural networks. Deep neural networks are often considered to be black boxes and highly uninterpretable models, which is a quality that when it comes to optimal decision making problems could be desirable \cite{mott2019towards}. However, supervised learning algorithms that are largely more interpretable than neural networks do exist \cite{breiman2001random}, and we therefore foresee that these could be used in combination with UDRL in situations where it is desirable to interpret the decisions taken by an agent.
\end{itemize}
  
