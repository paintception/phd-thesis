%Chapter 7

\chapter{On the Transferability of Deep-Q Networks} % Chapter title
\label{ch:dqn_transfer} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 

\begin{remark}{Outline}
\end{remark}




\section{Motivation}


\section{A large-scale Empirical Study}
\label{sec:empirical_study}

# Mention similarities and dissimilarities among games

\subsection{Experimental Setup}

\subsection{Results}

When analyzing the performance of fine-tuned models, more exciting results have been obtained. Specifically, we have noticed that this approach can result in one out of three possible TL types. We classify them as follows:

\begin{itemize}
	\item\textcolor{RoyalBlue}{Negative-TL}: this is a training scenario in which it is counterproductive to start solving the problem modeled by $\mathcal{M}_t$ with a network that is pre-trained on $\mathcal{M}_s$. The most beneficial strategy instead is to train a randomly initialized network from scratch. We can visually observe how this type of TL presents itself in the first plot of Fig. \ref{fig:types_of_tl}, where we report the TL performance of a model which is pre-trained on \texttt{BankHeist} and is fine-tuned on \texttt{FishingDerby}.
   We can observe that over $\approx 3000$ training episodes, the cumulative reward obtained by the \texttt{BankHeist} model is significantly lower than the one obtained by a randomly initialized model.        
    
    \item\textcolor{RoyalBlue}{Absent-TL}: this training scenario does not present significant benefits, nor particular drawbacks when a pre-trained network is trained on $\mathcal{M}_t$ instead of a randomly initialized one. We report an example of this TL scenario in the central plot of Fig. \ref{fig:types_of_tl} where a \texttt{BankHeist} model gets fine-tuned on the \texttt{Boxing} game. We can easily observe that there are no significant differences between the performance of the two networks. While the final network's performance is not as bad as the one obtained when the previous TL scenario occurs, Absent-TL is still not desirable since there are no practical benefits that can motivate the effort required for pre-training a Deep Q-Network on a source-task.
    
    \item\textcolor{RoyalBlue}{Positive-TL}: this corresponds to the most desirable outcome that can be achieved when a pre-trained network gets fine-tuned on the target-task. As reviewed in Chapter \ref{ch:transfer_learning} TL has three main objectives: `Learning speed Improvements", `Asymptotic Improvements" and `Jumpstart Improvements". We show how the first two of these improvements can appear in DRL in the last plot of Fig. \ref{fig:types_of_tl}, where a model pre-trained on \texttt{BankHeist} can significantly outperform a model that is trained from scratch on the \texttt{Enduro} environment. We can observe a \textit{Learning speed Improvement} since the \texttt{BankHeist} model starts improving its policy already after $\approx 50$ epochs, while at the same time, there also is \textit{Asymptotic Improvement} since the cumulative reward that is obtained by the networks is significantly higher than the one that is obtained by an agent trained from scratch. 
\end{itemize}



\section{Control Experiments}

The results presented in the previous section seem to be questioning the level of transferability of convolutional neural networks that get trained for solving RL tasks. In fact, the training strategy of fine-tuning a pre-trained model on $\mathcal{M}_T$ does not result in the same type of performance gains that we have extensively observed throughout the first part of this dissertation, when pre-trained models were transferred and fine-tuned in a supervised learning context. To better characterize the transfer learning properties of pre-trained convolutional neural networks for DRL, we have designed a set of simple control experiments that allow us to examine their transfer learning behavior in training conditions that are as similar as possible to the ones that have characterized the experimental setup adopted in Sec. \ref{sec:empirical_study}. The main difference, however, is that the RL tasks that will be considered from now on, are much simpler than the ones defined within the ALE environment, and therefore do not require extraordinarily long training times for learning an optimal policy.   

\subsection{The Catch Environments}

To this end we have implemented four different versions of the \texttt{Catch} game, a simple RL task that was first presented by \citet{mnih2014recurrent}, and that has been widely used within the literature for investigating the performance of DRL algorithms in a fast, and computationally less expensive manner than the one usually required by the \tattt{Atari} games \cite{vanjos2018deep, aittahar2020empirical}. In the game of \texttt{Catch}, an agent controls a paddle at the bottom of the environment, represented by a $21 \times 21$ grid, and has to catch a ball falling from top to bottom which can potentially bounce off walls. At each time-step, the agent can choose between three different actions: move the paddle one pixel to the right, move it to the left, or do not perform any of the aforementioned actions, therefore keeping its paddle in the same position in the grid. A RL episode ends either when the agent manages to catch the ball, in which case it receives a reward of $1$, or when it misses the ball, which naturally results in a reward of $0$. Following the design choices presented in \cite{vanjos2017deep}, we model the ball to have vertical speed of $v_y=-1 \: cell/s$ and horizontal speed of $v_x \in \{-2,-1,0,1,2\}$. From now on we will refer to this version of the game as \texttt{Catch-v0}, as it is the most basic and simplest form of the game that will be used throughout our experiments (see Fig. \ref{fig:fig1} for an impression of the game). Next to \texttt{Catch-v0} we have implemented three, slightly different and arguably more complex versions of the game as well. The first of such versions is \texttt{Catch-v1}, visually represented in Fig. \ref{fig:fig2}, where the complexity of the game is increased by reducing the size of the paddle that is controlled by the agent. While for \texttt{Catch-v0} its size is of five pixels, in \texttt{Catch-v1} it is of two pixels, therefore requiring the agent to be more precise if it wants to successfully catch the falling ball. The second alternative version of \texttt{Catch} is \texttt{Catch-v2}. In this case, the dynamics of the game are identical to the ones that define \texttt{Catch-v0}, however, as can be seen in Fig. \ref{fig:fig3} the way the $21 \times 21$ grid is represented changes. While in \texttt{Catch-v0} as well as in \texttt{Catch-v1} the state is represented by a binary grid where all pixels, but the ones representing the paddle and the ball have a value of $0$, in \texttt{Catch-v3} the cells around the paddle and the ball can have a random value between $0$ and $255$. This design choice makes it much harder for a convolutional network to correctly locate and identify the position of the paddle and of the falling ball, and makes \texttt{Catch-v2} the arguably most complex version among the different \texttt{Catch} versions of the game. Lastly we have implemented \texttt{Catch-v3}, a version of \texttt{Catch} which is identical to the one that is modeled by \texttt{Catch-v0} with the main difference that the representation of the state is now mirrored (see Fig. \ref{fig:fig4}), therefore requiring the agent to look at different parts of the grid if it wants to locate the paddle, and understand that the ball is unnaturally moving from the bottom to the top.  

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v0}
\caption{\texttt{Catch-v0}}
\label{fig:fig1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v2}
\caption{\texttt{Catch-v1}}
\label{fig:fig2}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v3}
\caption{\texttt{Catch-v2}}
\label{fig:fig3}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v1}
\caption{\texttt{Catch-v3}}
\label{fig:fig4}
\end{minipage}
\end{figure}



\paragraph{Experimental Setup}
\label{sec:experimental_setup_control}
Given the overall simplicity of the different \texttt{Catch} environments, we now train a DQN agent instead of the arguably more complex DQV and DDQN agents that we considered in Sec. \ref{}. The agent comes in the form a two-hidden layer convolutional neural network which is followed by a fully connected layer of $256$ hidden units preceding the final linear layer responsible for estimating the different $Q(s,a)$ values. The first convolutional layer has $32$ channels whereas the second one has $64$ channels. All layers of the network are activated by a ReLU non-linearity. We use an experience replay memory buffer that is set to contain $400000$ trajectories, and start training the model as soon as $5000$ trajectories have been collected. For exploration purposes, we adopt the popular epsilon-greedy strategy with an initial $\epsilon$ value of $1$ which gets linearly annealed over time to $0.1$. Learning a near optimal policy on the aforementioned \texttt{Catch} environments with this type of DQN agent can require between the $3$ and the $5$ hours of training time. We visualize the learning curves for each of the four different \texttt{Catch} environments in Fig. \ref{fig:catch_baselines}, where for every training epoch (reported on the $x$ axis) we report the average performance that is obtained by an agent that is evaluated on ten different episodes ($y$ axis). Shaded areas around the line plots correspond to $\pm 1$ std. deviation obtained over five different training runs with five different random seeds.    

\paragraph{DQN Performance}
As we can see from the results reported in Fig. \ref{fig:catch_baselines}, the DQN agent described above is able to successfully learn a near optimal policy for all \texttt{Catch} versions. When it comes to \texttt{Catch-v0}, \texttt{Catch-v2} and \texttt{Catch-v3} we can observe that by the end of training the agent is able to catch $\approx 100\%$ of the falling balls, whereas its performance is slightly worse ($\approx 90\%$) when it comes to \texttt{Catch-v1} \footnote{Please note that the performance on \texttt{Catch-v1} can be improved by increasing the complexity of the DQN agent by adding one more convolutional layer. However, as the goal is to transfer the same type of model across \texttt{Catch} games we did not modify the architecture of the DQN agent, at the cost of having a slightly worse performing model.}. We can also observe that among the different \texttt{Catch} versions, \texttt{Catch-v0} and \texttt{Catch-v3} appear to be the easiest ones, as the agent requires significantly less training iterations to converge when compared to \texttt{Catch-v1} and \texttt{Catch-v2}. Furthermore, in line with the explanation presented beforehand, our results also confirm the hypothesis that \texttt{Catch-v2} is the overall most complicated \texttt{Catch} version, as learning requires significantly more, and potentially unstable, training epochs.

\begin{figure}
\centering
\input{./Results/Chapter08/tex/catch_baselines.tex}
\caption{hello}
\label{fig:catch_baselines}
\end{figure}

\subsection{From one Catch to Another}
\label{sec:from_one_catch_to_another}

We now replicate the TL study presented in Sec. \ref{sec:empirical_study} on the aforementioned \texttt{Catch} environments, with the hope of identifying why the process of fine-tuning a pre-trained convolutional neural network in a DRL context, seems to not be as beneficial as it is in the supervised learning one. To this end, our goal is to find at least one pair of \texttt{Catch} environments which results in positive transfer, and to then potentially identify some properties within the different \texttt{Catch} versions that could also hold for the pairs of \texttt{Atari} games which have yielded positive transfer in Tables \ref{} and \ref{}. Specifically, we formulate \textcolor{RoyalBlue}{two hypothesis}, based on which, we expect to experimentally observe positive transfer. First, we foresee that positive transfer will happen for all possible \texttt{Catch} combinations, as in the end the source MDP $\mathcal{M}_S$ and the target MDP $\mathcal{M}_T$ do not significantly differ from each other: in fact, the main task across the different \texttt{Catch} versions remains that of catching a falling ball; the action space is identical; and so is the reward function that always returns a value of $1$ when the agent succeeds in catching the falling ball. This hypothesis is also motivated by the results that we presented throughout the first part of this dissertation, where we showed that the higher the similarity between the source domain $\mathcal{D}_S$ and the target domain $\mathcal{D}_T$, the better the performance of a transferred pre-trained network. Second, in the case the previous hypothesis will not be empirically supported, we expect to at least observe positive transfer when using a model that comes as pre-trained either on \texttt{Catch-v1} or on \texttt{Catch-v2}. In fact, as described above and also shown by the performance reported in Fig. \ref{fig:catch_baselines}, these variants of the game can be considered as the two most complicated versions of the \texttt{Catch} environment. This hypothesis is motivated by numerous claims that stem from transfer learning research performed in computer vision. Indeed, when it comes to supervised learning, one of the main factors that makes a certain source task $\mathcal{T}_S$ good for transferring is its complexity \cite{}, which is usually defined in terms of dataset size and number of classes to classify. We therefore expect the complexity of the source MDP $\mathcal{M}_S$ to play an important role within DRL as well.
Similarly to what we did for DQV and DDQN in Sec. \ref{sec:empirical_study}, we take the four different models which have been trained from scratch on their respective \texttt{Catch} version, randomly re-initialize their last layer (responsible for estimating the different state-action values $Q(s,a)$) and fully fine-tune the pre-trained network on the three remaining \texttt{Catch} environments, following the experimental protocol described in Sec. \ref{sec:experimental_setup_control}. The results of this study are reported in Fig. \ref{fig:catch_transfer}, where in clockwise order we show the performance that is obtained when considering \texttt{Catch-v0}, \texttt{Catch-v1}, \texttt{Catch-v2} and \texttt{Catch-v3} as target MDP $\mathcal{M}_T$. The performance of each transferred network is compared against the performance that is obtained after training a DQN agent from scratch which matches with the results reported in Fig. \ref{fig:catch_baselines}.
Surprisingly we found that fine-tuning a pre-trained DQN agent never resulted in positive transfer learning. This can clearly be seen in all plots represented in Fig. \ref{fig:catch_transfer} and by the results reporting the area ratio metric in Table \ref{tab:}. The only case where starting from a pre-trained network appeared to be at least in part beneficial is represented by the first plot of Fig. \ref{fig:catch_transfer} when \texttt{Catch-v1} is considered as source MDP $\mathcal{M}_S$. In this case we can in fact observe some learning speed improvements within the first $25$ learning epochs. This is not surprising as an agent which is able to catch a ball with a small paddle (as defined by the game \texttt{Catch-v1}) should in principle also be able to do this when the size of its paddle is larger (which is the case for \texttt{Catch-v0}). What is more surprising, however, is that while training progresses we see that the performance of a \texttt{Catch-v1} pre-trained model starts deteriorating and that this model barely converges to the same performance that is obtained by a model trained from scratch. When it comes to all other $\mathcal{M}_S-\mathcal{M}_T$ pairs we see that pre-trained networks always perform significantly worse than randomly initialized models trained from scratch, with some extreme cases, as the ones reported for the pair represented in the last plot of Fig. \ref{}, where a pre-trained agent is barely able to improve its policy over time at all. These results invalidate our two hypotheses mentioned above as they clearly show that positive transfer in DRL does not arise when $\mathcal{M}_S$ and $\mathcal{M}_T$ are similar, nor when $\mathcal{M}_S$ is more complex than $\mathcal{M}_T$. 

\input{./Results/Chapter08/tex/catch_tl_experiments.tex}


\subsection{Self-Transfer}

The surprisingly poor transfer learning performance observed in the previous experiment made us question the level of transferability of Deep-Q Networks even more. To further characterize their TL properties we decided to investigate whether pre-trained DRL agents are at least able to transfer to themselves. To this end we studied what happens when a DQN agent gets transferred to a version of \texttt{Catch} that matches with the version of the game that was also used during the pre-training stage. This experiment is in large part identical to the one presented in Sec. \ref{sec:from_one_catch_to_another}, with the only difference being that now $\mathcal{M}_S = \mathcal{M}_T$. Moreover, differently from the previous study, we now also investigate what happens if instead of fine-tuning the network completely we just use the pre-trained DQN agent as a simple feature extractor, therefore only training its last layer responsible for estimating the different state-action values. Our results are presented in Fig. \ref{fig:catch_self_transfer}, where for each \texttt{Catch} version the full lines represent the performance of a network that is trained from scratch, whereas the dashed and dotted lines report the performance that is obtained when the network is either used as simple feature extractor or entirely fine-tuned respectively. We can see that if a pre-trained Deep-Q Network is used as a simple feature extractor, then the agent is able to converge to the optimal policy almost immediately. In fact as can consistently be observed in all plots of Fig. \ref{fig:catch_self_transfer} and from the results presented in Table \ref{}, training only the last layer of a pre-trained network does yield positive transfer for all different \texttt{Catch} versions. However, when a fine-tuning training strategy is adopted much more surprising results have been obtained. First, and perhaps more importantly, we can see that despite all models showing some learning speed improvements at early training iterations, their final performance is never on par with the one that is obtained when the same kind of model is either used as a feature extractor or trained from scratch (the dotted lines are consistently below the dashed and full lines). While when it comes to \texttt{Catch-v0} the policy learned by a fine-tuned model still allows the agent to successfully catch $\approx 95\%$ of the falling balls, the same cannot be said when the models are tested on \texttt{Catch-v1, Catch-v2} and \texttt{Catch-v3}, where the difference in terms of performance between a model trained from scratch and a fine-tuned one is much more significant. Second, as highlighted by the large variance across different training runs, fine-tuning on \texttt{Catch-v1} and \texttt{Catch-v3} resulted in highly unstable learning as well.    


\input{./Results/Chapter08/tex/catch_self_tl_experiments.tex}


\section{Deep-Q Networks Are Poor Feature Extractors}

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/pre_trained_network.pdf}
\caption{DQN I}
\label{fig:net1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/only_head.pdf}
\caption{DQN II}
\label{fig:net2}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/fine_tuned_network.pdf}
\caption{DQN III}
\label{fig:net3}
\end{minipage}
\end{figure}







Explain the experiment designed with Pierre
 Results performance goes down and back up
 Conclusion has to be discussed with Pierre


