%Chapter 7

\chapter{On the Transferability of Deep-Q Networks} % Chapter title
\label{ch:dqn_transfer} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 

\begin{remark}{Outline}
\end{remark}




\section{Motivation}

Model based transfer seems to work
No proper study in the model-free setting, no clear answer wrt transfer learning properties + limited set of algorithms and testing environments
Chapter before, extraordinarily long training times for benchmarks which are actually simple and sometimes not great performance neither -> tl could help achieve super human performance on environments which training from scratch appears to be limiting.


\section{A large-scale Empirical Study}
\label{sec:empirical_study}


\subsection{The Atari Environments}
\label{sec:atari_environments}

As DRL testbed we keep using the Atari Arcade Learning Environment (ALE) that we have also adopted in the previous chapter when investigating the performance of DQV and DQV-Max Learning. Next to being one of the most popular benchmarks in DRL, the Atari suite provided by \citet{bellemare2013arcade} is particularly well suited for transfer learning research as it allows to choose among a set of $57$ Atari games that can be used as source and target MDPs within a deep transfer learning setting. Furthermore, despite each game coming with its own challenges, the way a state is represented and used as input for a convolutional neural network stays consistent across all Atari games, a property that makes it possible to use the same type of neural architecture over all $57$ environments, and that overcomes the need of having to define a new neural network for each game (which would have made transfer learning not possible). Since training a DRL agent on the games of the ALE is a process which can go from lasting days to even weeks, we have carefully selected a subset of $10$ different environments that will serve for the transfer learning study presented in this chapter. These games are the following: . The game selection process was guided by numerous reasons. First, we have selected games for which we have the guarantee that a model-free DRL agent is able to learn a good policy for. Since, as discussed in Chapter \ref{ch:transfer_learning}, one of the key requirements of transfer learning is that of transferring knowledge across source and target tasks, we naturally ensured that some knowledge coming in the form of neural network parameters representing a near optimal value function was available for transfer. Second, while it is true that all of the selected games result in an agent that is able to improve its policy over time, some games were chosen because the learned policy also resulted in a final performance that was not on par with that of a human expert player. This is for example the case of the \texttt{Frostbite} game, where the gap in performance between a DQV agent ($\approx 270$) and a human expert player ($\approx 4300$) is particularly significant. Therefore, it follows that \texttt{Frostbite} is a potentially interesting target task for transfer, as there is definitely room for improving the agent's performance, which could be done through transfer learning. Furthermore, we have also ensured that among the selected games, some environments are more similar to each other than others. This if for example the case for the \texttt{Ms.Pacman} and \texttt{Bank Heist} games which, as can be seen in Fig. \ref{}, are two games where the state space is represented as a maze and where the end goal of an agent is that of learning how to navigate it. In like manner we have also included games which are very different from each other as is e.g., the case for the \texttt{Crazy Climber} and \texttt{Pong} games, where it is clear from Fig. \ref{} that no visual similarities are shared among the two environments. Including visually similar and dissimilar games easily allows us to investigate whether, as is the case in supervised learning, a source task is particularly well suited for transfer if it similar to its respective target task. 

\subsection{Experimental Setup}

Regarding the model-free DRL algorithms used for our study we investigate the transfer learning performance of agents that get trained with the DQV-Learning algorithm \cite{sabatelli2018deep} presented in Chapter \ref{ch:dqv_family_of_algorithms}, and with the DDQN algorithm \cite{van2016deep} reviewed in Chapter \ref{ch:reinforcement_learning}. We take models which come as pre-trained on all the aforementioned Atari games and transfer them to the remaining environments. This is particularly easy to achieve as both algorithms learn an approximation of the optimal state-action value function $Q(s,a;\theta)$ by training a convolutional neural network directly on the images representing the state of the game. In fact, as mentioned beforehand, the state space across Atari games is always represented as a $84\times84\times4$ tensor, which makes it straightforward to transfer the same neural architecture among various Atari environments without needing special modifications. The only modification that we apply to a pre-trained network, however, concerns its last layer responsible for estimating the different $Q$ values, which we always replace and randomly re-initialize. Following the typical deep transfer learning experimental setup discussed in Chapter \ref{ch:transfer_learning} and the results obtained throughout the second part of this dissertation, we investigate whether in DRL it is as beneficial as it is in supervised learning to fine-tune a $\mathcal{M}_S$ pre-trained network on $\mathcal{M}_T$. We do this by quantitatively assessing the transfer learning benefit on each $\mathcal{M}_S/\mathcal{M}_T$ pair by computing the area ratio metric $r$ \cite{taylor2009transfer} given the learning curve representing the performance of a model as follows:
\begin{equation}
	r = \frac{\text{area of $\mathcal{M}_S$ $-$ area of $\mathcal{M}_T$}}{\text{area of $\mathcal{M}_T$}},
\label{eq:area_ratio_metric}
\end{equation}
where the area under the curve is approximated via trapezoidal numerical integration
\begin{equation}
	\int^{b}_{a}f(x)\:dx \approx(b-a) \cdot \frac{1}{2}(f(a)+f(b))
\end{equation}
and $a$ and $b$ correspond to the first and last training epochs respectively (see Fig. \ref{} for a visualization of $r$). 


\subsection{Results}
When analyzing the performance of fully fine-tuned models, more exciting results have been obtained. Specifically, we have noticed that this approach can result in one out of three possible TL types. We classify them as follows:

\begin{itemize}
	\item\textcolor{RoyalBlue}{Negative-TL}: this is a training scenario in which it is counterproductive to start solving the problem modeled by $\mathcal{M}_t$ with a network that is pre-trained on $\mathcal{M}_s$. The most beneficial strategy instead is to train a randomly initialized network from scratch. We can visually observe how this type of TL presents itself in the first plot of Fig. \ref{fig:types_of_tl}, where we report the TL performance of a model which is pre-trained on \texttt{BankHeist} and is fine-tuned on \texttt{FishingDerby}.
   We can observe that over $\approx 3000$ training episodes, the cumulative reward obtained by the \texttt{BankHeist} model is significantly lower than the one obtained by a randomly initialized model.        
    
    \item\textcolor{RoyalBlue}{Absent-TL}: this training scenario does not present significant benefits, nor particular drawbacks when a pre-trained network is trained on $\mathcal{M}_t$ instead of a randomly initialized one. We report an example of this TL scenario in the central plot of Fig. \ref{fig:types_of_tl} where a \texttt{BankHeist} model gets fine-tuned on the \texttt{Boxing} game. We can easily observe that there are no significant differences between the performance of the two networks. While the final network's performance is not as bad as the one obtained when the previous TL scenario occurs, Absent-TL is still not desirable since there are no practical benefits that can motivate the effort required for pre-training a Deep Q-Network on a source-task.
    
    \item\textcolor{RoyalBlue}{Positive-TL}: this corresponds to the most desirable outcome that can be achieved when a pre-trained network gets fine-tuned on the target-task. As reviewed in Chapter \ref{ch:transfer_learning} TL has three main objectives: `Learning speed Improvements", `Asymptotic Improvements" and `Jumpstart Improvements". We show how the first two of these improvements can appear in DRL in the last plot of Fig. \ref{fig:types_of_tl}, where a model pre-trained on \texttt{BankHeist} can significantly outperform a model that is trained from scratch on the \texttt{Enduro} environment. We can observe a \textit{Learning speed Improvement} since the \texttt{BankHeist} model starts improving its policy already after $\approx 50$ epochs, while at the same time, there also is \textit{Asymptotic Improvement} since the cumulative reward that is obtained by the networks is significantly higher than the one that is obtained by an agent trained from scratch. 
\end{itemize}

These three different transfer learning outcomes are also reflected by the results that are presented in Tables \ref{} and \ref{}, where we report for all different $\mathcal{M}_S/\mathcal{M}_T$ pairs the area ration obtained after fine-tuning a DQV pre-trained agent, and a DDQN pre-trained agent respectively.  



\section{Control Experiments}

The results presented in the previous section seem to be questioning the level of transferability of convolutional neural networks that get trained for solving RL tasks. In fact, the training strategy of fine-tuning a pre-trained model on $\mathcal{M}_T$ does not result in the same type of performance gains that we have extensively observed throughout the first part of this dissertation, when pre-trained models were transferred and fine-tuned in a supervised learning context. To better characterize the transfer learning properties of pre-trained convolutional neural networks for DRL, we have designed a set of simple control experiments that allow us to examine their transfer learning behavior in training conditions that are as similar as possible to the ones that have characterized the experimental setup adopted in Sec. \ref{sec:empirical_study}. The main difference, however, is that the RL tasks that will be considered from now on, are much simpler than the ones defined within the ALE environment, and therefore do not require extraordinarily long training times for learning an optimal policy.   

\subsection{The Catch Environments}
\label{sec:catch_environments}

To this end we have implemented four different versions of the \texttt{Catch} game, a simple RL task that was first presented by \citet{mnih2014recurrent}, and that has been widely used within the literature for investigating the performance of DRL algorithms in a fast, and computationally less expensive manner than the one usually required by the \tattt{Atari} games \cite{vanjos2018deep, aittahar2020empirical}. In the game of \texttt{Catch}, an agent controls a paddle at the bottom of the environment, represented by a $21 \times 21$ grid, and has to catch a ball falling from top to bottom which can potentially bounce off walls. At each time-step, the agent can choose between three different actions: move the paddle one pixel to the right, move it to the left, or do not perform any of the aforementioned actions, therefore keeping its paddle in the same position in the grid. A RL episode ends either when the agent manages to catch the ball, in which case it receives a reward of $1$, or when it misses the ball, which naturally results in a reward of $0$. Following the design choices presented in \cite{vanjos2017deep}, we model the ball to have vertical speed of $v_y=-1 \: cell/s$ and horizontal speed of $v_x \in \{-2,-1,0,1,2\}$. From now on we will refer to this version of the game as \texttt{Catch-v0}, as it is the most basic and simplest form of the game that will be used throughout our experiments (see Fig. \ref{fig:fig1} for an impression of the game). Next to \texttt{Catch-v0} we have implemented three, slightly different and arguably more complex versions of the game as well. The first of such versions is \texttt{Catch-v1}, visually represented in Fig. \ref{fig:fig2}, where the complexity of the game is increased by reducing the size of the paddle that is controlled by the agent. While for \texttt{Catch-v0} its size is of five pixels, in \texttt{Catch-v1} it is of two pixels, therefore requiring the agent to be more precise if it wants to successfully catch the falling ball. The second alternative version of \texttt{Catch} is \texttt{Catch-v2}. In this case, the dynamics of the game are identical to the ones that define \texttt{Catch-v0}, however, as can be seen in Fig. \ref{fig:fig3} the way the $21 \times 21$ grid is represented changes. While in \texttt{Catch-v0} as well as in \texttt{Catch-v1} the state is represented by a binary grid where all pixels, but the ones representing the paddle and the ball have a value of $0$, in \texttt{Catch-v3} the cells around the paddle and the ball can have a random value between $0$ and $255$. This design choice makes it much harder for a convolutional network to correctly locate and identify the position of the paddle and of the falling ball, and makes \texttt{Catch-v2} the arguably most complex version among the different \texttt{Catch} versions of the game. Lastly we have implemented \texttt{Catch-v3}, a version of \texttt{Catch} which is identical to the one that is modeled by \texttt{Catch-v0} with the main difference that the representation of the state is now mirrored (see Fig. \ref{fig:fig4}), therefore requiring the agent to look at different parts of the grid if it wants to locate the paddle, and understand that the ball is unnaturally moving from the bottom to the top.  

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v0}
\caption{\texttt{Catch-v0}}
\label{fig:fig1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v2}
\caption{\texttt{Catch-v1}}
\label{fig:fig2}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v3}
\caption{\texttt{Catch-v2}}
\label{fig:fig3}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.35\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/catch_v1}
\caption{\texttt{Catch-v3}}
\label{fig:fig4}
\end{minipage}
\end{figure}



\paragraph{Experimental Setup}
\label{sec:experimental_setup_control}
Given the overall simplicity of the different \texttt{Catch} environments, we now train a DQN agent instead of the arguably more complex DQV and DDQN agents that we considered in Sec. \ref{}. The agent comes in the form a two-hidden layer convolutional neural network which is followed by a fully connected layer of $256$ hidden units preceding the final linear layer responsible for estimating the different $Q(s,a)$ values. The first convolutional layer has $32$ channels whereas the second one has $64$ channels. All layers of the network are activated by a ReLU non-linearity. We use an experience replay memory buffer that is set to contain $400000$ trajectories, and start training the model as soon as $5000$ trajectories have been collected. For exploration purposes, we adopt the popular epsilon-greedy strategy with an initial $\epsilon$ value of $1$ which gets linearly annealed over time to $0.1$. Learning a near optimal policy on the aforementioned \texttt{Catch} environments with this type of DQN agent can require between the $3$ and the $5$ hours of training time. We visualize the learning curves for each of the four different \texttt{Catch} environments in Fig. \ref{fig:catch_baselines}, where for every training epoch (reported on the $x$ axis) we report the average performance that is obtained by an agent that is evaluated on ten different episodes ($y$ axis). Shaded areas around the line plots correspond to $\pm 1$ std. deviation obtained over five different training runs with five different random seeds.    

\paragraph{DQN Performance}
As we can see from the results reported in Fig. \ref{fig:catch_baselines}, the DQN agent described above is able to successfully learn a near optimal policy for all \texttt{Catch} versions. When it comes to \texttt{Catch-v0}, \texttt{Catch-v2} and \texttt{Catch-v3} we can observe that by the end of training the agent is able to catch $\approx 100\%$ of the falling balls, whereas its performance is slightly worse ($\approx 90\%$) when it comes to \texttt{Catch-v1} \footnote{Please note that the performance on \texttt{Catch-v1} can be improved by increasing the complexity of the DQN agent by adding one more convolutional layer. However, as the goal is to transfer the same type of model across \texttt{Catch} games we did not modify the architecture of the DQN agent, at the cost of having a slightly worse performing model.}. We can also observe that among the different \texttt{Catch} versions, \texttt{Catch-v0} and \texttt{Catch-v3} appear to be the easiest ones, as the agent requires significantly less training episodes to converge when compared to \texttt{Catch-v1} and \texttt{Catch-v2}. Furthermore, in line with the explanation presented beforehand, our results also confirm the hypothesis that \texttt{Catch-v2} is the overall most complicated \texttt{Catch} version, as learning requires significantly more, and potentially unstable, training epochs.

\begin{figure}
\centering
\input{./Results/Chapter08/tex/catch_baselines.tex}
\caption{Learning curves obtained by a DQN agent that is trained from scratch on the four different versions of the \texttt{Catch} game presented in Sec. \ref{sec:catch_environments}. We can see that \texttt{Catch-v0} and \texttt{Catch-v3} are equivalent in terms of complexity as they require the $\approx$ same number of training episodes to to reach convergence, whereas \textttt{Catch-v1} and \texttt{Catch-v2} result in an agent that either obtains on average lower rewards (\texttt{Catch-v1}), or converges slower (\texttt{Catch-v2}).}
\label{fig:catch_baselines}
\end{figure}

\subsection{From one Catch to Another}
\label{sec:from_one_catch_to_another}

We now replicate the TL study presented in Sec. \ref{sec:empirical_study} on the aforementioned \texttt{Catch} environments, with the hope of identifying why the process of fine-tuning a pre-trained convolutional neural network in a DRL context, seems to not be as beneficial as it is in the supervised learning one. To this end, our goal is to find at least one pair of \texttt{Catch} environments which results in positive transfer, and to then potentially identify some properties within the different \texttt{Catch} versions that could also hold for the pairs of \texttt{Atari} games which have yielded positive transfer in Tables \ref{} and \ref{}. Specifically, we formulate \textcolor{RoyalBlue}{two hypothesis}, based on which, we expect to experimentally observe positive transfer. First, we foresee that positive transfer will happen for all possible \texttt{Catch} combinations, as in the end the source MDP $\mathcal{M}_S$ and the target MDP $\mathcal{M}_T$ do not significantly differ from each other: in fact, the main task across the different \texttt{Catch} versions remains that of catching a falling ball; the action space is identical; and so is the reward function that always returns a value of $1$ when the agent succeeds in catching the falling ball. This hypothesis is also motivated by the results that we presented throughout the first part of this dissertation, where we showed that the higher the similarity between the source domain $\mathcal{D}_S$ and the target domain $\mathcal{D}_T$, the better the performance of a transferred pre-trained network. Second, in the case the previous hypothesis will not be empirically supported, we expect to at least observe positive transfer when using a model that comes as pre-trained either on \texttt{Catch-v1} or on \texttt{Catch-v2}. In fact, as described above and also shown by the performance reported in Fig. \ref{fig:catch_baselines}, these variants of the game can be considered as the two most complicated versions of the \texttt{Catch} environment. This hypothesis is motivated by numerous claims that stem from transfer learning research performed in computer vision. Indeed, when it comes to supervised learning, one of the main factors that makes a certain source task $\mathcal{T}_S$ good for transferring is its complexity \cite{}, which is usually defined in terms of dataset size and number of classes to classify. We therefore expect the complexity of the source MDP $\mathcal{M}_S$ to play an important role within DRL as well.
Similarly to what we did for DQV and DDQN in Sec. \ref{sec:empirical_study}, we take the four different models which have been trained from scratch on their respective \texttt{Catch} version, randomly re-initialize their last layer (responsible for estimating the different state-action values $Q(s,a)$) and fully fine-tune the pre-trained network on the three remaining \texttt{Catch} environments, following the experimental protocol described in Sec. \ref{sec:experimental_setup_control}. The results of this study are reported in Fig. \ref{fig:catch_transfer}, where in clockwise order we show the performance that is obtained when considering \texttt{Catch-v0}, \texttt{Catch-v1}, \texttt{Catch-v2} and \texttt{Catch-v3} as target MDP $\mathcal{M}_T$. The performance of each transferred network is compared against the performance that is obtained after training a DQN agent from scratch which matches with the results reported in Fig. \ref{fig:catch_baselines}.
Surprisingly we found that fine-tuning a pre-trained DQN agent never resulted in positive transfer learning. This can clearly be seen in all plots represented in Fig. \ref{fig:catch_transfer} and by the results reporting the area ratio metric in Table \ref{tab:}. The only case where starting from a pre-trained network appeared to be at least in part beneficial is represented by the first plot of Fig. \ref{fig:catch_transfer} when \texttt{Catch-v1} is considered as source MDP $\mathcal{M}_S$. In this case we can in fact observe some learning speed improvements within the first $25$ learning epochs. This is not surprising as an agent which is able to catch a ball with a small paddle (as defined by the game \texttt{Catch-v1}) should in principle also be able to do this when the size of its paddle is larger (which is the case for \texttt{Catch-v0}). What is more surprising, however, is that while training progresses we see that the performance of a \texttt{Catch-v1} pre-trained model starts deteriorating and that this model barely converges to the same performance that is obtained by a model trained from scratch. When it comes to all other $\mathcal{M}_S-\mathcal{M}_T$ pairs we see that pre-trained networks always perform significantly worse than randomly initialized models trained from scratch, with some extreme cases, as the ones reported for the pair represented in the last plot of Fig. \ref{}, where a pre-trained agent is barely able to improve its policy over time at all. These results invalidate our two hypotheses mentioned above as they clearly show that positive transfer in DRL does not arise when $\mathcal{M}_S$ and $\mathcal{M}_T$ are similar, nor when $\mathcal{M}_S$ is more complex than $\mathcal{M}_T$. 

\input{./Results/Chapter08/tex/catch_tl_experiments.tex}

\begin{table}
	\caption{The area ratio obtained after fine-tuning a pre-trained DQN agent on the different \texttt{Catch} environments. We can see that no matter which source game is used for pre-training, transfer learning surprisingly always results in negative transfer.}
	\input{./Tables/Chapter08/catch_tl_results.tex}
\end{table}


\subsection{Self-Transfer}

The surprisingly poor transfer learning performance observed in the previous experiment made us question the level of transferability of Deep-Q Networks even more. To further characterize their TL properties we decided to investigate whether pre-trained DRL agents are at least able to transfer to themselves. To this end we studied what happens when a DQN agent gets transferred to a version of \texttt{Catch} that matches with the version of the game that was also used during the pre-training stage. This experiment is in large part identical to the one presented in Sec. \ref{sec:from_one_catch_to_another}, with the only difference being that now $\mathcal{M}_S = \mathcal{M}_T$. Moreover, differently from the previous study, we now also investigate what happens if instead of fine-tuning the network completely we just use the pre-trained DQN agent as a simple feature extractor, therefore only training its last layer responsible for estimating the different state-action values. Our results are presented in Fig. \ref{fig:catch_self_transfer}, where for each \texttt{Catch} version the full lines represent the performance of a network that is trained from scratch, whereas the dashed and dotted lines respectively report the performance that is obtained when the network is either used as simple feature extractor or entirely fine-tuned. We can see that if a pre-trained Deep-Q Network is used as a simple feature extractor, then the agent is able to converge to the optimal policy almost immediately. In fact as can consistently be observed in all plots of Fig. \ref{fig:catch_self_transfer} and from the results presented in Table \ref{}, training only the last layer of a pre-trained network yields positive transfer for all different \texttt{Catch} versions. However, when a fine-tuning training strategy is adopted much more surprising results have been obtained. First, and more importantly, we can see that despite all models showing some learning speed improvements at early training iterations, their final performance is never on par with the one that is obtained when the same kind of model is either used as a feature extractor or trained from scratch (the dotted lines are consistently below the dashed and full lines). While when it comes to \texttt{Catch-v0} the policy learned by a fine-tuned model still allows the agent to successfully catch $\approx 95\%$ of the falling balls, the same cannot be said when the models are tested on \texttt{Catch-v1, Catch-v2} and \texttt{Catch-v3}, where the difference in terms of performance between a model trained from scratch and a fine-tuned one is much more significant. Second, as highlighted by the large variance across different training runs, fine-tuning on \texttt{Catch-v1} and \texttt{Catch-v3} resulted in highly unstable learning as well.    


\input{./Results/Chapter08/tex/catch_self_tl_experiments.tex}

\begin{table}
	\input{./Tables/Chapter08/catch_self_tl_results.tex}
\end{table}


\section{Deep-Q Networks Are Poor Feature Extractors}

\begin{figure}[ht]
	\centering
\begin{minipage}[b]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/pre_trained_network.pdf}
\caption{DQN I}
\label{fig:net1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/only_head.pdf}
\caption{DQN II}
\label{fig:net2}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/fine_tuned_network.pdf}
\caption{DQN III}
\label{fig:net3}
\end{minipage}
\end{figure}







Explain the experiment designed with Pierre
 Results performance goes down and back up
 Conclusion has to be discussed with Pierre


