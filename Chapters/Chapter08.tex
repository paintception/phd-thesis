%Chapter 7

\chapter{On the Transferability of Deep-Q Networks} % Chapter title
\label{ch:dqn_transfer} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 

\begin{remark}{Outline}
	




\end{remark}


\section{Introduction}
\label{sec:intro}

Over the last years, the marriage between Reinforcement Learning (RL) algorithms and deep neural networks, commonly denoted as Deep Reinforcement Learning (DRL) \cite{franccois2018introduction} has gained tremendous attention \cite{henderson2018deep}. Neural networks have, in fact, proven to be extremely successful both in a model-free RL setting as in a model-based one. Large part of their success can be attributed to their ability of serving as feature extractors as well as function approximators, a property that allows them to successfully learn optimal value functions \cite{mnih2013playing,mnih2015human,van2016deep,zhao2016deep,wang2016dueling,sabatelli2020deep}, stochastic policies \cite{lillicrap2015continuous,schulman2015high,schulman2015trust,wang2016sample,mnih2016asynchronous,schulman2017proximal,haarnoja2018soft,fujimoto2018addressing}, and models of an environment \cite{ha2018world,kaiser2019model,hafner2019dream,hafner2019learning,hafner2020mastering} that is usually formalized as a Markov Decision Process (MDP) $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$ \cite{puterman1990markov}. Despite the many remarkable achievements, training a DRL agent is a process that can be very time-consuming. The task of solving an optimal decision making problem is,  in fact, a challenging problem of its own, which is sometimes made even more difficult by the DRL community itself, which requires DRL practitioners to test the performance of their algorithms on benchmarks that are computationally very expensive (for an excellent position paper about this topic see \cite{obando2020revisiting}). One way of overcoming the need of individually training a DRL agent from scratch each time a new RL problem is encountered is based on Transfer Learning (TL). TL focuses on designing training strategies that allow machine learning models to retain and reuse previously learned knowledge when getting trained on new, unseen problems \cite{pan2009survey, zhuang2020comprehensive}. Within deep learning, TL is largely adopted by the Supervised Learning (SL) community \cite{huh2016makes,mormont2018comparison,sabatelli2018deep,dominguez2019transfer,vandaele2021deep,ho2021evaluation}, as it allows to train neural networks on problems that are characterized by a lack of appropriate training data or sufficient computational resources; however, typical TL approaches such as off the shelf feature extraction, or fine-tuning \cite{sharif2014cnn} have rarely been thoroughly studied from a DRL perspective. Therefore, the degree of transferability of DRL algorithms is not yet known. In this work, we focus on value-based, model-free algorithms, a family of techniques which focuses on training neural networks with the intent of learning an approximation of an optimal value function. While several of such algorithms, commonly denoted as Deep-Q Networks, exist, research studying their TL properties is, on the contrary scarce, and a clear answer to the question \textit{"How transferable are Deep-Q Networks?"} has yet to be given. In the attempt to clearly answering this question, we present the following three contributions:
\begin{itemize}
	\item We present a first \textbf{large scale empirical study} that studies the TL properties of popular model-free DRL algorithms on the Atari Arcade Learning Environment (ALE), where we show that transferring pre-trained networks in a DRL context can be a very challenging task.
	\item We design a set of \textbf{novel, control experiments} which allows us to thoroughly characterize the TL dynamics of Deep-Q Networks.
	\item While studying Deep-Q Networks from a TL perspective, we discover \textbf{novel learning dynamics} that provide a better understanding of how this family of algorithms deals with RL tasks.  
\end{itemize}


\section{A large-scale Empirical Study}
\label{sec:empirical_study}


\subsection{The Atari Environments}
\label{sec:atari_environments}

In this study, we use the Atari Arcade Learning Environment (ALE) \cite{bellemare2013arcade}. Next to being one of the most popular benchmarks in DRL, the ALE is particularly well suited for TL research as it allows to choose among a set of $57$ Atari games that can be used as source $\mathcal{M}_S$ and target $\mathcal{M}_T$ MDPs within a deep transfer learning setting. Since training a model-free agent on the games of the ALE is a process which can be computationally very expensive, we have carefully selected a subset of $10$ different environments. Numerous reasons guided the game selection process. First, we have selected games for which we guarantee that a model-free DRL agent can learn a good policy for. Since, as discussed by \citet{lazaric2012transfer}, one of the key requirements of TL is that of correctly identifying and transferring knowledge across source and target tasks, we naturally ensured that some knowledge coming in the form of neural network parameters representing a near-optimal value function was available for transfer. Second, while it is true that all of the selected games result in an agent that can improve its policy over time, some games were chosen because the learned policy resulted in a final performance that was not on par with that of a human expert player. This is, for example, the case of the \texttt{Frostbite} game, where the gap in performance between an agent trained with the DQV-Learning algorithm \cite{sabatelli2018deepqv} ($\approx 270$) and a human expert player ($\approx 4300$) is particularly significant. It follows that \texttt{Frostbite} is an interesting target task for transfer, as the agent's performance can potentially be improved through TL. Furthermore, we have also ensured that among the selected games, some environments are more similar to each other than others. This is, for example, the case for the \texttt{Ms. Pacman} and \texttt{Bank Heist} games which, as can be seen in Fig. \ref{fig:similar_games}, are two games where the state space is represented as a maze, and where the end goal of an agent is that of learning how to navigate it. In like manner, we have also included games that are very different from each other as is, e.g., the case for the \texttt{Crazy Climber} and \texttt{Pong} games, where it is clear from Fig. \ref{fig:dissimilar_games} that no visual similarities are shared among the two environments. Including visually similar and dissimilar games easily allows us to investigate whether, as is the case for supervised learning, a source task is particularly well suited for transfer if it is similar to its respective target task \cite{mensink2021factors}. 

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/similar_sources}
\caption{The visually similar \texttt{Ms Pacman} and \texttt{Bank Heist} games.}
\label{fig:similar_games}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{./Images/Chapter08/dissimilar_sources}
\caption{The highly different \texttt{Crazy Climber} and \texttt{Pong} games.}
\label{fig:dissimilar_games}
\end{minipage}
\end{figure}


\subsection{Experimental Setup}
\label{sec:experimental_setup_1}

We investigate the TL performance of agents that get trained with the DQV-Learning algorithm \cite{sabatelli2018deep}, and with the DDQN algorithm \cite{van2016deep}. We take models which come as pre-trained on 10 Atari games and transfer them to all remaining environments. We mostly consider the same games for both algorithms (\texttt{Bank Heist, Boxing, Crazy Climber, Fishing Derby, Frostbite, James Bond, Ms. Pacman, Pong} and \texttt{Zaxxon}) with the only exception that for DDQN \texttt{Frostbite} is replaced by \texttt{Gopher}, and \texttt{Zaxxon} is replaced by \texttt{Ice Hockey} as the \texttt{Frostbite} and \texttt{Zaxxon} DDQN agents failed to improve their policy whilst training. It is worth noting that TL is particularly easy to perform as both algorithms learn an approximation of the optimal state-action value function $Q(s,a;\theta)$ by training a convolutional neural network directly on the images representing the state of the game. Since the state space across Atari games is always represented as an $84\times84\times4$ tensor, it is straightforward to transfer the same neural architecture among various Atari environments without needing special modifications. However, the only modification that we apply to a pre-trained network concerns its last layer responsible for estimating the different $Q$ values, which we always replace and randomly re-initialize. Following the typical deep transfer learning literature, we investigate whether in DRL it is as beneficial as it is in supervised learning to transfer a network that comes as pre-trained on $\mathcal{M}_S$ and fine-tune it on $\mathcal{M}_T$. We do this by quantitatively assessing the transfer learning benefits on each $\mathcal{M}_S/\mathcal{M}_T$ pair by computing the area ratio metric $r$ \cite{taylor2009transfer}. Specifically, given a learning curve representing the performance of an agent pre-trained on $\mathcal{M}_S$, and that of an agent that is instead trained from scratch, we compute $r$ as follows:
\begin{equation}
	r = \frac{\text{area of $\mathcal{M}_S$ $-$ area of $\mathcal{M}_T$}}{\text{area of $\mathcal{M}_T$}}.
\label{eq:area_ratio_metric}
\end{equation}
where the area under the curve is approximated via trapezoidal numerical integration
\begin{equation}
	\int^{b}_{a}f(x)\:dx \approx(b-a) \cdot \frac{1}{2}(f(a)+f(b))
\end{equation}
and $a$ and $b$ correspond to the first and last training epochs respectively.

\subsection{Results}
\label{sec:results_1}

The results on each $\mathcal{M}_S/\mathcal{M}_T$ pair for the DQV and DDQN algorithms are presented in Tables \ref{tab:dqv_res} and \ref{tab:ddqn_res} respectively. In each cell of the tables, we report the area ratio metric defined in Eq. \ref{eq:area_ratio_metric}: the lower this score, the redder the color of the cell, and therefore the less beneficial it is to transfer and fine-tune a pre-trained agent. When it comes to the DQV algorithm, we can see that out of nine target environments; there is only one Atari game for which it is always beneficial to transfer and fine-tune a pre-trained model: \texttt{Fishing Derby}. In fact, a positive area ratio score is obtained no matter which source environment is used for pre-training, although the best results have been obtained when starting from an \texttt{Enduro} or \texttt{Pong} pre-trained network, which both resulted in an area ratio score of $\approx 0.72$. Positive transfer can also be observed on the \texttt{Frostbite} and \texttt{James Bond} games, but only for a very limited number of source games. For example, a \texttt{Bank Heist} pre-trained agent transfers well to both target games as it obtains an area ratio score of $0.729$ and $0.973$ respectively, but the same cannot be said for an \texttt{Enduro} pre-trained network, which on \texttt{Frostbite} results in absent transfer (the area ratio score is, in fact, $-0.017$), and yields negative transfer on \texttt{James Bond} ($r=-0.41$). We can also observe that there are environments where it is surprisingly never beneficial to transfer and fine-tune a pre-trained agent. This is, for example, the case for the \texttt{Bank Heist} and \texttt{Pong} games, where independently from which source game $\mathcal{M}_S$ is used for pre-training, a negative area ratio score is always obtained. Furthermore, it can also be observed that transfer learning across environments is not symmetric, as one source game $\mathcal{M}_S$ can result in positive transfer when it gets transferred to a certain target game $\mathcal{M}_T$, but the same outcome is not obtained when transfer is performed in the opposite direction. As an example we can consider the \texttt{Boxing/Fishing Derby} games: positive transfer is obtained when transferring from \texttt{Boxing}$\rightarrow$\texttt{Fishing Derby} ($r=0.552$), but negative transfer is obtained when transferring from \texttt{Fishing Derby} $\rightarrow$ \texttt{Boxing} ($r=-0.893$). When it comes to the DDQN algorithm, similar conclusions can be drawn: we can again observe that there are only very few cases for which it is beneficial to transfer and fine-tune a pre-trained DRL agent. Examples of such cases are networks that are pre-trained on \texttt{Ice Hockey} and \texttt{James Bond} which get transferred to \texttt{Boxing} ($r=0.245$ and $r=0.232$ respectively), or \texttt{Boxing} and \texttt{Enduro} models that get transferred to \texttt{Pong} ($r=0.936$ and $r=0.248$). \texttt{Bank Heist} and \texttt{Pong} are again the two target environments for which most of the transferred source models resulted in negative transfer, while differently from the experiments performed with the DQV algorithm, this time no positive transfer can be observed on \texttt{Fishing Derby}. Overall, the process of fine-tuning a pre-trained DDQN agent mostly results in absent transfer, as can be observed by the area ratio scores obtained on \texttt{Enduro, Fishing Derby, Gopher} and \texttt{Ice Hockey} which are all $\approx 0$ on average.  


\begin{table}[ht!]
	\caption{The results obtained when fine-tuning ten different pre-trained DQV agents (rows) on nine other Atari games (columns). The lower the area ratio score, the redder the color of the cell.} %We can observe that fine-tuning a pre-trained agent is only beneficial on a limited set of target MDPs $\mathcal{M}_T$ such as \texttt{Fishing Derby}, where positive transfer is obtained no matter what source game is used for pre-training, and \texttt{James Bond}, where negative transfer is only obtained when using \texttt{Enduro} and \texttt{Zaxxon} as source games.}
	\input{./Tables/Chapter08/dqv_tl_results.tex}
	\label{tab:dqv_res}
\end{table}


\begin{table}[ht!]
	\caption{The results obtained when fine-tuning ten different pre-trained DDQN agents (rows) on nine other Atari games (columns). Similarly to Table \ref{tab:dqv_res} the lower the area ratio score, the redder the color of the cell.} %We can again observe that fine-tuning a pre-trained agent is only beneficial in a very limited number of cases and that the DDQN algorithm results in different transfer learning performance than DQV.}
	\input{./Tables/Chapter08/ddqn_tl_results.tex}
	\label{tab:ddqn_res}
\end{table}




\section{Control Experiments}
\label{sec:control_experiments}
The results presented in the previous section seem to be questioning the level of transferability of DRL agents. In fact, the training strategy of fine-tuning a pre-trained model on $\mathcal{M}_T$ does not result in the same type of performance gains that have been extensively observed in a supervised learning context. To better characterize their TL properties, we have designed a set of simple control experiments that allow us to examine their transfer learning behavior in training conditions that do not require extraordinarily long training times for learning an optimal policy.   

\subsection{The Catch Environments}
\label{sec:catch_environments}

To this end, we have implemented four different versions of the \texttt{Catch} game, a simple RL task that was first presented by \citet{mnih2014recurrent}, and that has been widely used within the literature for investigating the performance of DRL algorithms in a fast, and computationally less expensive manner than the one required by the \tattt{Atari} games \cite{vanjos2018deep, aittahar2020empirical}. In the game of \texttt{Catch}, an agent controls a paddle at the bottom of the environment, represented by a $21 \times 21$ grid, and has to catch a ball falling from top to bottom, which can potentially bounce off walls.
At each time step, the agent can choose between three actions: move the paddle one pixel to the right, move it to the left, or do not perform any of the aforementioned actions, therefore keeping its paddle in the same position in the grid. An RL episode ends either when the agent manages to catch the ball, in which case it receives a reward of $1$, or when it misses the ball, which naturally results in a reward of $0$. Following the design choices presented in \cite{vanjos2017deep}, we model the ball to have vertical speed of $v_y=-1 \: cell/s$ and horizontal speed of $v_x \in \{-2,-1,0,1,2\}$. From now on, we will refer to this version of the game as \texttt{Catch-v0}, as it is the most basic and simplest form of the game that will be used throughout our experiments. Next to \texttt{Catch-v0} we have implemented three slightly different and arguably more complex versions of the game as well: \texttt{Catch-v1}, where we increased the complexity of the game by reducing the size of the paddle that the agent controls. While for \texttt{Catch-v0} its size is of five pixels, in \texttt{Catch-v1} it is of two pixels, therefore requiring the agent to be more precise if it wants to successfully catch the falling ball. The second alternative version of \texttt{Catch} is \texttt{Catch-v2}. In this case, the dynamics of the game are identical to the ones that define \texttt{Catch-v0}; however, the way the $21 \times 21$ grid is represented changes. While in \texttt{Catch-v0} as well as in \texttt{Catch-v1} the state is represented by a binary grid where all pixels, but the ones representing the paddle and the ball have a value of $0$, in \texttt{Catch-v2} the cells around the paddle and the ball can have a random value between $0$ and $255$. 
\begin{figure}[ht]
\begin{minipage}{0.5\textwidth}
	\centering
\includegraphics[width=5.5cm]{./Images/Chapter08/catch_games}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
	\centering
	\input{./Results/Chapter08/tex/catch_baselines.tex}
\end{minipage}
\caption{Image on the left: the four different versions of the \texttt{Catch} environment. In clockwise order: \texttt{Catch-v0, Catch-v1, Catch-v3} and \texttt{Catch-v2}. Image on the right: learning curves obtained by a DQN agent that is trained from scratch on the aforementioned \texttt{Catch} versions. Shaded areas correspond to $\pm1$ std. obtained over 5 different random seeds.}
\label{fig:catch_baselines}
\end{figure}
This design choice makes it much harder for a convolutional network to correctly locate and identify the position of the paddle and of the falling ball and makes \texttt{Catch-v2} the arguably most complex version among the different \texttt{Catch} environments. Lastly, we have implemented \texttt{Catch-v3}, a version of \texttt{Catch} which is identical to the one that is modeled by \texttt{Catch-v0} with the main difference that the representation of the state is now mirrored, therefore requiring the agent to look at different parts of the grid if it wants to locate the paddle, and understand that the ball is unnaturally moving from the bottom to the top. For an impression of all four \texttt{Catch} versions see the left image of Fig. \ref{fig:catch_baselines}.
Given the overall simplicity of the different \texttt{Catch} environments, we now train a DQN agent instead of the arguably more complex DQV and DDQN agents that we considered in Sec. \ref{sec:empirical_study}. As we can see from the results reported in the right plot of Fig. \ref{fig:catch_baselines}, averaged over five different runs, the agent is able to successfully learn a near optimal policy for all \texttt{Catch} versions. When it comes to \texttt{Catch-v0}, \texttt{Catch-v2} and \texttt{Catch-v3} we can observe that by the end of training, the agent is able to catch $\approx 100\%$ of the falling balls, whereas its performance is slightly worse ($\approx 90\%$) when it comes to \texttt{Catch-v1} \footnote{Please note that the performance on \texttt{Catch-v1} can be improved by increasing the complexity of the DQN agent by adding one more convolutional layer. However, as the goal is to transfer the same type of model across \texttt{Catch} games, we did not modify the architecture of the DQN agent, at the cost of having a slightly worse performing model.}. We can also observe that among the different \texttt{Catch} versions, \texttt{Catch-v0} and \texttt{Catch-v3} appear to be the easiest ones, as the agent requires significantly less training episodes to converge when compared to \texttt{Catch-v1} and \texttt{Catch-v2}. Furthermore, in line with the explanation presented beforehand, our results also confirm the hypothesis that \texttt{Catch-v2} is the overall most complicated \texttt{Catch} version, as learning requires significantly more, and potentially unstable, training epochs.


\subsection{From one Catch to Another}
\label{sec:from_one_catch_to_another}
We now replicate the TL study presented in Sec. \ref{sec:empirical_study} on the aforementioned \texttt{Catch} environments, with the hope of identifying why the process of fine-tuning a pre-trained convolutional neural network in a DRL context, seems to not be as beneficial as it is in the supervised learning one. Our goal is to find at least one pair of \texttt{Catch} environments which results in positive transfer, and to then potentially identify some properties within the different \texttt{Catch} versions that could also hold for the pairs of \texttt{Atari} games which have yielded positive transfer in Tables \ref{tab:dqv_res} and \ref{tab:ddqn_res}. We formulate two hypothesis, based on which, we expect to experimentally observe positive transfer. First, we foresee that positive transfer will happen for all possible \texttt{Catch} combinations, as in the end the source MDP $\mathcal{M}_S$ and the target MDP $\mathcal{M}_T$ do not significantly differ from each other: in fact, the main task across different \texttt{Catch} versions remains that of catching a falling ball; the action space is identical; and so is the reward function that always returns a value of $1$ when the agent succeeds in catching the falling ball. This hypothesis is also motivated by supervised learning research which shows that the higher the similarity between the source domain $\mathcal{D}_S$ and the target domain $\mathcal{D}_T$, the better the performance of a transferred pre-trained network \cite{sabatelli2018deep}.
Second, in the case the previous hypothesis will not be empirically supported, we expect to at least observe positive transfer when using a model that comes as pre-trained either on \texttt{Catch-v1} or on \texttt{Catch-v2}. In fact, as described above and also shown by the performance reported in Fig. \ref{fig:catch_baselines}, these are the two most complicated versions of the \texttt{Catch} environment. In supervised learning, one of the main factors that makes a certain source task $\mathcal{T}_S$ good for transferring is its complexity \cite{mensink2021factors}, which is usually defined in terms of dataset size and number of classes to classify, therefore we expect the complexity of the source game to play an important role within DRL as well. Similarly to what we did for DQV and DDQN in Sec. \ref{sec:empirical_study}, we take the four different models that have been trained from scratch on their respective \texttt{Catch} version, randomly re-initialize their last layer (responsible for estimating the different state-action values $Q(s,a)$) and fully fine-tune the pre-trained network on the three remaining \texttt{Catch} environments.
\begin{table}[ht!]
	\centering
	\caption{The area ratio obtained after fine-tuning a pre-trained DQN agent on the different \texttt{Catch} environments. We can see that no matter which source game is used for pre-training, transfer learning surprisingly never results in positive transfer.}
	\input{./Tables/Chapter08/catch_tl_results.tex}
	\label{tab:catch_tl_area_ratio}
\end{table}
The results of this study are reported in Fig. \ref{fig:catch_transfer}, where, from left to right, we show the performance that is obtained when considering \texttt{Catch-v0}, \texttt{Catch-v1}, \texttt{Catch-v2} and \texttt{Catch-v3} as target MDP $\mathcal{M}_T$. The performance of each transferred network is compared against the performance that is obtained after training a DQN agent from scratch which matches with the results reported in Fig. \ref{fig:catch_baselines}.
Surprisingly we found that fine-tuning a pre-trained DQN agent never resulted in positive transfer learning. This can clearly be seen in all plots represented in Fig. \ref{fig:catch_transfer} and by the results reporting the area ratio metric in Table \ref{tab:catch_tl_area_ratio}. The only case where starting from a pre-trained network appeared to be at least in part beneficial is represented by the first plot of Fig. \ref{fig:catch_transfer} when \texttt{Catch-v1} is considered as source MDP $\mathcal{M}_S$. In this case we can in fact observe some learning speed improvements within the first $25$ learning epochs. This is not surprising as an agent which is able to catch a ball with a small paddle (as defined by the game \texttt{Catch-v1}) should in principle also be able to do this when the size of its paddle is larger (which is the case for \texttt{Catch-v0}). What is more surprising, however, is that while training progresses we see that the performance of a \texttt{Catch-v1} pre-trained model starts deteriorating and that this model barely converges to the same performance that is obtained by a model trained from scratch. When it comes to all other $\mathcal{M}_S/\mathcal{M}_T$ pairs we see that pre-trained networks always perform significantly worse than randomly initialized models trained from scratch, with some extreme cases, as the one reported in the last plot of Fig. \ref{fig:catch_transfer}, where a \texttt{Catch-v0} pre-trained agent is barely able to improve its policy over time at all. These results invalidate our two hypotheses mentioned above as they clearly show that positive transfer in DRL does not arise when $\mathcal{M}_S$ and $\mathcal{M}_T$ are similar, nor when $\mathcal{M}_S$ is more complex than $\mathcal{M}_T$. 


\input{./Results/Chapter08/tex/catch_tl_experiments.tex}



\subsection{Self-Transfer}
\label{sec:self_transfer}

The surprisingly poor transfer learning performance observed in the previous experiment made us question the level of transferability of Deep-Q Networks even more. To further characterize their TL properties, we decided to investigate whether pre-trained DRL agents are at least able to transfer to themselves. To this end, we studied what happens when a DQN agent gets transferred to a version of \texttt{Catch} that matches with the version of the game that was also used during the pre-training stage. This experiment is in large part identical to the one presented in Sec. \ref{sec:from_one_catch_to_another}, with the only difference being that now $\mathcal{M}_S = \mathcal{M}_T$. Moreover, differently from the previous study, we now also investigate what happens if instead of fine-tuning the network completely, we just use the pre-trained DQN agent as a simple feature extractor, therefore only training its last layer (the head) responsible for estimating the different state-action values. 
\begin{table}[ht!]
	\centering
	\caption{The area ratio scores obtained after performing self-transfer. We can see that if only the last linear layer is trained, then positive transfer is obtained on all \texttt{Catch} environments, whereas if the network is fine-tuned, positive transfer is (in part) only obtained on \texttt{Catch-v2}.}
	\input{./Tables/Chapter08/catch_self_tl_results.tex}
	\label{tab:self_tl_area_ratio}
\end{table}
Our results are presented in Fig. \ref{fig:catch_self_transfer}, where for each \texttt{Catch} version, the full lines represent the performance of a network that is trained from scratch, whereas the dashed and dotted lines respectively report the performance that is obtained when the network is either used as simple feature extractor or entirely fine-tuned. We can see that if a pre-trained Deep-Q Network is used as a simple feature extractor, the agent can converge to the optimal policy almost immediately. In fact, as can consistently be observed in all plots of Fig. \ref{fig:catch_self_transfer} and from the results presented in Table \ref{tab:self_tl_area_ratio}, training only the last layer of a pre-trained network yields positive transfer for all different \texttt{Catch} versions. However, when a fine-tuning training strategy is adopted, much more surprising results have been obtained. First, and more importantly, we can see that despite all models showing some learning speed improvements at early training iterations, their final performance is never on par with the one that is obtained when the same kind of model is either used as a feature extractor or trained from scratch (the dotted lines are consistently below the dashed and full lines). While when it comes to \texttt{Catch-v0} the policy learned by a fine-tuned model still allows the agent to successfully catch $\approx 95\%$ of the falling balls, the same cannot be said when the models are tested on \texttt{Catch-v1, Catch-v2} and \texttt{Catch-v3}, where the difference in terms of performance between a model trained from scratch and a fine-tuned one is much more significant. Please also note that special attention should be given to \texttt{Catch-v2}, which is an environment where the area ratio score reported in Table \ref{tab:self_tl_area_ratio} can be misleading as it does not entirely reflect the quality of the final policy learned by the agent. In fact, while it is true that an $r$ value of $0.393$ is obtained, it is worth noting that a fine-tuned network converges to a policy that is significantly worse than the one of a network trained from scratch, as the agent is only able of catching $\approx 80\%$ of the falling balls. Second, as highlighted by the large variance across different training runs, fine-tuning on \texttt{Catch-v1} and \texttt{Catch-v3} resulted in highly unstable learning as well.    
\input{./Results/Chapter08/tex/catch_self_tl_experiments.tex}


\section{The Two Learning Phases of Deep-Q Networks}
\label{sec:hybrid_self_transfer}

A prototypical Deep-Q Network takes as input an image representing the state of the environment and processes it through a series of convolutions and a fully connected layer. When this is done, it outputs as many $Q(s,a)$ values as there are actions available to the agent, a process that corresponds to learning a linear policy in the latent feature space of the network. It follows that by the end of training, such a model has to serve two purposes: it has to perform as a feature extractor as well as an optimal value function approximator.
Extracting relevant features from high dimensional inputs and learning an optimal value function can arguably be seen as two separate tasks; yet, despite their dissimilarity, we believe that they are more interconnected than one might expect. Specifically, we hypothesize that while learning, a Deep-Q Network has to carefully find a balance between training the parts that serve as feature extractors and the components that are responsible for estimating a policy. The poor TL performance observed throughout this work could therefore be the result of using models where the feature extractor component of an agent, as it comes as pre-trained, is too detached from the respective final layer of the network, which is randomly initialized instead.   
To show that a Deep-Q Network has to carefully coordinate training its feature extractor components and its final layer, we have designed the following experiment: let us start by considering the left image of Fig. \ref{fig:networks}. The first of the depicted networks corresponds to a pre-trained model where the parts that are involved in the feature extraction phase are colored in green, whereas the final linear layer is colored in red. Following the results presented in Sec. \ref{sec:self_transfer}, we know that such a model results in positive transfer when its last linear layer is trained in isolation after being randomly re-initialized, whereas it yields negative transfer if a fine-tuning training strategy is adopted. These two TL approaches are represented by the second and third networks, where we can, in fact, see that when the last layer is trained, only the color of the final output nodes is changed (from red to purple), whereas all colors are changed to yellow and blue when representing a fine-tuned network.  

\begin{figure}[ht]
\begin{minipage}{0.5\textwidth}
	\centering
\includegraphics[width=4.5cm]{./Images/Chapter08/networks_.pdf}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
	\centering
	\input{./Results/Chapter08/tex/final_tl_experiment.tex}
\end{minipage}
\caption{Image on the left, left to right and top to bottom: a pre-trained network; a network which only updates its last linear layer; a network which gets wholly fine-tuned and a network whose last linear layer is initialized with the parameters of the second network while the remaining layers are initialized with the weights of the third network. Image on the right: a successful example of positive transfer.}
\label{fig:networks}
\end{figure}

We now investigate the TL performance of a model that is a combination between the one depicted in the second image of the left figure of Fig. \ref{fig:networks}, and the one represented in the third image of the left figure of Fig. \ref{fig:networks}. Specifically, we fine-tune a Deep-Q Network whose last linear layer is initialized with the parameters that yielded positive transfer in Sec. \ref{sec:self_transfer}, whereas its convolutional and fully connected layers are initialized with the parameters that yielded negative transfer (see the last image in the left figure of Fig. \ref{fig:networks}). We visualize the self-transfer performance of these models, denoted as `Hybrid" , as they are a hybrid combination of two differently pre-trained networks, in Fig. \ref{fig:hybrid_self_transfer} with a cyan dashed dotted line. We can observe that learning is characterized by a very atypical behavior: the network starts by improving its performance (thanks to the already trained final layer); it then goes through a second stage where it starts to perform more poorly (due to the poor feature extractor part), and then finally starts learning stably (when the feature extractor and the head of the model are synchronized). We believe that the poor TL performance observed throughout this work is, therefore, the result of models which could not find a balance between a randomly initialized head and their respective pre-trained layers which are too biased towards the source task.   

Based on these results, one critical question still remains to be answered: how come positive transfer for some Atari games was observed in Sec. \ref{sec:empirical_study}?. We believe that the answer to this question does not lie within the feature representations that a Deep-Q Network learns, but rather in some inner properties of the environment that is used as target task and that favors a Deep-Q Network to correctly synchronize its components. As a proof of concept, we have created one final \texttt{Catch} environment, called \texttt{Catch-v4}, which is identical to \texttt{Catch-v0} with the only difference being that a positive reward is returned to the agent only if it manages to catch five falling balls in a row. We can see from the right image of Fig. \ref{fig:networks} that a model trained from scratch (represented by the purple line) is not able to improve its policy over time at all, as the reward signal is probably too sparse for learning, whereas a \texttt{Catch-v0} model is now able to yield positive transfer. We hence believe that some environments, if combined with certain learning algorithms, are more prone to positive transfer than others, as was, e.g., the case for \texttt{Fishing Derby} and DQV-Learning where positive transfer was observed no matter what source task was used for pre-training. This is not due to the representations that are learned by a pre-trained network but rather because of some specific dynamics within the target MDP $\mathcal{M}_T$. 

\input{./Results/Chapter08/tex/catch_hybrid_self_tl.tex}


\section{Related Work \& Conclusion}

The closest research to the one presented in this work is certainly that of \cite{farebrother2018generalization} and \cite{tyo2020transferable}, who also studied the generalization properties of Deep-Q Networks in a model-free DRL context. Our extensive experiments confirm some of the preliminary claims that were made by the former about the potentially poor TL properties of Deep-Q Networks, but contradict the study of the latter, who suggested that fine-tuning DRL agents results in positive transfer when moving from a simpler task to a harder. While both works are certainly valuable, we also believe that their experimental results are not as thorough and on par with the ones of this study as they both considered a very limited number of RL problems (four and three respectively), therefore leaving the question \textit{"How transferable are Deep-Q Networks?"} unanswered. We believe that the answer to it is \textit{"barely"}, but we also believe that their poor TL properties, as well as the learning dynamics identified in Sec. \ref{sec:hybrid_self_transfer}, are inherent to this family of algorithms only. In fact, it is worth noting that several works describing the benefits of TL in RL do exist (but they all differ from the study presented in this work): \cite{tirinzoni2018transfer} show that it possible to successfully transfer value functions across tasks, yet their work does not consider deep networks as function approximators but rather Gaussian mixtures. \cite{parisotto2015actor} show that it can be beneficial to fine-tune a pre-trained DRL agent, but they consider multi-task learning and policy gradient algorithms as a way of pre-training. \cite{rusu2016progressive}, also show that fine-tuning can be beneficial, but in the context of progressive networks and again of policy gradient techniques. Similar conclusions for Actor-Critic algorithms can also be found in the works of \cite{zhu2017target} and \cite{chen2021improving}. Furthermore, \cite{landolfi2019model} and \cite{sasso2021fractional} show that fine-tuning a pre-trained network can be beneficial for DRL tasks, but for model-based RL approaches, which are again part of a family of techniques that is different from the ones analyzed in this work. To conclude, we would like to stress out that despite the overall poor TL performance observed throughout this paper, positive transfer can nevertheless be obtained in a model-free DRL setup, and hope that this paper can serve as a solid starting point for the DRL community which is interested in designing general and transferable agents.  








