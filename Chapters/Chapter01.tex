\chapter{Supervised Learning and Deep Neural Networks}
\label{ch:supervised_learning}

\begin{remark}{Outline}
structure ...
\end{remark}

\section{Introduction}
\label{sec:introduction01}


\section{Statistical Learning Theory}
\label{sec:learning_from_data}

We start by defining a Supervised Learning (SL) problem with a triplet containing the following elements:
\begin{itemize}
	\item An input space $\mathcal{X}$,
	\item An output space $\mathcal{Y}$,
	\item A joint probability distribution $P(X,Y)$.
\end{itemize}
Let us define with $\mathcal{F}$ the set of all functions $f$ that can be produced by a certain learning algorithm. In SL the main goal is to find a function $f:\mathcal{X}\rightarrow\mathcal{Y} \in \mathcal{F}$ that minimizes the expectation over $P(X,Y)$ of a certain loss $\ell$, based on the predictions made by $f$ and the correct outputs defined in $\mathcal{Y}$.

This expectation is also known as the \textcolor{RoyalBlue}{expected risk}, or generalization error, and is defined as:
\begin{equation}
	R(f) = \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \big[\ell(y,f(\vec{x}))\big],
\label{eq:expected_risk}
\end{equation}
where $f$ is built from a limited set of observations that define the SL problem we would like to solve. Such observations constitute the \textcolor{RoyalBlue}{learning set} $\mathcal{L}$ which is defined by $N$ pairs of input vectors and output values $(\vec{x}_1, y_1),...,(\vec{x}_N, y_N)$ where $\vec{x}_i \in \mathcal{X}$ and $y_i \in \mathcal{Y}$. 

As $P(X,Y)$ is unknown and $\mathcal{L}$ is finite, one cannot evaluate the quantity defined in Eq. \ref{eq:expected_risk}, however, one can compute an estimate of it instead. To this end it is common to use part of the learning set $\mathcal{L}$ for constructing a \textcolor{RoyalBlue}{training set} $\mathcal{L}_{\text{Train}}$, of size $M$, that can be used for computing the \textcolor{RoyalBlue}{empirical risk}, or training error, as follows:
\begin{equation}
	\hat{R}(f,\mathcal{L}_\text{Train}) = \frac{1}{M} \sum_{(\vec{x}_i, y_i)\in \mathcal{L}_{\text{Train}}} \ell(y_i,f(\vec{x}_i)).
\label{eq:empirical_risk}
\end{equation}
Computing Eq. \ref{eq:empirical_risk} results in an unbiased estimate that can be used for finding a good approximation of the optimal function $f^{*}$ that minimizes Eq. \ref{eq:expected_risk}. Formally this corresponds to satisfying the equality
\begin{equation}
	f^{*}_{\mathcal{L}_{\text{Train}}} = \underset{f\in\mathcal{F}}{\argmin}\hat{R}(f,\mathcal{L}_{\text{Train}}),
\end{equation}
which is known as the empirical risk minimization principle \cite{vapnik1992principles}. As mentioned by \citet{} empirical risk minimizers converge in the limit to optimal models:
\begin{equation}
	\lim_{N \to \infty} f^{*}_{\mathcal{L}_\text{Train}} = f^{*}.
\end{equation}
With these concepts in place we can summarize the goal of SL as finding a function $f$ that on average makes good predictions over $P(X,Y)$. To this end, when explaining the data generating process underlying $P(X,Y)$, $f$ does not have to be too `simple" nor too `complex". In order to assess this let $\mathcal{Y}^{\mathcal{X}}$ be the set of all functions $f:\mathcal{X}\rightarrow\mathcal{Y}$. The minimal expected risk over all these functions is defined as
\begin{equation}
	R_B = \underset{f\in\mathcal{Y}^{\mathcal{X}}}{\min} R(f),
	\label{eq:bayes_risk}
\end{equation}
and is called the \textcolor{RoyalBlue}{Bayes risk}. When minimized, the quantity defined in Eq. \ref{eq:bayes_risk} results in the best possible function $f_B$, which is called the Bayes model. If the capacity of the hypothesis space $\mathcal{F}$ chosen for finding $f$ is too low, then it follows that $R(f)-R_B$ will be large for any $f\in\mathcal{F}$, including $f^{*}$ and $f^{*}_{\mathcal{L}_{\text{Train}}}$. Similarly, if the capacity of $\mathcal{F}$ is too high then albeit $R(f)-R_B$ will be small, $f^{*}_\mathcal{L}_{\text{Train}}$ can fit 
$\mathcal{L}_{\text{Train}}$ arbitrarily well such that:
\begin{equation}
	R(f^{*}_{\mathcal{L}_{\text{Train}}}) \geq R_B \geq \hat{R}(f^{*}_{\mathcal{L}_{\text{Train}}},\mathcal{L}_{\text{Train}}) \geq 0.
\end{equation}
When $f$ is too simple then it is said to \textcolor{RoyalBlue}{underfit} the data, whereas it said to \textcolor{RoyalBlue}{overfit} it when it is too complex. As a result, one wants both the expected risk $R$ and the empirical risk $\hat{R}$ minimizers to be as low as possible. To achieve this we can again evaluate the perfomance of $f^{*}_{\mathcal{L}_{\text{Train}}}$ by computing the empirical risk defined in Eq. \ref{eq:empirical_risk} on a separate independent dataset known as the \textcolor{RoyalBlue}{testing set} $\mathcal{L}_{\text{Test}}$. Note however that this quantity should be used for model evaluation purposes only, and not for model selection ones, which is usually done through a separate dataset called the \textcolor{RoyalBlue}{validation set}. 

So far we have defined the concepts of expected risk and empirical risk with respect to a loss function $\ell$, however, we have not yet seen how this loss function looks like in practice. In SL $\ell$ changes based on the characteristics of $Y$. This allows us to distinguish between two different SL problems: \textcolor{RoyalBlue}{classification} and \textcolor{RoyalBlue}{regression}. In the first case $\mathcal{Y}$ comes in the form of a finite set of classes $\{c_1, c_2,...,c_i\}$, whereas in the latter case $\mathcal{Y}=\mathds{R}$. For classification the arguably most straightforward loss function is the $0-1$ loss defined as 
\begin{equation}
	\ell(f(\vec{x},y)) = \mathbb{1}(f(\vec{x}\neq y)),
\end{equation}
while for regression problems $\ell$ comes either in the form of the squared error loss:
\begin{equation}
	\ell(f(\vec{x},y)=(y-f(\vec{x})))^2
\end{equation}
or in the form of the absolute error loss
\begin{equation}
	\ell(f(\vec{x},y))=|y-f(\vec{x})|,
\end{equation}
depending on how much one wants to penalize the errors made by $f$.

While several SL algorithms adopting empirical risk minimization principles exist, throughout this dissertation we will, however, only focus on artificial neural networks, a family of techniques that will be reviewed hereafter.  

\section{Neural Networks}
\label{sec:neural_networks}

\subsection{Multilayer Perceptrons}
\label{sec:general_architecture}
The first mathematical model developed with the intention of mimicking the biological processes underlying the human brain was proposed by \citet{rosenblatt1958perceptron}. Inspired by the work of \citet{mcculloch1943logical}, Rosenblatt developed the \textcolor{RoyalBlue}{perceptron}, the simplest form of artificial neural network able of tackling supervised learning binary classification problems. Given an input vector $\vec{x}$ the perceptron produces the following output:
\begin{equation}
	f(\vec{x}) = 
		\begin{cases}
			1  & \text{if} \sum_i w_i x_i + b \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
\end{equation}
which is given by summing up each input $x$ with a certain weight $w$ and a final additional bias term $b$. The result of this sum is then passed through the sign non-linear activation function which yields output $h$:
\begin{equation}
	\text{sign}(x) = 
		\begin{cases}
			1  & \text{if} \: x \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
\end{equation}
The way the perceptron works is visually represented in Fig. \ref{fig:perceptron} and can be summarized as follows:
\begin{equation}
	f(\vec{x}) = \text{sign}(\sum_i w_i x_i + b).
	\label{eq:perceptron_equation}
\end{equation}

\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/perceptron.tex}
\caption{A visualization of a perceptron.}
\label{fig:perceptron}
\end{figure}
Interestingly, Eq. \ref{eq:perceptron_equation} can also be rewritten in terms of tensor operations. This allows us to express the perceptron classification rule as 
\begin{equation}
	f(\vec{x}) = \text{sign}(\vec{w}^\intercal \vec{x} + b),
	\label{eq:tensor_perceptron}
\end{equation}
and to conveniently visualize its mathematical operations through a \textcolor{RoyalBlue}{computational graph}, a directed graph where each node represents a certain mathematical operation. The computational graph of Eq. \ref{eq:tensor_perceptron} is represented in Fig. \ref{fig:computational_graph_0} and can be considered as the main building block of artificial neural networks.
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_0.tex}
	\caption{The computational graph representing the mathematical operations performed by the perceptron represented in Fig. \ref{fig:perceptron}.}
\label{fig:computational_graph_0}
\end{figure}

Eq. \ref{eq:tensor_perceptron} summarizes the computations that are performed by one single input unit with respect to $\vec{x}\in\mathds{R}^{p}$, $\vec{w}\in\mathds{R}^{p}$ and $b\in\mathds{R}$. If such units are stacked in parallel they perform a layer with $q$ outputs defined as:
\begin{equation}
	\vec{h} = \sigma(\vec{W}^\intercal\vec{x}+\vec{b}),
\end{equation}
where $\vec{h}\in\mathds{R}^{q},\vec{x}\in\mathds{R}^{p},\vec{W}\in\mathds{R}^{p\times q},b\in\mathds{R}^{q}$. One can then compose a series of $L$ layers
\begin{equation}
	\begin{split}
		\vec{h}_0 & = \vec{x} \\ 
		\vec{h}_1 & = \sigma(\vec{W}^{\intercal}_{1}\vec{h}_0 + \vec{b}_1) \\ 
	... \\
		\vec{h}_L & = \sigma(\vec{W}^{\intercal}_{L}\vec{h}_{L-1}+\vec{b}_{L})
	\end{split}
\end{equation}
and define the \textcolor{RoyalBlue}{multilayer perceptron}, also known as feedforward neural network, $f(\vec{x};\theta)$ where $\theta=\{\vec{W}_k,\vec{b}_k,...|k=1,...,L\}$. 

Now that we defined the mathematical computations that are performed by a feedforward neural network we move on to explaining how one can train these kind of models to perform empirical risk minimization.

\subsection{Stochastic Gradient Descent}
\label{sec:sgd}

Training a neural network consists in finding parameters $\theta$ such that a loss function $\mathscr{L}(\theta)$, also denoted as the \textcolor{RoyalBlue}{objective function}, is minimized. Such loss functions are typically expressed as a sum of the losses $\ell_n$ incurred by each sample $n$ in a training set of size $N$, and can be expressed in the following form:
\begin{equation}
	\mathscr{L}(\theta) = \sum_{n=1}^{N}\ell_n(\theta).
	\label{eq:sum_of_losses}
\end{equation}
When neural networks are used, $\mathscr{L}$ has to be differentiable as this allows to minimize it through first order optimization algorithms. Among such methods, the arguably most straightforward one is gradient descent, which updates the parameters $\theta$ proportionally to the negative gradient of $\mathscr{L}$. This is done by applying the following update rule:
\begin{equation}
	\begin{split}
	\theta_{t+1} & = \theta_t - \gamma_t(\nabla\ell(\theta_t))^{\intercal} \\ 
	& = \theta_t - \gamma_t \sum_{n=1}^{N}(\nabla \ell_n(\theta_t))^{\intercal},
	\end{split}
	\label{eq:gradient_descent}
\end{equation}
where $t$ is a time counter variable, and $\gamma\geq0$ is the learning rate, sometimes also denoted as the step-size parameter. We can easily observe that computing Eq. \ref{eq:gradient_descent} can become computationally very expensive as it requires to evaluate gradients from all individual functions $\ell_n$. This property is in fact what defines gradient descent as a batch optimization method, which makes it unfortunately unsuitable for dealing with large datasets. A possible solution to this computational burden consists in reducing the amount of computation required by the sum in Eq.\ref{eq:gradient_descent} by simply considering a small, random batch of samples of the training set. In the extreme case, one can even just estimate the gradient on one single, randomly chosen, training sample, which is a method called Stochastic Gradient Descent (SGD). While it is true that this approach gives an unbiased estimate of the true gradient, its estimate can also be very noisy, which is the reason behind why it is preferable to evaluate the gradient for a mini-batch of samples instead than on one single, unique sample. A large body of work has investigated the effect that the batch-size has on neural network training \cite{keskar2016large,radiuk2017impact,kandel2020effect}; however, so far no exact rule for determining an optimal batch-size exists. Yet, provided that enough computational resources are available, large mini-batches are usually preferred as they will result in more accurate estimates of the gradient, and therefore reduce the variance in the parameter update $\theta_{t+1}$. 

When the optimization surface is made of valley floors, gradient descent has the limitation of being very slow. To deal with such issue, several works have designed optimization strategies which make gradient based optimization faster and more efficient. The most straightforward improvement to the gradient descent algorithm is the one proposed by \citet{rumelhart1986learning} who suggested to use of an additional term in the update rule presented in Eq.\ref{eq:gradient_descent}, named \textcolor{RoyalBlue}{momentum}. This term, simply keeps track of what happened when the parameters were updated at $t-1$ and determines the next parameters' update as a linear combination between the current and previous gradients. This results in the following update rule:
\begin{equation}
	\theta_{t+1} = \theta_t - \gamma_t ((\nabla \ell(\theta_t)))^{\intercal} + \alpha\Delta\theta_t
	\label{eq: momentum}
\end{equation}
where  
\begin{equation}
	\begin{split}
	\theta_{t} & = \theta_t - \theta_{t-1} \\ 
		   & = \alpha\Delta\theta_{t-1}-\gamma_{t-1}(\nabla \ell(\theta_{t-1}))^{\intercal},
	\end{split}
\label{eq:gradient_descent}
\end{equation}
and $\alpha\in[0,1]$, which accelerates the optimization process and allows the algorithm average out noisy estimates of the gradient.

Next to adding a momentum term to improve the performance of gradient descent, another common method that can accelerate its convergence revolves around dynamically adapting the learning rate parameter $\gamma$. Popular neural network optimizers such as \texttt{RMSProp} \cite{tieleman2012lecture}, \texttt{AdaGrad} \cite{duchi2011adaptive}, and the very well-known \texttt{Adam} optimizer \cite{kingma2014adam}, all adapt this method. While discussing these algorithms into detail is out of the scope of this thesis, we refer the reader to the work of \citet{ruder2016overview}, which provides a nice overview of the most common gradient descent optimization algorithms, and to the work of \citet{schmidt2020descending} who empirically evaluate their performance across different networks and machine learning problems. 

Before ending this section it is worth noting that next to SGD-like methods, there also exist several alternative algorithms that can be used for optimization problems. Among such methods, we mention second order optimization techniques such as Newton, Quasi-Newton and the Conjugate gradient methods discussed in \cite{tan2019review}. While these algorithms are able to minimize the empirical risk faster and even better than SGD, they do not result in equally good generalization performance. Recall from Sec. \ref{sec:learning_from_data}, that in SL minimizing the expected risk is just as important as minimizing the empirical risk, which is a property that the aforementioned second order optimization algorithms do not have. This key result, first presented by \citet{bottou201113}, is what motivates the use of SGD-like optimizers in deep learning.       

\subsection{Backpropagation}
\label{sec:backprop}

From Eq.\ref{eq:gradient_descent} we can note that a crucial role in the optimization process is played by the gradient $\nabla\ell(\theta)$. As we have seen in Sec. \ref{sec:general_architecture} neural networks can be considered as a composition of nested functions $k$ for $k=0,...,K-1$, where each function comes with its own parameters $\theta_k$. Therefore the gradient comes in the form of a vector which contains all the partial derivatives of the loss $\ell$ with respect to the weights $\theta$ that parametrize the neural network:
\begin{equation}
	\nabla\ell(\theta) = \Big[\frac{\partial\ell}{\partial\theta_0}(\theta),...,\frac{\partial\ell}{\partial\theta_{K-1}}(\theta)\Big].
\end{equation}
As the number of functions increases, so does the complexity of the gradient, therefore an efficient way of calculating it is necessary. The backpropagation algorithm is a special case of a more general technique, called \textcolor{RoyalBlue}{automatic differentiation}, that allows to evaluate the gradient of complicated functions numerically and automatically. This is done by exploiting the chain rule, which can be applied recursively on the computation graph that keeps track of all the arithmetic operations that are performed by the network. 

To this end let us define a simplified version of a two hidden layer perceptron $f$ that is parametrized with weight matrices $\vec{W}_1$ and $\vec{W}_2$. When given input data $\vec{x}$ the network produces a prediction $\hat{y}$ which results from traversing the computational graph represented in Fig. \ref{fig:computational_graph_1}.  
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_1.tex}
\caption{Computational graph 1}
\label{fig:computational_graph_1}
\end{figure}
During the traversal, also known as the \textcolor{RoyalBlue}{forward pass}, the result of each mathematical operation is stored within its own output variable $u$ (see Fig. \ref{fig:computational_graph_2}) 
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_2.tex}
\caption{Computational graph 2}
\label{fig:computational_graph_2}
\end{figure}
Having such an annotated graph it is now possible to compute all partial derivatives efficiently by traversing the graph backwards (\textcolor{RoyalBlue}{backward pass}), and by applying the chain rule which in its general form states that:
\begin{equation}
	\frac{\partial \ell}{\partial \theta_i} = \sum_{k\in\text{parents}(\ell)} \frac{\partial \ell}{\partial u_k} \frac{\partial u_k}{\partial \theta_i}.
\end{equation}
Therefore taking as example $\vec{W}_1$, the derivative of the network's output $\hat{y}$ with respect to this weight matrix is given by:
\label{eq:general_chain_rule}
\begin{equation}
	\begin{split}
	\frac{\partial \hat{y}}{\partial \vec{W}_1} & = \frac{\partial \hat{y}}{\partial u_2} \frac{\partial u_2}{\partial h_1} \frac{\partial h_1}{\partial u_1} \frac{\partial u_1}{\partial\vec{W}_1} \\
	& = \frac{\partial\sigma(u_2)}{\partial u_2} \frac{\partial\vec{W}_2^{\intercal}h_1}{\partial h_1} \frac{\partial \sigma(u_1)}{\partial u_1} \frac{\partial \vec{W}_1^{\intercal}\vec{x}}{\partial{\vec{W}_1}}.
	\end{split}
	\label{eq:applied_chain_rule}
\end{equation}

\subsection{Loss Functions}
\label{sec:loss_functions}



\subsection{Activation Functions}
\label{sec:activation_functions}

\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/activation_functions.tex}
\caption{Activation functions}
\label{fig:activation_functions}
\end{figure}


\section{Convolutional Neural Networks}
\label{sec:convolutional_networks}

Convolutional Neural Networks (CNNs) are a family of artificial neural networks that are particularly well suited for problems involving high-dimensional inputs such as images or videos. This kind of data in fact prohibits the use of the multi-layer perceptrons presented in Sec. \ref{sec:general_architecture}, as it requires to represent images as unstructured vectors, which is a process that for obvious computational reasons is not feasible. Furthermore MLPs present some additional limitations: first and foremost, due to their fully connected structure, they do not involve any sort of parameter sharing across the network. Second, as the output of each unit in a layer is given as input to all the units in the subsequent layer, the interaction among all such neurons is also extremely dense. CNNs address these limitations by exploiting \textcolor{RoyalBlue}{sparse weight sharing} strategies that result into neural networks that are significantly more memory and computationally efficient. 

\subsection{Mathematical Operations}
\label{sec:operations}

As their name suggests, the key mathematical operation behind CNNs is that of \textcolor{RoyalBlue}{convolution}. A convolution operation is performed over two arguments: an input vector $\vec{x}\in\mathds{R}^{W}$, and a kernel $\vec{u}\in\mathds{R}^{w}$. Its output is a new vector of size $W-w+1$ such that:
\begin{equation}
	(\vec{x}\circledast\vec{u}[i]) = \sum_{m=0}^{w-1}x_{m+i}u_m,
	\label{eq:convolution}
\end{equation}
where $\circledast$ technically denotes the cross-correlation operation, namely a convolution operation that does not flip the kernel. The process described in Eq. \ref{eq:convolution} can easily be generalized to multi-dimensional tensors such as images which can in fact be seen as three-dimensional tensors $\vec{x}\in\mathds{R}^{C,W,H}$, of width and height $W$ and $H$ respectively, defined over the RGB color domain ($C=3$). Similarly one can also define a three-dimensional kernel $\vec{u}\in\mathds{R}^{C,w,h}$ whose purpose is to slide over the input tensor $\vec{x}$ and which yields a two-dimensional output tensor $\vec{o}$ of size $(H-h+1)\times(W-1+1)$ that is computed as follows:
\begin{equation}
	\begin{split}
		\vec{o}_{i,j} & = \vec{b}_{i,j} + \sum_{c=0}^{C-1}(\vec{x}_c\circledast\vec{u}_c)[j,i] \\ 
			      & = \vec{b}_{i,j} + \sum_{c=0}^{C-1} \sum_{n=0}^{h-1}\sum_{m=0}^{w-1} \vec{x}_{c,n+j,m+i}\vec{u}_{c,n,m},
	\end{splot}
\end{equation}
where $\vec{b}$ and $\vec{u}$ are learnable parameters. Within the deep learning literature, $\vec{o}$ is also referred to as a \textcolor{RoyalBlue}{feature map} \cite{goodfellow2016deep}.

Note that by adopting a convolution approach, one input unit in the network only affects as many output units as defined by the size of the kernel, which improves the computational efficiency of the network greatly. Furthermore, each member of the kernel is used across the entire image, which means that the parameters that define a convolution operation are shared alongside the different locations that are visited by $\vec{u}$. The way the kernel interacts with its respective tensor is usually defined by two additional components that both play an important role in the design of convolutional networks. The first of these components is \textcolor{RoyalBlue}{padding} which is a technique that adds some extra values around the perimeter of the input tensor $\vec{x}$, with the aim of preserving the information that is depicted around its corners. Second, there is the concept of \textcolor{RoyalBlue}{strides} which defines by how many elements at a time we wish to slide $\vec{u}$ over $\vec{x}$. As the goal of CNNs is that of downsampling the input tensor in a computationally efficient manner, it is usually good practice to have strides larger than one, albeit this comes at the cost of extracting features less thoroughly. 

Convolutional networks typically perform several convolutions in parallel, as multiple kernels are used. The output of each convolution is then passed through a non linear activation function such as the ones that we represented in Fig. \ref{fig:activation_functions}. To downsample the resulting feature maps even further, a \textcolor{RoyalBlue}{pooling} function is usually adopted. Its idea is to summarize the output of the convolving process at a certain location of the feature map through a summary statistic. This reduces its size while at the same time preserves the presence of the detected features. There are two common pooling operations one can choose from: max-pooling \cite{zhou1988computation}, which given a three dimensional tensor $\vec{x}\in\mathds{R}^{C\times(rh)\times(sw)}$ produces a tensor $\vec{o}\in\mathds{R}^{C\times r\times s}$ by simply keeping the maximum value of a feature map within a certain rectangular neighborhood such that 
\begin{equation}
	\vec{o}_{c,j,i} = \underset{n<h,m<w}{\max} \vec{x}_{c,rj+n,si+m},
\end{equation}
and average pooling, which instead computes the mean of a feature map such that 
\begin{equation}
	\vec{o}_{c,j,i} = \frac{1}{hw} \sum_{n=0}^{h-1}\sum_{m=0}^{w-1} \vec{x}_{c,rj+n,si+m}.
\end{equation}
Besides reducing the size of a feature map, pooling operations have also the important benefit of making the representations learned by the network invariant to small translations. In fact, one could translate the input by a small amount and still obtain the same output after pooling. Note however, that albeit desirable in most cases, there are situations where adopting pooling strategies should be avoided \cite{sabatelli2018learning, bidoiadeep}.  


\subsection{General Structure}
\label{sec:general_structure}

\subsection{Popular Architectures}
\label{sec:architectures}


\section{Conclusion}
\label{sec:conclusion01}
