\chapter{Supervised Learning and Deep Neural Networks}
\label{ch:supervised_learning}

\begin{remark}{Outline}
structure ...
\end{remark}

\section{Introduction}
\label{sec:introduction01}


\section{Statistical Learning Theory}
\label{sec:learning_from_data}

We start by defining a Supervised Learning (SL) problem with a triplet containing the following elements:
\begin{itemize}
	\item An input space $\mathcal{X}$,
	\item An output space $\mathcal{Y}$,
	\item A joint probability distribution $P(X,Y)$.
\end{itemize}
Let us define with $\mathcal{F}$ the set of all functions $f$ that can be produced by a certain learning algorithm. In SL the main goal is to find a function $f:\mathcal{X}\rightarrow\mathcal{Y} \in \mathcal{F}$ that minimizes the expectation over $P(X,Y)$ of a certain loss $\ell$, based on the predictions made by $f$ and the correct outputs defined in $\mathcal{Y}$.

This expectation is also known as the \textcolor{RoyalBlue}{expected risk}, or generalization error, and is defined as:
\begin{equation}
	R(f) = \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \big[\ell(y,f(\vec{x}))\big],
\label{eq:expected_risk}
\end{equation}
where $f$ is built from a limited set of observations that define the SL problem we would like to solve. Such observations constitute the \textcolor{RoyalBlue}{learning set} $\mathcal{L}$ which is defined by $N$ pairs of input vectors and output values $(\vec{x}_1, y_1),...,(\vec{x}_N, y_N)$ where $\vec{x}_i \in \mathcal{X}$ and $y_i \in \mathcal{Y}$. 

As $P(X,Y)$ is unknown and $\mathcal{L}$ is finite, one cannot evaluate the quantity defined in Eq. \ref{eq:expected_risk}, however, one can compute an estimate of it instead. To this end it is common to use part of the learning set $\mathcal{L}$ for constructing a \textcolor{RoyalBlue}{training set} $\mathcal{L}_{\text{Train}}$, of size $M$, that can be used for computing the \textcolor{RoyalBlue}{empirical risk}, or training error, as follows:
\begin{equation}
	\hat{R}(f,\mathcal{L}_\text{Train}) = \frac{1}{M} \sum_{(\vec{x}_i, y_i)\in \mathcal{L}_{\text{Train}}} \ell(y_i,f(\vec{x}_i)).
\label{eq:empirical_risk}
\end{equation}
Computing Eq. \ref{eq:empirical_risk} results in an unbiased estimate that can be used for finding a good approximation of the optimal function $f^{*}$ that minimizes Eq. \ref{eq:expected_risk}. Formally this corresponds to satisfying the equality
\begin{equation}
	f^{*}_{\mathcal{L}_{\text{Train}}} = \underset{f\in\mathcal{F}}{\argmin}\hat{R}(f,\mathcal{L}_{\text{Train}}),
\end{equation}
which is known as the empirical risk minimization principle \cite{vapnik1992principles}. As mentioned by \citet{} empirical risk minimizers converge in the limit to optimal models:
\begin{equation}
	\lim_{N \to \infty} f^{*}_{\mathcal{L}_\text{Train}} = f^{*}.
\end{equation}
With these concepts in place we can summarize the goal of SL as finding a function $f$ that on average makes good predictions over $P(X,Y)$. To this end, when explaining the data generating process underlying $P(X,Y)$, $f$ does not have to be too `simple" nor too `complex". In order to assess this let $\mathcal{Y}^{\mathcal{X}}$ be the set of all functions $f:\mathcal{X}\rightarrow\mathcal{Y}$. The minimal expected risk over all these functions is defined as
\begin{equation}
	R_B = \underset{f\in\mathcal{Y}^{\mathcal{X}}}{\min} R(f),
	\label{eq:bayes_risk}
\end{equation}
and is called the \textcolor{RoyalBlue}{Bayes risk}. When minimized, the quantity defined in Eq. \ref{eq:bayes_risk} results in the best possible function $f_B$, which is called the Bayes model. If the capacity of the hypothesis space $\mathcal{F}$ chosen for finding $f$ is too low, then it follows that $R(f)-R_B$ will be large for any $f\in\mathcal{F}$, including $f^{*}$ and $f^{*}_{\mathcal{L}_{\text{Train}}}$. Similarly, if the capacity of $\mathcal{F}$ is too high then albeit $R(f)-R_B$ will be small, $f^{*}_\mathcal{L}_{\text{Train}}$ can fit 
$\mathcal{L}_{\text{Train}}$ arbitrarily well such that:
\begin{equation}
	R(f^{*}_{\mathcal{L}_{\text{Train}}}) \geq R_B \geq \hat{R}(f^{*}_{\mathcal{L}_{\text{Train}}},\mathcal{L}_{\text{Train}}) \geq 0.
\end{equation}
When $f$ is too simple then it is said to \textcolor{RoyalBlue}{underfit} the data, whereas it said to \textcolor{RoyalBlue}{overfit} it when it is too complex. As a result, one wants both the expected risk $R$ and the empirical risk $\hat{R}$ minimizers to be as low as possible. To achieve this we can again evaluate the perfomance of $f^{*}_{\mathcal{L}_{\text{Train}}}$ by computing the empirical risk defined in Eq. \ref{eq:empirical_risk} on a separate independent dataset known as the \textcolor{RoyalBlue}{testing set} $\mathcal{L}_{\text{Test}}$. Note however that this quantity should be used for model evaluation purposes only, and not for model selection ones, which is usually done through a separate dataset called the \textcolor{RoyalBlue}{validation set}. 

So far we have defined the concepts of expected risk and empirical risk with respect to a loss function $\ell$, however, we have not yet seen how this loss function looks like in practice. In SL $\ell$ changes based on the characteristics of $Y$. This allows us to distinguish between two different SL problems: \textcolor{RoyalBlue}{classification} and \textcolor{RoyalBlue}{regression}. In the first case $\mathcal{Y}$ comes in the form of a finite set of classes $\{c_1, c_2,...,c_i\}$, whereas in the latter case $\mathcal{Y}=\mathds{R}$. For classification the arguably most straightforward loss function is the $0-1$ loss defined as 
\begin{equation}
	\ell(f(\vec{x},y)) = \mathbb{1}(f(\vec{x}\neq y)),
\end{equation}
while for regression problems $\ell$ can either come in the form of the squared error loss:
\begin{equation}
	\ell(f(\vec{x},y)=(y-f(\vec{x})))^2
\end{equation}
or in the form of the absolute error loss
\begin{equation}
	\ell(f(\vec{x},y))=|y-f(\vec{x})|,
\end{equation}
depending on how much one wants to penalize the errors made by $f$.

While several SL algorithms adopting empirical risk minimization principles exist, throughout this dissertation we will only focus on artificial neural networks, a family of techniques that will be reviewed hereafter.  

\section{Neural Networks}
\label{sec:neural_networks}

\subsection{Multilayer Perceptrons}
\label{sec:general_architecture}
The first mathematical model developed with the intention of mimicking the biological processes underlying the human brain was proposed by \citet{rosenblatt1958perceptron}. Inspired by the work of \citet{mcculloch1943logical}, Rosenblatt developed the \textcolor{RoyalBlue}{perceptron}, the simplest form of artificial neural network able of tackling supervised learning binary classification problems. Given an input vector $\vec{x}$ the perceptron produces the following output:
\begin{equation}
	f(\vec{x}) = 
		\begin{cases}
			1  & \text{if} \sum_i w_i x_i + b \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
\end{equation}
which is given by summing up each input $x$ with a certain weight $w$ and a final additional bias term $b$. The result of this sum is then passed through the sign non-linear activation function which yields output $h$:
\begin{equation}
	\text{sign}(x) = 
		\begin{cases}
			1  & \text{if} \: x \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
\end{equation}
The way the perceptron works is visually represented in Fig. \ref{fig:perceptron} and can be summarized as follows:
\begin{equation}
	f(\vec{x}) = \text{sign}(\sum_i w_i x_i + b).
	\label{eq:perceptron_equation}
\end{equation}

\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/perceptron.tex}
\caption{A visualization of a perceptron.}
\label{fig:perceptron}
\end{figure}
Interestingly, Eq. \ref{eq:perceptron_equation} can also be rewritten in terms of tensor operations. This allows us to express the perceptron classification rule as 
\begin{equation}
	f(\vec{x}) = \text{sign}(\vec{w}^\intercal \vec{x} + b),
	\label{eq:tensor_perceptron}
\end{equation}
and to conveniently visualize its mathematical operations through a \textcolor{RoyalBlue}{computational graph}, a directed graph where each node represents a certain mathematical operation. The computational graph of Eq. \ref{eq:tensor_perceptron} is represented in Fig. \ref{fig:computational_graph_0} and can be considered as the main building block of artificial neural networks.
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_0.tex}
	\caption{The computational graph representing the mathematical operations performed by the perceptron represented in Fig. \ref{fig:perceptron} and defined by Eq. \ref{eq:tensor_perceptron}.}
\label{fig:computational_graph_0}
\end{figure}

Eq. \ref{eq:tensor_perceptron} summarizes the computations that are performed by one single input where $\vec{x}\in\mathds{R}^{p}$, $\vec{w}\in\mathds{R}^{p}$ and $b\in\mathds{R}$. However, the computation capabilities of such a single unit are very limited and can rarely be adopted to solve complex tasks. To overcome this, one can stack several units in parallel such that they create a layer with $q$ outputs defined as:
\begin{equation}
	\vec{h} = \sigma(\vec{W}^\intercal\vec{x}+\vec{b}),
\end{equation}
where $\vec{h}\in\mathds{R}^{q},\vec{x}\in\mathds{R}^{p},\vec{W}\in\mathds{R}^{p\times q},b\in\mathds{R}^{q}$. To increase the flexibility and capabilities of the model even further, one can then compose a sequence of $L$ layers
\begin{equation}
	\begin{split}
		\vec{h}_0 & = \vec{x} \\ 
		\vec{h}_1 & = \sigma(\vec{W}^{\intercal}_{1}\vec{h}_0 + \vec{b}_1) \\ 
	... \\
		\vec{h}_L & = \sigma(\vec{W}^{\intercal}_{L}\vec{h}_{L-1}+\vec{b}_{L})
	\end{split}
\end{equation}
and define a \textcolor{RoyalBlue}{multilayer perceptron} (MLP), also known as feedforward neural network. From now on we will refer to an MLP as $f(\vec{x};\theta)$ where $\theta=\{\vec{W}_k,\vec{b}_k,...|k=1,...,L\}$. 

Now that we defined the mathematical computations that are performed by a feedforward neural network we move on to explaining how one can train these kind of models to perform empirical risk minimization.

\subsection{Stochastic Gradient Descent}
\label{sec:sgd}

Training a neural network consists in finding parameters $\theta$ such that a loss function $\mathscr{L}(\theta)$, also denoted as the \textcolor{RoyalBlue}{objective function}, is minimized. Such loss functions are typically expressed as a sum of the losses $\ell_n$ incurred by each sample $n$ in a training set of size $N$, and can be expressed in the following form:
\begin{equation}
	\mathscr{L}(\theta) = \sum_{n=1}^{N}\ell_n(\theta).
	\label{eq:sum_of_losses}
\end{equation}
When neural networks are used, $\mathscr{L}$ has to be differentiable as this allows to minimize it through first order optimization algorithms. Among such methods, the arguably most straightforward one is gradient descent, which updates the parameters $\theta$ proportionally to the negative gradient of $\mathscr{L}$. This is done by applying the following update rule:
\begin{equation}
	\begin{split}
	\theta_{t+1} & = \theta_t - \gamma_t(\nabla\ell(\theta_t))^{\intercal} \\ 
	& = \theta_t - \gamma_t \sum_{n=1}^{N}(\nabla \ell_n(\theta_t))^{\intercal},
	\end{split}
	\label{eq:gradient_descent}
\end{equation}
where $t$ is a time counter variable, and $\gamma\geq0$ is the learning rate, sometimes also denoted as the step-size parameter. We can easily observe that computing Eq. \ref{eq:gradient_descent} can become computationally very expensive as it requires to evaluate gradients from all individual functions $\ell_n$. This property is in fact what defines gradient descent as a batch optimization method, which makes it unfortunately unsuitable for dealing with large datasets. A possible solution to this computational burden consists in reducing the amount of computation required by the sum in Eq.\ref{eq:gradient_descent} by simply considering a small, random batch of samples of the training set. In the extreme case, one can even just estimate the gradient on one single, randomly chosen, training sample, which is a method called Stochastic Gradient Descent (SGD). While it is true that this approach gives an unbiased estimate of the true gradient, its estimate can also be very noisy, which is the reason behind why it is preferable to evaluate the gradient for a mini-batch of samples instead than on one single, unique sample. A large body of work has investigated the effect that the batch-size has on neural network training \cite{keskar2016large,radiuk2017impact,kandel2020effect}; however, so far no exact rule for determining an optimal batch-size exists. Yet, provided that enough computational resources are available, large mini-batches are usually preferred as they will result in more accurate estimates of the gradient, and therefore reduce the variance in the parameter update $\theta_{t+1}$. 

When the optimization surface is made of valley floors, gradient descent has the limitation of being very slow. To deal with such issue, several works have designed optimization strategies which make gradient based optimization faster and more efficient. The most straightforward improvement to the gradient descent algorithm is the one proposed by \citet{rumelhart1986learning} who suggested to use of an additional term in the update rule presented in Eq.\ref{eq:gradient_descent}, named \textcolor{RoyalBlue}{momentum}. This term, simply keeps track of what happened when the parameters were updated at $t-1$ and determines the next parameters' update as a linear combination between the current and previous gradients. This results in the following update rule:
\begin{equation}
	\theta_{t+1} = \theta_t - \gamma_t ((\nabla \ell(\theta_t)))^{\intercal} + \alpha\Delta\theta_t
	\label{eq: momentum}
\end{equation}
where  
\begin{equation}
	\begin{split}
	\theta_{t} & = \theta_t - \theta_{t-1} \\ 
		   & = \alpha\Delta\theta_{t-1}-\gamma_{t-1}(\nabla \ell(\theta_{t-1}))^{\intercal},
	\end{split}
\label{eq:gradient_descent}
\end{equation}
and $\alpha\in[0,1]$, which accelerates the optimization process and allows the algorithm average out noisy estimates of the gradient.

Next to adding a momentum term to improve the performance of gradient descent, another common method that can accelerate its convergence revolves around dynamically adapting the learning rate parameter $\gamma$. Popular neural network optimizers such as \texttt{RMSProp} \cite{tieleman2012lecture}, \texttt{AdaGrad} \cite{duchi2011adaptive}, and the very well-known \texttt{Adam} optimizer \cite{kingma2014adam}, all adapt this method. While discussing these algorithms into detail is out of the scope of this thesis, we refer the reader to the work of \citet{ruder2016overview}, which provides a nice overview of the most common gradient descent optimization algorithms, and to the work of \citet{schmidt2020descending} who empirically evaluate their performance across different networks and machine learning problems. 

Before ending this section it is worth noting that next to SGD-like methods, there also exist several alternative algorithms that can be used for optimization problems. Among such methods, we mention second order optimization techniques such as Newton, Quasi-Newton and the Conjugate gradient methods discussed in \cite{tan2019review}. While these algorithms are able to minimize the empirical risk faster and even better than SGD, they do not result in equally good generalization performance. Recall from Sec. \ref{sec:learning_from_data}, that in SL minimizing the expected risk is just as important as minimizing the empirical risk, which is a property that the aforementioned second order optimization algorithms do not have. This key result, first presented by \citet{bottou201113}, is what motivates the use of SGD-like optimizers in deep learning.       

\subsection{Backpropagation}
\label{sec:backprop}

From Eq.\ref{eq:gradient_descent} we can note that a crucial role in the optimization process is played by the gradient $\nabla\ell(\theta)$. As we have seen in Sec. \ref{sec:general_architecture} neural networks can be considered as a composition of nested functions $k$ for $k=0,...,K-1$, where each function comes with its own parameters $\theta_k$. Therefore the gradient comes in the form of a vector which contains all the partial derivatives of the loss $\ell$ with respect to the weights $\theta$ that parametrize the neural network:
\begin{equation}
	\nabla\ell(\theta) = \Big[\frac{\partial\ell}{\partial\theta_0}(\theta),...,\frac{\partial\ell}{\partial\theta_{K-1}}(\theta)\Big].
\end{equation}
As the number of functions increases, so does the complexity of the gradient, therefore an efficient way of calculating it is necessary. The backpropagation algorithm is a special case of a more general technique, called \textcolor{RoyalBlue}{automatic differentiation}, that allows to evaluate the gradient of complicated functions numerically and automatically. This is done by exploiting the chain rule, which can be applied recursively on the computation graph that keeps track of all the arithmetic operations that are performed by the network. 

To this end let us define a simplified version of a two hidden layer perceptron $f$ that is parametrized with weight matrices $\vec{W}_1$ and $\vec{W}_2$. When given input data $\vec{x}$ the network produces a prediction $\hat{y}$ which results from traversing the computational graph represented in Fig. \ref{fig:computational_graph_1}.  
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_1.tex}
	\caption{The computational graph representing a simplified version of a multi-layer perceptron with one hidden layer. Note that no bias term is added after multiplying $\vec{x}$ and $\vec{h1}$ by $\vec{W_1}$ and $\vec{W}_2$ respectively.}
\label{fig:computational_graph_1}
\end{figure}
During the traversal, also known as the \textcolor{RoyalBlue}{forward pass}, the result of each mathematical operation is stored within its own output variable $u$ (see Fig. \ref{fig:computational_graph_2}) 
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_2.tex}
	\caption{The computational graph that results after having performed one forward pass through the network. We can see that the result of each mathematical operation is stored within a new node $\vec{u}$ that will be necessary for computing the partial derivatives required to perform stochastic gradient descent.}
\label{fig:computational_graph_2}
\end{figure}
Having such an annotated graph it is now possible to compute all partial derivatives efficiently by traversing the graph backwards (\textcolor{RoyalBlue}{backward pass}), and by applying the chain rule which in its general form states that:
\begin{equation}
	\frac{\text{d} \ell}{\text{d} \theta_i} = \sum_{k\in\text{parents}(\ell)} \frac{\partial \ell}{\partial u_k} \frac{\partial u_k}{\partial \theta_i}.
\end{equation}
Therefore taking as example $\vec{W}_1$, the derivative of the network's output $\hat{y}$ with respect to this weight matrix is given by:
\label{eq:general_chain_rule}
\begin{equation}
	\begin{split}
		\frac{\text{d} \hat{y}}{\text{d} \vec{W}_1} & = \frac{\partial \hat{y}}{\partial u_2} \frac{\partial u_2}{\partial h_1} \frac{\partial h_1}{\partial u_1} \frac{\partial u_1}{\partial\vec{W}_1} \\
	& = \frac{\partial\sigma(u_2)}{\partial u_2} \frac{\partial\vec{W}_2^{\intercal}h_1}{\partial h_1} \frac{\partial \sigma(u_1)}{\partial u_1} \frac{\partial \vec{W}_1^{\intercal}\vec{x}}{\partial{\vec{W}_1}}.
	\end{split}
	\label{eq:applied_chain_rule}
\end{equation}

\subsection{Loss Functions}
\label{sec:loss_functions}
Defining an appropriate loss function is a task that has important practical implications when it comes to the design of the neural architecture. Just like for any other type of machine learning model, the choice of which loss function to minimize depends from the SL task we would like to solve. Fortunately, since neural networks are parametric models, their loss functions are not too different from the ones that are typically used by e.g.,  linear models such as . The most important concept underlying the loss functions used by neural networks is that of \textcolor{RoyalBlue}{maximum likelihood estimation}. As many other parametric models, neural networks implicitly define a distribution $p(Y|\vec{x};\theta)$. This is convenient as it makes it possible to exploit the cross-entropy between the training data and the model's predictions. Therefore, no matter whether we are dealing with a classification problem or a regression one, the loss function that will be adopted by a neural network will always come in the following general form:
\begin{equation}
	\mathscr{L}(\theta) = - \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \log p_\text{model} (Y|\vec{x}).
	\label{eq:maximum_likelihood}
\end{equation}

Typical loss functions that derive from Eq. \ref{eq:maximum_likelihood} (see Chapter 5 of \cite{goodfellow2016deep} for the exact derivations) are the mean squared error (MSE) loss
\begin{equation}
	\mathscr{L}(\theta) = \frac{1}{2}\mathds{E}_{(\vec{x},y\sim P(X,Y))} ||y - f(\vec{x};\theta) ||^{2}
	\label{eq:mean_squared_error}
\end{equation}
which is used for tackling regression problems, and the categorical cross-entropy loss
\begin{equation}
	\mathscr{L}(\theta) = - \mathds{E}_{(\vec{x},y\sim P(X,Y))} \sum_{i=1}^{C} y^{i} \log f(\vec{x};\theta)
	\label{eq:cross_entropy}
\end{equation}
which is used for multi-class classification problems, where $C$ is the number of classes we would like to classify. 

Based on whether Eq. \ref{eq:mean_squared_error} or Eq. \ref{eq:cross_entropy} is minimized, the final layer of a neural network comes in different forms. As the goal of a regression problem is to predict a single numerical value, it follows that the final layer simply consists of one individual unit that is necessary for estimating $\mathcal{Y}\in\mathds{R}$. One single output unit is also used for binary classification problems, where it is combined with the sigmoid activation function
\begin{equation}
	\sigma(x) = \frac{1}{1+\exp(-1)}
	\label{eq:sigmoid}
\end{equation}
which allows to model a Bernoulli distribution over a binary variable. For classification problems, where $C>2$, and the goal is to represent the distribution over a discrete variable that can have $C$ possible values, the sigmoid function can be generalized to a softmax function by producing a vector for $i=1,...,C$ such that:
\begin{equation}
	\text{Softmax}(\vec{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^{C} \exp(z_j)}.
	\label{eq:softmax}
\end{equation}
We can observe that Eq. \ref{eq:softmax} makes the log probabilities, typically estimated by the second-last layer of a network, positive and sum up to one, therefore successfully modeling a multinoulli distribution. While the aforementioned output layers are arguably the most popular it is worth noting that several other types of output layers exist. Since throughout this dissertation none of these additional layers will be used in practice we will not describe them here and refer the reader to Chapter 6 of \cite{goodfellow2016deep} for more information about this topic.

\subsection{Vanishing Gradients and Activation Functions}
\label{sec:activation_functions}

A typical problem of neural networks that come with many hidden layers is given by vanishing gradients. Recall from Sec. \ref{sec:backprop} that in order to perform SGD we first need to collect all the partial derivatives of the network's output with respect to its parameters. As we do this by applying the chain rule this can have the drawback of making the gradient decrease exponentially with respect to the depth of the network. As a result deeper layers can become particularly hard to train, since no information necessary for updating the respective weights will be contained within the gradient. The most common cause of this problem is given by the activation function that is used for introducing non-linearity across the network. For example, let us consider the sigmoid function presented in Eq. \ref{eq:sigmoid} and its derivative which comes in the following form:
\begin{equation}
	\frac{\text{d}\sigma}{\text{d}x}(x) = \sigma(x)(1-\sigma(x)).
	\label{eq:derivative_sigmoid}
\end{equation}
As we can see from the first image of Fig. \ref{fig:activation_functions}, the maximum value of Eq. \ref{eq:derivative_sigmoid} is 0.25. If we then use this value when adopting the chain rule as done in Eq. \ref{eq:applied_chain_rule}, and assume the network comes with a large number of hidden layers, it is easy to see that the gradient $\frac{\text{d}\hat{y}}{\text{d}\vec{W}_1}$ will shrink to zero as the number of layers increases. The sigmoid function is not the only activation function which suffers from this phenomenon, which is also not restricted to feedforward neural networks. In fact as first presented by \citet{} another non-linear activation function suffering from the vanishing gradient problem is the hyperbolic tangent 
\begin{equation}
	\text{tanh}(x) = \frac{1-\exp(-2x)}{1+\exp(-2x)}.
\end{equation}
As can be seen in the second plot of Fig. \ref{fig:activation_functions} the tanh is very similar in shape to the sigmoid. This activation function is largely used within Recurrent Neural Networks (RNNs), a particular type of neural network that can be unfolded into very deep MLPs. For many years its vanishing gradient issues have questioned whether RNNs could be trained and used in practice, a problem which has been successfully solved with the introduction of the Long Short Term Memory (LSTM) cells \cite{hochreiter1997long}. 

\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/activation_functions.tex}
	\caption{In the left plot a visualization of the vanishing gradient problem that can come from using a sigmoid non-linear activation function throughout a network. In the right plot a representation of typical non-linear activation functions within the $[-3,3]$ range that are currently used by popular neural architectures.}
\label{fig:activation_functions}
\end{figure}

Another solution to the vanishing gradient problem is to use the Rectified Linear Unit (ReLU) activation function (represented in green in Fig. \ref{fig:activation_functions}), which is arguably the most popular choice when it comes to the design of deep neural networks. This activation function is simply defined as
\begin{equation}
	\text{ReLU}(x) = \max(0,x).
\end{equation}
Its derivative has the appealing property of staying constant to 1 whenever a unit is activated as defined by: 
\begin{equation}
	\frac{\text{d}}{\text{d}x} \text{ReLU}(x) = \begin{cases} 0 & \text{if} x \seq 0 \\ 1 & \text{otherwise} \end{cases}
\end{equation}

A potential drawback of ReLU is that whenever its input is negative gradient based methods could not be used for learning, as the unit will have a value of 0. To overcome this several activation functions that generalize the ReLU to negative inputs have been proposed within the literature \cite{clevert2015fast, maas2013rectifier, he2015delving}, among which we mention the Elu \cite{clevert2015fast} that is visually represented in red in the last plot of Fig. \ref{fig:activation_functions}. 


\section{Convolutional Neural Networks}
\label{sec:convolutional_networks}

Convolutional Neural Networks (CNNs) are a family of artificial neural networks that are particularly well suited for problems involving high-dimensional inputs such as images or videos. This kind of data in fact prohibits the use of the multi-layer perceptrons presented in Sec. \ref{sec:general_architecture}, as it requires to represent images as unstructured vectors, which is a process that for obvious computational reasons is not feasible. Furthermore MLPs present some additional limitations: first and foremost, due to their fully connected structure, they do not involve any sort of parameter sharing across the network. Second, as the output of each unit in a layer is given as input to all the units in the subsequent layer, the interaction among all such neurons is also extremely dense. CNNs address these limitations by exploiting \textcolor{RoyalBlue}{sparse weight sharing} strategies that result into neural networks that are significantly more memory and computationally efficient. 

\subsection{Mathematical Operations}
\label{sec:operations}

As their name suggests, the key mathematical operation behind CNNs is that of \textcolor{RoyalBlue}{convolution}. A convolution operation is performed over two arguments: an input vector $\vec{x}\in\mathds{R}^{W}$, and a kernel $\vec{u}\in\mathds{R}^{w}$. Its output is a new vector of size $W-w+1$ such that:
\begin{equation}
	(\vec{x}\circledast\vec{u}[i]) = \sum_{m=0}^{w-1}x_{m+i}u_m,
	\label{eq:convolution}
\end{equation}
where $\circledast$ technically denotes the cross-correlation operation, namely a convolution operation that does not flip the kernel. The process described in Eq. \ref{eq:convolution} can easily be generalized to multi-dimensional tensors such as images which can in fact be seen as three-dimensional tensors $\vec{x}\in\mathds{R}^{C,W,H}$, of width and height $W$ and $H$ respectively, defined over the RGB color domain ($C=3$). Similarly one can also define a three-dimensional kernel $\vec{u}\in\mathds{R}^{C,w,h}$ whose purpose is to slide over the input tensor $\vec{x}$ and which yields a two-dimensional output tensor $\vec{o}$ of size $(H-h+1)\times(W-1+1)$ that is computed as follows:
\begin{equation}
	\begin{split}
		\vec{o}_{i,j} & = \vec{b}_{i,j} + \sum_{c=0}^{C-1}(\vec{x}_c\circledast\vec{u}_c)[j,i] \\ 
			      & = \vec{b}_{i,j} + \sum_{c=0}^{C-1} \sum_{n=0}^{h-1}\sum_{m=0}^{w-1} \vec{x}_{c,n+j,m+i}\vec{u}_{c,n,m},
	\end{splot}
\end{equation}
where $\vec{b}$ and $\vec{u}$ are learnable parameters. Within the deep learning literature, $\vec{o}$ is also referred to as a \textcolor{RoyalBlue}{feature map} \cite{goodfellow2016deep}.

Note that by adopting a convolution approach, one input unit in the network only affects as many output units as defined by the size of the kernel, which improves the computational efficiency of the network greatly. Furthermore, each member of the kernel is used across the entire image, which means that the parameters that define a convolution operation are shared alongside the different locations that are visited by $\vec{u}$. The way the kernel interacts with its respective tensor is usually defined by two additional components that both play an important role in the design of convolutional networks. The first of these components is \textcolor{RoyalBlue}{padding} which is a technique that adds some extra values around the perimeter of the input tensor $\vec{x}$, with the aim of preserving the information that is depicted around its corners. Second, there is the concept of \textcolor{RoyalBlue}{strides} which defines by how many elements at a time we wish to slide $\vec{u}$ over $\vec{x}$. As the goal of CNNs is that of downsampling the input tensor in a computationally efficient manner, it is usually good practice to have strides larger than one, albeit this comes at the cost of extracting features less thoroughly. 

Convolutional networks typically perform several convolutions in parallel, as multiple kernels are used. The output of each convolution is then passed through a non linear activation function such as the ones that we represented in Fig. \ref{fig:activation_functions}. To downsample the resulting feature maps even further, a \textcolor{RoyalBlue}{pooling} function is usually adopted. Its idea is to summarize the output of the convolving process at a certain location of the feature map through a summary statistic. This reduces its size while at the same time preserves the presence of the detected features. There are two common pooling operations one can choose from: max-pooling \cite{zhou1988computation}, which given a three dimensional tensor $\vec{x}\in\mathds{R}^{C\times(rh)\times(sw)}$ produces a tensor $\vec{o}\in\mathds{R}^{C\times r\times s}$ by simply keeping the maximum value of a feature map within a certain rectangular neighborhood such that 
\begin{equation}
	\vec{o}_{c,j,i} = \underset{n<h,m<w}{\max} \vec{x}_{c,rj+n,si+m},
\end{equation}
and average pooling, which instead computes the mean of a feature map such that 
\begin{equation}
	\vec{o}_{c,j,i} = \frac{1}{hw} \sum_{n=0}^{h-1}\sum_{m=0}^{w-1} \vec{x}_{c,rj+n,si+m}.
\end{equation}
Besides reducing the size of a feature map, pooling operations have also the important benefit of making the representations learned by the network invariant to small translations. In fact, one could translate the input by a small amount and still obtain the same output after pooling. Note however, that albeit desirable in most cases, there are situations where adopting pooling strategies should be avoided \cite{sabatelli2018learning, bidoiadeep}.  


\subsection{Popular Architectures}
\label{sec:architectures}

Mention general structure here


\section{Conclusion}
\label{sec:conclusion01}
