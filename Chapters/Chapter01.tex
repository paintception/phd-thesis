\chapter{Supervised Learning and Deep Neural Networks}
\label{ch:supervised_learning}

\begin{remark}{Outline}
	In this first chapter, we present Supervised Learning (SL), a branch of machine learning that aims to create statistical models that, given a set of previously collected input-output observations, can predict the value of new unseen output variables. Throughout the chapter, as well as during this dissertation, we will focus on how one can represent such models through artificial neural networks, a family of algorithms that, over the last decade, has gained tremendous popularity within the artificial intelligence community. We start by providing a general overview of SL in Sec. \ref{sec:introduction01} where we describe the main ideas behind this machine learning paradigm before characterizing it from a more mathematical perspective in Sec. \ref{sec:learning_from_data}. We then move on towards presenting artificial neural networks in Sec. \ref{sec:neural_networks} where we will describe how these kinds of algorithms can be used for solving SL problems, as well as how these models are trained and designed. We then present convolutional neural networks, a particular type of artificial neural network that is particularly well suited for dealing with SL problems with high-dimensional and spatially organized inputs. We will do this in Sec. \ref{sec:convolutional_networks}, before ending the chapter with some concluding remarks in Sec. \ref{sec:conclusion01}.   
\end{remark}

\section{Introduction}
\label{sec:introduction01}

Today's modern society is surrounded by technological services that aim at exploiting the power of \textcolor{RoyalBlue}{data}. It is in fact not a hard task to think of newspapers, or companies, referring to data-related terms like "Big Data" or "Internet of Things" in the context of Artificial Intelligence (AI). While their resulting articles and products are not always scientifically accurate, nor necessarily useful, it comes without a doubt that they nevertheless do focus on one critical component of today's AI toolbox. Data plays indeed a crucial role  nowadays in the development of AI services, and many AI-based solutions, ranging from recommendation systems underlying our favorite streaming services to online language translators that allow us to easily translate the most exotic of the languages to our mother tongue, would not be as effective without data. Yet, what does it mean to build an AI system based on data, and why does data play such an important role? In this chapter we will answer these questions by focusing on supervised learning, a branch of machine learning that aims at developing statistical models that are able of capturing relationships within structured datasets that come in the form of input-output pairs. The general idea underlying supervised learning algorithms is that there is a very precise and defined process governing the generation of data, which if discovered, can result in the development of successful AI-based applications. However, correctly identifying such data generation process can be particularly challenging. We will now see how one can approach this difficult, yet exciting, problem.    

\section{Statistical Learning}
\label{sec:learning_from_data}

We start by defining a Supervised Learning (SL) problem with a quadruplet containing the following elements \cite{friedman2001elements, louppe2014understanding}:
\begin{itemize}
	\item An input space $\mathcal{X}$,
	\item An output space $\mathcal{Y}$,
	\item A joint probability distribution $P(X,Y)$.
	\item A loss function $\ell(\mathcal{Y} \times \mathcal{Y}) \rightarrow \Re$
\end{itemize}
Let us define with $\mathcal{F}$ the set of all functions $f$ that a certain learning algorithm can produce. In SL the main goal is to find a function $f:\mathcal{X}\rightarrow\mathcal{Y} \in \mathcal{F}$ that minimizes the expectation over $P(X,Y)$ of a certain loss $\ell$, based on the predictions made by $f$ and the correct outputs defined in $\mathcal{Y}$.

This expectation is also known as the \textcolor{RoyalBlue}{expected risk}, or generalization error, and is defined as:
\begin{equation}
	R(f) = \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \big[\ell(y,f(\vec{x}))\big],
\label{eq:expected_risk}
\end{equation}
where $f$ is built from a limited set of observations that define the SL problem we would like to solve. Such observations constitute the \textcolor{RoyalBlue}{learning set} $\mathcal{L}$ which is defined by $N$ pairs of input vectors and output values $(\vec{x}_1, y_1),...,(\vec{x}_N, y_N)$ where $\vec{x}_i \in \mathcal{X}$ and $y_i \in \mathcal{Y}$ are i.i.d. drawn from $P(X,Y)$ \cite{geurts2002contributions}. 

As $P(X,Y)$ is unknown and $\mathcal{L}$ is finite, one cannot evaluate the quantity defined in Eq. \ref{eq:expected_risk}, however, one can compute an estimate of it instead. To this end it is common to use part of the learning set $\mathcal{L}$ for constructing a \textcolor{RoyalBlue}{training set} $\mathcal{L}_{\text{Train}}$, of size $M$, that can be used for computing the \textcolor{RoyalBlue}{empirical risk}, or training error, as follows:
\begin{equation}
	\hat{R}(f,\mathcal{L}_\text{Train}) = \frac{1}{M} \sum_{(\vec{x}_i, y_i)\in \mathcal{L}_{\text{Train}}} \ell(y_i,f(\vec{x}_i)).
\label{eq:empirical_risk}
\end{equation}
Computing Eq. \ref{eq:empirical_risk} results in an unbiased estimate that can be used for finding a good approximation of the optimal function $f^{*}$ that minimizes Eq. \ref{eq:expected_risk}. Formally this corresponds to satisfying the equality
\begin{equation}
	f^{*}_{\mathcal{L}_{\text{Train}}} = \underset{f\in\mathcal{F}}{\argmin}\hat{R}(f,\mathcal{L}_{\text{Train}}),
\end{equation}
which is known as the empirical risk minimization principle \cite{vapnik1992principles}. As mentioned by \citet{vapnik2015uniform} empirical risk minimizers should converge in the limit to optimal models:
\begin{equation}
	\lim_{N \to \infty} f^{*}_{\mathcal{L}_\text{Train}} = f^{*}.
\end{equation}
With these concepts in place, we can summarize the goal of SL as finding a function $f$ that, on average, makes good predictions over $P(X,Y)$. To this end, when explaining the data generating process underlying $P(X,Y)$, $f$ does not have to be too `simple" nor too `complex". In order to assess this, let $\mathcal{Y}^{\mathcal{X}}$ be the set of all functions $f:\mathcal{X}\rightarrow\mathcal{Y}$. The minimal expected risk over all these functions is defined as
\begin{equation}
	R_B = \underset{f\in\mathcal{Y}^{\mathcal{X}}}{\min} R(f),
	\label{eq:bayes_risk}
\end{equation}
and is called the \textcolor{RoyalBlue}{Bayes risk}. When minimized, the quantity defined in Eq. \ref{eq:bayes_risk} results in the best possible function $f_B$, which is called the Bayes model. If the capacity of the hypothesis space $\mathcal{F}$ chosen for finding $f$ is too low, then it follows that $R(f)-R_B$ will be large for any $f\in\mathcal{F}$, including $f^{*}$ and $f^{*}_{\mathcal{L}_{\text{Train}}}$. Similarly, if the capacity of $\mathcal{F}$ is too high then although $R(f)-R_B$ will be small, $f^{*}_{\mathcal{L}_\text{Train}}$ can fit 
$\mathcal{L}_{\text{Train}}$ arbitrarily well such that:
\begin{equation}
	R(f^{*}_{\mathcal{L}_{\text{Train}}}) \geq R_B \geq \hat{R}(f^{*}_{\mathcal{L}_{\text{Train}}},\mathcal{L}_{\text{Train}}) \geq 0.
\end{equation}
When $f$ is too simple, then it is said to \textcolor{RoyalBlue}{underfit} the data, whereas it said to \textcolor{RoyalBlue}{overfit} it when it is too complex. As a result, one wants both the expected risk $R$ and the empirical risk $\hat{R}$ minimizers to be as low as possible. To achieve this we can again evaluate the performance of $f^{*}_{\mathcal{L}_{\text{Train}}}$ by computing the empirical risk defined in Eq. \ref{eq:empirical_risk} on a separate independent dataset known as the \textcolor{RoyalBlue}{testing set} $\mathcal{L}_{\text{Test}}$. Note, however, that this quantity should be used for model evaluation purposes only and not for model selection ones as this would lead to some bias, which is usually done through a separate dataset called the \textcolor{RoyalBlue}{validation set}. 

So far we have defined the concepts of expected risk and empirical risk with respect to a loss function $\ell$. However, we have not yet seen what this loss function looks like in practice. In SL $\ell$ changes based on the characteristics of $Y$. This allows us to distinguish between two different SL problems: \textcolor{RoyalBlue}{classification} and \textcolor{RoyalBlue}{regression}. In the first case $\mathcal{Y}$ comes in the form of a finite set of classes $\{c_1, c_2,...,c_i\}$, whereas in the latter case $\mathcal{Y}\in\mathds{R}$. For classification the arguably most straightforward loss function is the $0-1$ loss defined as 
\begin{equation}
	\ell(f(\vec{x}),y)) = \mathbb{1}(f(\vec{x})\neq y)),
\end{equation}
while for regression problems $\ell$ can for example come in the form of the squared error loss:
\begin{equation}
	\ell(f(\vec{x}),y))=(y-f(\vec{x}))^2
\end{equation}
or in the form of the absolute error loss
\begin{equation}
	\ell(f(\vec{x},y))=|y-f(\vec{x})|,
\end{equation}
depending on how much one wants to penalize large errors made by $f$.

While several SL algorithms adopting the empirical risk minimization principle exist, throughout this dissertation, we will only focus on artificial neural networks, a family of techniques that will be reviewed hereafter.  

\section{Neural Networks}
\label{sec:neural_networks}
Artificial neural networks are learning algorithms that have originally been developed with the goal of mimicking the neural interactions within biological systems \cite{mitchell1997machine}. Therefore, their primal intent was, to serve as mathematical models that could be used for better understanding biological learning processes. Despite being motivated by a well-grounded research objective, the most successful algorithms that nowadays fall within the category of artificial neural networks have, however, only been developed with the simple objective of resulting in effective empirical risk minimizers. We, therefore, note that most of the neural architectures that will be presented and studied throughout this dissertation mirror actual existing biological processes only to a very limited extent. Nevertheless, as will be explained in the coming section, they do build on top of ideas that are in line with biologically plausible artificial networks.   

\subsection{Multilayer Perceptrons}
\label{sec:general_architecture}
The first mathematical model developed with the intention of mimicking the biological processes underlying the human brain was proposed by \citet{rosenblatt1958perceptron}. Inspired by the work of \citet{mcculloch1943logical}, Rosenblatt developed the \textcolor{RoyalBlue}{perceptron}, the simplest form of artificial neural network capable of tackling binary classification problems through supervised learning. Given an input vector $\vec{x}$ the perceptron produces the following output:
\begin{equation}
	f(\vec{x}) = 
		\begin{cases}
			1  & \text{if} \sum_i w_i x_i + b \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
\end{equation}
which is given by summing up each input $x$ with a certain weight $w$ and a final additional bias term $b$. The result of this sum is then passed through the sign non-linear activation function which yields output $h$:
\begin{equation}
	\text{sign}(x) = 
		\begin{cases}
			1  & \text{if} \: x \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
\end{equation}
The way the perceptron works is visually represented in Fig. \ref{fig:perceptron} and can be summarized by the following equation
\begin{equation}
	f(\vec{x}) = \text{sign}(\sum_i w_i x_i + b),
	\label{eq:perceptron_equation}
\end{equation}

\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/perceptron.tex}
\caption{A visualization of a perceptron.}
\label{fig:perceptron}
\end{figure}
\noindent which, interestingly, can also be rewritten in terms of tensor operations. This allows us to express the perceptron classification rule as 
\begin{equation}
	f(\vec{x}) = \text{sign}(\vec{w}^\intercal \vec{x} + b).
	\label{eq:tensor_perceptron}
\end{equation}
Eq. \ref{eq:tensor_perceptron} makes it possible to conveniently visualize the mathematical operations of the perceptron through a \textcolor{RoyalBlue}{computational graph}, a directed graph where each node represents a certain mathematical operation. The computational graph of Eq. \ref{eq:tensor_perceptron} is represented in Fig. \ref{fig:computational_graph_0} and can be considered as the main building block of modern artificial neural networks.
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_0.tex}
	\caption{The computational graph representing the mathematical operations performed by the perceptron represented in Fig. \ref{fig:perceptron} and defined by Eq. \ref{eq:tensor_perceptron}.}
\label{fig:computational_graph_0}
\end{figure}

Eq. \ref{eq:tensor_perceptron} summarizes the computations that are performed by one single input where $\vec{x}\in\mathds{R}^{p}$, $\vec{w}\in\mathds{R}^{p}$ and $b\in\mathds{R}$. However, the computation capabilities of such a single unit are very limited and can rarely be adopted to solve complex tasks. To overcome this, one can stack several units in parallel such that they create a layer with $q$ outputs defined as:
\begin{equation}
	\vec{h} = \sigma(\vec{W}^\intercal\vec{x}+\vec{b}),
\end{equation}
where $\vec{h}\in\mathds{R}^{q},\vec{x}\in\mathds{R}^{p},\vec{W}\in\mathds{R}^{p\times q},b\in\mathds{R}^{q}$. To increase the flexibility and capabilities of the model even further, one can then compose a sequence of $L$ layers
\begin{equation}
	\begin{split}
		\vec{h}_0 & = \vec{x} \\ 
		\vec{h}_1 & = \sigma(\vec{W}^{\intercal}_{1}\vec{h}_0 + \vec{b}_1) \\ 
	... \\
		\vec{h}_L & = \sigma(\vec{W}^{\intercal}_{L}\vec{h}_{L-1}+\vec{b}_{L})
	\end{split}
\end{equation}
that define a \textcolor{RoyalBlue}{multilayer perceptron} (MLP), also known as feedforward neural network. From now on we will refer to an MLP as $f(\vec{x};\theta)$ where $\theta=\{\vec{W}_k,\vec{b}_k|k=1,...,L\}$. 

Now that we defined the mathematical computations that are performed by a feedforward neural network we move on to explaining how one can train these kind of models to perform empirical risk minimization.

\subsection{Stochastic Gradient Descent}
\label{sec:sgd}

Training a neural network consists in finding parameters $\theta$ such that a loss function $\mathscr{L}(\theta)$, also denoted as the \textcolor{RoyalBlue}{objective function}, is minimized. Such loss functions are typically expressed as a sum of the losses $\ell_n$ incurred by each sample $n$ in a training set of size $N$, and can be expressed in the following form:
\begin{equation}
	\mathscr{L}(\theta) = \sum_{n=1}^{N}\ell_n(\theta).
	\label{eq:sum_of_losses}
\end{equation}
When neural networks are used, $\mathscr{L}$ has to be differentiable as this allows to minimize it through first-order optimization algorithms. Among such methods, the arguably most straightforward one is gradient descent, which updates the parameters $\theta$ proportionally to the negative gradient of $\mathscr{L}$. This is done by applying the following update rule:
\begin{equation}
	\begin{split}
	\theta_{t+1} & = \theta_t - \eta_t(\nabla\mathscr{L}(\theta_t))^{\intercal} \\ 
	& = \theta_t - \eta_t \sum_{n=1}^{N}(\nabla \ell_n(\theta_t))^{\intercal},
	\end{split}
	\label{eq:gradient_descent_1}
\end{equation}
where $t$ is a time counter variable, and $\eta\geq0$ is the learning rate, sometimes also denoted as the step-size parameter. We can easily observe that computing Eq. \ref{eq:gradient_descent_1} can become computationally very expensive as it requires evaluating gradients from all individual functions $\ell_n$. In fact, this property defines gradient descent as a batch optimization method, which makes it unfortunately unsuitable for dealing with large datasets. A possible solution to this computational burden consists in reducing the amount of computation required by the sum in Eq.\ref{eq:gradient_descent_1} by simply considering a small, random batch of samples of the training set. In extreme cases, one can even just estimate the gradient on one single, randomly chosen training sample, a method called Stochastic Gradient Descent (SGD). While it is true that this approach gives an unbiased estimate of the true gradient, its estimate can also be very noisy, which is the reason why it is preferable to evaluate the gradient for a mini-batch of samples rather than on one single unique sample. A large body of work has investigated the effect that the batch size has on neural network training \cite{keskar2016large,radiuk2017impact,kandel2020effect}; however, so far, no exact rule for determining an optimal batch size exists. Yet, provided that enough computational resources are available, large mini-batches are usually preferred as they will result in more accurate estimates of the gradient, and therefore reduce the variance in the parameter update $\theta_{t+1}$. 

When the optimization surface is made of valley floors, gradient descent has the limitation of being very slow. To deal with this issue, several works have designed optimization strategies which make gradient based optimization faster and more efficient. The most straightforward improvement to the gradient descent algorithm is the one proposed by \citet{rumelhart1986learning} who suggested the use of an additional term in the update rule presented in Eq. \ref{eq:gradient_descent_1}, named \textcolor{RoyalBlue}{momentum}. This term, simply keeps track of what happened when the parameters were updated at $t-1$ and determines the next parameter update as a linear combination between the current and previous gradients. This results in the following update rule:
\begin{equation}
	\theta_{t+1} = \theta_t - \eta_t ((\nabla \ell(\theta_t)))^{\intercal} + \rho\Delta\theta_t
	\label{eq: momentum}
\end{equation}
where  
\begin{equation}
	\begin{split}
	\Delta\theta_{t} & = \theta_t - \theta_{t-1} \\ 
		   & = 
\rho\Delta\theta_{t-1}-\eta_{t-1}(\nabla \ell(\theta_{t-1}))^{\intercal},
	\end{split}
\label{eq:gradient_descent}
\end{equation}
and $\rho\in[0,1]$. Momentum is well-known to overall accelerate the optimization process while also allowing the algorithm to average out noisy estimates of the gradient.

Next to adding a momentum term to improve the performance of gradient descent, another common method that can accelerate its convergence revolves around dynamically adapting the learning rate parameter $\eta$. Popular neural network optimizers such as \texttt{RMSProp} \cite{tieleman2012lecture}, \texttt{AdaGrad} \cite{duchi2011adaptive}, and the very well-known \texttt{Adam} optimizer \cite{kingma2014adam}, all adopt this method. While discussing these algorithms in detail is out of the scope of this thesis, we refer the reader to the work of \citet{ruder2016overview}, which provides a nice overview of the most common gradient descent optimization algorithms, and to the work of \citet{schmidt2020descending} who empirically evaluate their performance across different networks and machine learning problems. 

Before ending this section, it is worth noting that there are several alternative algorithms besides SGD-like methods that can be used for optimization problems. Among such methods, we mention second-order optimization techniques such as Newton, Quasi-Newton, and the Conjugate gradient methods discussed in \cite{tan2019review}. While these algorithms are able to minimize the empirical risk faster and even better than SGD, they do not result in equally good generalization performance. Recall from Sec. \ref{sec:learning_from_data}, that in SL, minimizing the expected risk is just as important as minimizing the empirical risk, which is a property that the aforementioned second-order optimization algorithms do not have. This key result, first presented by \citet{bottou201113}, is what motivates the use of SGD-like optimizers in deep learning.       

\subsection{Backpropagation}
\label{sec:backprop}

From Eq. \ref{eq:gradient_descent} we can note that a crucial role in the optimization process is played by the gradient $\nabla\ell(\theta)$. As we have seen in Sec. \ref{sec:general_architecture} neural networks can be considered as a composition of nested functions $l$ for $k=0,...,L-1$, where each function comes with its own parameters $\theta_k$. Therefore the gradient comes in the form of a vector which contains all the partial derivatives of the loss $\ell$ with respect to the weights $\theta$ that parametrize the neural network:
\begin{equation}
	\nabla\ell(\theta) = \Big[\frac{\partial\ell}{\partial\theta_0}(\theta),...,\frac{\partial\ell}{\partial\theta_{K-1}}(\theta)\Big].
\end{equation}
As the number of functions increases, so does the complexity of the gradient; therefore, an efficient way of calculating it is necessary. The backpropagation algorithm \cite{linnainmaa1970representation,bryson1975applied,rumelhart1986learning} is a special case of a more general technique, called \textcolor{RoyalBlue}{automatic differentiation} (see \cite{baydin2018automatic} for a general review about the topic), that allows evaluating the gradient of complicated functions numerically and automatically. This is done by exploiting the chain rule, which can be applied recursively on the computation graph that keeps track of all the arithmetic operations that are performed by the network. 

To this end, let us define a simplified version of a two hidden layer perceptron $f$ that is parametrized with weight matrices $\vec{W}_1$ and $\vec{W}_2$. When given input data $\vec{x}$ the network produces a prediction $\hat{y}$ which results from traversing the computational graph represented in Fig. \ref{fig:computational_graph_1}.  
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_1.tex}
	\caption{The computational graph representing a simplified version of a multi-layer perceptron with one hidden layer. Note that no bias term is added after multiplying $\vec{x}$ and $\vec{h1}$ by $\vec{W_1}$ and $\vec{W}_2$ respectively.}
\label{fig:computational_graph_1}
\end{figure}
During the traversal, also known as the \textcolor{RoyalBlue}{forward pass}, the result of each mathematical operation is stored within its own output variable $u$ (see Fig. \ref{fig:computational_graph_2}). 
\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/computational_graph_2.tex}
	\caption{The computational graph that results after having performed one forward pass through the network. We can see that the result of each mathematical operation is stored within a new node $\vec{u}$ that will be necessary for computing the partial derivatives required to perform stochastic gradient descent.}
\label{fig:computational_graph_2}
\end{figure}
Having such an annotated graph it is now possible to compute all partial derivatives efficiently by traversing the graph backwards (\textcolor{RoyalBlue}{backward pass} \cite{linnainmaa1970representation}), and by applying the chain rule which in its general form states that:
\begin{equation}
	\frac{\text{d} \ell}{\text{d} \theta_i} = \sum_{k\in\text{parents}(\ell)} \frac{\partial \ell}{\partial u_k} \frac{\partial u_k}{\partial \theta_i}.
\end{equation}
Therefore taking as example $\vec{W}_1$, the derivative of the network's output $\hat{y}$ with respect to this weight matrix is given by:
\label{eq:general_chain_rule}
\begin{equation}
	\begin{split}
		\frac{\text{d} \hat{y}}{\text{d} \vec{W}_1} & = \frac{\partial \hat{y}}{\partial u_2} \frac{\partial u_2}{\partial h_1} \frac{\partial h_1}{\partial u_1} \frac{\partial u_1}{\partial\vec{W}_1} \\
	& = \frac{\partial\sigma(u_2)}{\partial u_2} \frac{\partial\vec{W}_2^{\intercal}h_1}{\partial h_1} \frac{\partial \sigma(u_1)}{\partial u_1} \frac{\partial \vec{W}_1^{\intercal}\vec{x}}{\partial{\vec{W}_1}}.
	\end{split}
	\label{eq:applied_chain_rule}
\end{equation}

\subsection{Loss Functions}
\label{sec:loss_functions}
Defining an appropriate loss function is a task that has important practical implications for the design of the neural architecture. As for any other type of machine learning model, the choice of which loss function to minimize depends on the SL task we would like to solve. Fortunately, since neural networks are parametric models, their loss functions are not too different from those typically used by, e.g.,  linear models such as logistic regression. The most important concept underlying the loss functions used by neural networks is that of \textcolor{RoyalBlue}{maximum likelihood estimation}. As many other parametric models, neural networks implicitly define a distribution $p(Y|\vec{x};\theta)$. This is convenient as it makes it possible to exploit the cross-entropy between the training data and the model's predictions. Therefore, no matter whether we are dealing with a classification problem or a regression one, the loss function that a neural network will adopt will always come in the following general form:
\begin{equation}
	\mathscr{L}(\theta) = - \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \log p_\text{model} (Y|\vec{x}).
	\label{eq:maximum_likelihood}
\end{equation}

Typical loss functions that derive from Eq. \ref{eq:maximum_likelihood} (see Chapter 5 of \cite{goodfellow2016deep} for the exact derivations) are the mean squared error (MSE) loss
\begin{equation}
	\mathscr{L}(\theta) = \frac{1}{2}\mathds{E}_{(\vec{x},y)\sim P(X,Y)} ||y - f(\vec{x};\theta) ||^{2}
	\label{eq:mean_squared_error}
\end{equation}
which is used for tackling regression problems, and the categorical cross-entropy loss
\begin{equation}
	\mathscr{L}(\theta) = - \mathds{E}_{(\vec{x},y)\sim P(X,Y)} \sum_{i=1}^{C} 1(y=i) \log p_\text{model} f_i(\vec{x};\theta)
	\label{eq:cross_entropy}
\end{equation}
which is used for multi-class classification problems, where $C$ is the number of classes we would like to classify. 

Based on whether Eq. \ref{eq:mean_squared_error} or Eq. \ref{eq:cross_entropy} is minimized, the final layer of a neural network comes in different forms. As the goal of a regression problem is to predict a single numerical value, it follows that the final layer simply consists of one individual unit that is necessary for estimating $\mathcal{Y}\in\mathds{R}$. One single output unit is also used for binary classification problems, where it is combined with the sigmoid activation function
\begin{equation}
	\sigma(x) = \frac{1}{1+\exp(-1)}
	\label{eq:sigmoid}
\end{equation}
which allows to model a Bernoulli distribution over a binary variable. For classification problems, where $C>2$, and the goal is to represent the distribution over a discrete variable that can have $C$ possible values, the sigmoid function can be generalized to a softmax function by producing a vector $\vec{z}$ for $i=1,...,C$ such that:
\begin{equation}
	\text{Softmax}(\vec{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^{C} \exp(z_j)}.
	\label{eq:softmax}
\end{equation}
We can observe that Eq. \ref{eq:softmax} makes the log probabilities $z_i$, typically estimated by the second-last layer of a network, positive and sum up to one, therefore successfully modeling a multinoulli distribution. While the aforementioned output layers are arguably among the most popular ones it is worth noting that several other types of output layers exist \cite{goodfellow2013maxout,graves2006connectionist}. Since throughout this dissertation none of these layers will be used in practice, we will not describe them here and refer the reader to Chapter 6 of \cite{goodfellow2016deep} for more information about this topic.

\subsection{Vanishing Gradients and Activation Functions}
\label{sec:activation_functions}

A typical problem of neural networks that come with many hidden layers is vanishing gradients. Recall from Sec. \ref{sec:backprop} that in order to perform SGD, we first need to collect all the partial derivatives of the network output with respect to its parameters. As we do this by applying the chain rule, this can have the drawback of making the gradient decrease exponentially with respect to the depth of the network. As a result, deeper layers can become particularly hard to train since no information necessary for updating the respective weights will be contained within the gradient. The most common cause of this problem is the activation function used for introducing non-linearity across the network. For example, let us consider the sigmoid function presented in Eq. \ref{eq:sigmoid} and its derivative, which comes in the following form:
\begin{equation}
	\frac{\text{d}\sigma}{\text{d}x}(x) = \sigma(x)(1-\sigma(x)).
	\label{eq:derivative_sigmoid}
\end{equation}
As we can see from the first image of Fig. \ref{fig:activation_functions}, the maximum value of Eq. \ref{eq:derivative_sigmoid} is 0.25. If we then use this value when adopting the chain rule as done in Eq. \ref{eq:applied_chain_rule}, and assume the network comes with a large number of hidden layers, it is easy to see that the gradient $\frac{\text{d}\hat{y}}{\text{d}\vec{W}_1}$ will shrink to zero as the number of layers increases. The sigmoid function is not the only activation function which suffers from this phenomenon, which is also not restricted to feedforward neural networks only. In fact as first presented by \citet{hochreiter1997long} another non-linear activation function that suffers from the vanishing gradient problem is the hyperbolic tangent 
\begin{equation}
	\text{tanh}(x) = \frac{1-\exp(-2x)}{1+\exp(-2x)}.
\end{equation}
As can be seen in the second plot of Fig. \ref{fig:activation_functions} the tanh is very similar in shape to the sigmoid. This activation function is largely used within Recurrent Neural Networks (RNNs), a particular type of neural network that can be unfolded into very deep MLPs. Its vanishing gradient issue has put the potential of RNNs into question for many years until it was solved thanks to the introduction of the Long Short Term Memory (LSTM) cells \cite{hochreiter1997long}. 

\begin{figure}[ht!]
	\centering
	\input{Images/Chapter01/activation_functions.tex}
	\caption{In the left plot a visualization of the vanishing gradient problem that can come from using a sigmoid non-linear activation function throughout a network. In the right plot a representation of typical non-linear activation functions within the $[-3,3]$ range that are currently used by popular neural architectures.}
\label{fig:activation_functions}
\end{figure}

Another solution to the vanishing gradient problem is to use the Rectified Linear Unit (ReLU) activation function (represented in green in Fig. \ref{fig:activation_functions}), which is arguably the most popular choice when it comes to the design of deep neural networks. This activation function is simply defined as
\begin{equation}
	\text{ReLU}(x) = \max(0,x).
\end{equation}
Its derivative has the appealing property of staying constant to 1 whenever a unit is activated as defined by: 
\begin{equation}
	\frac{\text{d}}{\text{d}x} \text{ReLU}(x) = \begin{cases} 0 & \text{if } x \leq 0 \\ 1 & \text{otherwise} \end{cases}
\end{equation}

However, a potential drawback of the ReLU is that whenever its input is negative, gradient-based methods could not be used for learning, as the unit will have a value of $0$. To overcome this, several activation functions that generalize the ReLU to negative inputs have been proposed within the literature \cite{clevert2015fast, maas2013rectifier, he2015delving}, among which we mention the Elu \cite{clevert2015fast} that is visually represented in red in the right plot of Fig. \ref{fig:activation_functions}. 


\section{Convolutional Neural Networks}
\label{sec:convolutional_networks}

Convolutional Neural Networks (CNNs) are a family of artificial neural networks that are particularly well suited for problems involving spatially organized inputs such as (2D) images or (3D) videos. This kind of data, in fact prohibits the use of the multi-layer perceptrons presented in Sec. \ref{sec:general_architecture}, as it requires representing images as unstructured vectors, which is a process that, for obvious computational reasons, is not feasible. Furthermore, MLPs present some additional limitations: first and foremost, due to their fully connected structure, they do not involve any sort of parameter sharing across the network. Second, as the output of each unit in a layer is given as input to all the units in the subsequent layer, the interaction among neurons is also extremely dense. CNNs address these limitations by exploiting \textcolor{RoyalBlue}{sparse weight sharing} strategies that result in neural networks that are significantly more memory, computationally and statistically efficient. 

\subsection{Mathematical Operations}
\label{sec:operations}

As their name suggests, the key mathematical operation behind CNNs is that of \textcolor{RoyalBlue}{convolution}. A convolution operation is performed over two arguments: an input vector $\vec{x}\in\mathds{R}^{W}$, and a kernel $\vec{u}\in\mathds{R}^{w}$. Its output is a new vector of size $W-w+1$ such that:
\begin{equation}
	(\vec{x}\circledast\vec{u}[i]) = \sum_{m=0}^{w-1}x_{m+i}u_m,
	\label{eq:convolution}
\end{equation}
where $\circledast$ technically denotes the cross-correlation operation, namely a convolution operation that does not flip the kernel. The process described in Eq. \ref{eq:convolution} can easily be generalized to multi-dimensional tensors such as images which can in fact be seen as three-dimensional tensors $\vec{x}\in\mathds{R}^{C,W,H}$, of width and height $W$ and $H$ respectively, defined over the RGB color domain ($C=3$). Similarly, one can also define a three-dimensional kernel $\vec{u}\in\mathds{R}^{C,w,h}$ whose purpose is to slide over the input tensor $\vec{x}$ and which yields a two-dimensional output tensor $\vec{o}$ of size $(H-h+1)\times(W-w+1)$ that is computed as follows:
\begin{equation}
	\begin{split}
		\vec{o}_{i,j} & = \vec{b}_{i,j} + \sum_{c=0}^{C-1}(\vec{x}_c\circledast\vec{u}_c)[j,i] \\ 
			      & = \vec{b}_{i,j} + \sum_{c=0}^{C-1} \sum_{n=0}^{h-1}\sum_{m=0}^{w-1} \vec{x}_{c,n+j,m+i}\vec{u}_{c,n,m},
	\end{split}
\end{equation}
where $\vec{b}$ and $\vec{u}$ are learnable parameters. Within the deep learning literature, $\vec{o}$ is also referred to as a \textcolor{RoyalBlue}{feature map} \cite{goodfellow2016deep}.

Note that by adopting a convolution approach, one input unit in the network only affects as many output units as defined by the size of the kernel, which improves the computational efficiency of the network greatly. Furthermore, each member of the kernel is used across the entire image, which means that the parameters that define a convolution operation are shared alongside the different locations that are visited by $\vec{u}$. The way the kernel interacts with its respective tensor is usually defined by two additional components that both play an important role in the design of convolutional networks. The first of these components is \textcolor{RoyalBlue}{padding} which is a technique that adds some extra values around the perimeter of the input tensor $\vec{x}$, to preserve the information that is depicted around its corners. Second, there is the concept of \textcolor{RoyalBlue}{strides} which defines by how many elements at a time we wish to slide $\vec{u}$ over $\vec{x}$. As the goal of CNNs is to downsample the input tensor in a computationally efficient manner, it is usually good practice to have strides larger than one, albeit this comes at the cost of extracting features less thoroughly. 

Convolutional networks typically perform several convolutions in parallel, as multiple kernels are used. The output of each convolution is then passed through a non linear activation function such as the ones that we represented in Fig. \ref{fig:activation_functions}. To downsample the resulting feature maps even further, a \textcolor{RoyalBlue}{pooling} function is usually adopted. Its idea is to summarize the output of the convolving process at a certain location of the feature map through a summary statistic. This reduces its size while at the same time preserves the presence of the detected features. There are two common pooling operations one can choose from: max-pooling \cite{zhou1988computation}, which given a three dimensional tensor $\vec{x}\in\mathds{R}^{C\times(rh)\times(sw)}$ produces a tensor $\vec{o}\in\mathds{R}^{C\times r\times s}$ by simply keeping the maximum value of a feature map within a certain rectangular neighborhood such that 
\begin{equation}
	\vec{o}_{c,j,i} = \underset{0\leq n<h,m<w}{\max} \vec{x}_{c,rj+n,si+m},
\end{equation}
and average pooling, which instead computes the mean of a feature map such that 
\begin{equation}
	\vec{o}_{c,j,i} = \frac{1}{hw} \sum_{n=0}^{h-1}\sum_{m=0}^{w-1} \vec{x}_{c,rj+n,si+m}.
\end{equation}
Besides reducing the size of a feature map, pooling operations also have the important benefit of making the representations learned by the network invariant to small translations. In fact, one could translate the input by a small amount and still obtain the same output after pooling. Note, however, that albeit desirable in most cases, there are situations where adopting pooling strategies should be avoided \cite{sabatelli2018learning, bidoiadeep}.  


\subsection{Popular Architectures}
\label{sec:architectures}

With all these concepts in place we can now define the general structure of a convolutional neural network. These models follow a general pattern, originally described in \cite{lecun1998gradient}, which is in principle very simple: an input tensor is processed by the aforementioned convolution operation, which is done many times in parallel, as different kernels are typically used. The resulting feature map is then given as input to one of the non-linear activation functions described in Sec. \ref{sec:activation_functions}, among which the ReLU is by far the most popular choice, as it allows controlling the vanishing gradient problem. The resulting feature map is then reduced by performing one of the aforementioned pooling operations. This process of convolving + ReLU + pooling is repeated several times, until the feature map is small enough to be reduced to a feature vector. This feature vector is finally processed by either a multilayer perceptron, or directly by the last output layer of the network, which as described in Sec. \ref{sec:loss_functions}, changes with respect to the SL problem we would like to solve. While this general principle has arguably barely changed over the last two decades, it is worth noting that several design choices have been proposed over the years with the aim of creating better performing, and increasingly more efficient models. We will review some of the most important ones hereafter.  

\paragraph{Image Classification Networks}

The first successful application of a convolutional neural network dates back to 1998, when \citet{lecun1998gradient} introduced \texttt{LeNet-5}, a 5-layer deep network which achieved state-of-the-art results on the MNIST handwriting recognition benchmark. Despite its success however, convolutional neural networks did not gain much popularity for over ten years. In fact, the largely limited computational resources of the time, prevented them to successfully tackle image classification tasks more complicated than the ones defined by the aforementioned MNIST dataset. Only in 2012, with the introduction of \texttt{AlexNet} \cite{krizhevsky2012imagenet}, did convolutional networks start to grab the spotlight within the computer vision community.  The work of \citet{krizhevsky2012imagenet}, resulted in an 8-layer convolutional network, which combined with a 3-layer multilayer perceptron, achieved state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a popular computer vision dataset which we will review in more detail in Chapter \ref{ch:transfer_learning}. Among the main contributions of their work we mention the first results reporting the possibility of applying convolutional networks to more complicated computer vision tasks, and the possibility of training these models in a distributed fashion by exploiting, at least partially, the benefits of parallel computing. The advent of better specialized hardware, among which we mention the development of increasingly more powerful Graphical Processing Units (GPUs), together with the successful results obtained by \texttt{AlexNet}, convinced the machine learning community to explore the potential benefits of convolutional networks further. The most promising line of work was certainly pioneered by Oxford's University Visual Geometry Group (VGG) which investigated whether deeper networks could yield better performance. Their \texttt{VGG16} and \texttt{VGG19} models \cite{simonyan2014very}, of depth 16 and 19 respectively, showed that this was indeed the case, a design choice which combined with the use of smaller kernels allowed these models to outperform \texttt{AlexNet}. Similar results were almost concurrently achieved by \citet{szegedy2015going} who introduced \texttt{GoogLeNet}, a convolutional network that uses the notion of Inception-Blocks, a specific form of convolutional layer which simultaneously uses kernels of different sizes. Among these kernels we mention the use of $1\times1$ convolutions which have the appealing benefit of acting as a powerful memory reduction technique. While all these networks are certainly deeper than \texttt{LeNet-5}, their number of hidden layers remains on average around a dozen. Despite adopting ReLU activation functions, all the aforementioned networks do still happen to suffer from the vanishing gradient problem. \citet{he2016deep} successfully addressed this limitation by introducing the concept of residual blocks and skipped connections. They propose using the output of one convolutional layer $l$, not only as input to the immediate subsequent layer $l+1$, but also to some of the subsequent layers e.g., $l+2$ and $l+3$. This simple, yet very effective trick, allowed \citet{he2016deep} to build \texttt{ResNets}, convolutional networks consisting of up to 152 layers which significantly outperformed \texttt{GoogLeNet}. \citet{huang2017densely} built on top of their ideas and introduced \texttt{DenseNets}, which take the concept of skipped connections to one level further, by designing models where each layer in the network takes as input the feature maps computed by all the predecessor layers. While the models presented so far are arguably the most popular ones, as they outperformed each other over the years when the ILSVRC was still an on-going yearly competition, it should be noted that many more, equivalently successful networks have been proposed over the years. Among such networks we mention \texttt{Inception-ResNets}, which combine inception and residual blocks \cite{szegedy2017inception} and \texttt{MobileNets} \cite{sandler2018mobilenetv2, howard2019searching} and \texttt{EfficientNet} \cite{tan2019efficientnet,}, which are models that are specifically built for minimizing inference time on devices with limited hardware capabilities. 

\paragraph{Beyond Image Classification}

So far we have only described convolutional architectures that are specifically designed for tackling the machine learning task of image classification; however, many other supervised learning problems involving high dimensional inputs exist. When it comes to the field of computer vision, lots of research attention has been given to the tasks of object detection and object segmentation. It naturally follows that several neural architectures, specifically designed for such problems, have been introduced over the years. When it comes to object detection, namely the task of locating and classifying existing objects in an image \cite{jiao2019survey}, convolutional neural networks are typically divided into two different families: models that adopt a region proposals based framework, and models that directly tackle the problem as a combination of classification and regression. The first family of techniques requires a network to go through two distinctive stages before being able to detect objects within images: a fisrt stage that aims at proposing candidate object bounding boxes, and a second, later stage, that classifies them. The most representative model that adopts region proposals is arguably \texttt{R-CNN} \cite{girshick2014rich} which, as described by \citet{jiao2019survey}, was among the very first works to show that convolutional neural networks could successfully be used for object detection tasks, and that this type of architectures could perform better than computer vision approaches based on HOG-like features \cite{jiao2019survey}. One of the main issues of \texttt{R-CNN}, and of most models that are based on region proposals, is their particularly high inference time. As a result several improvements to this architecture have been introduced over the years, among which we mention \texttt{Fast R-CNN} \cite{ren2015faster}, \texttt{Faster R-CNN} and \texttt{Mask R-CNN} \cite{he2017mask}, which all managed to significantly reduce inference time as well as the total training time that was originally required by \texttt{R-CNN}. When it comes to networks that do not involve region proposals, the arguably more popular, and by far successful architecture is \texttt{YOLO}, whose first version was introduced 
by \citet{redmon2016you}. As we will see in Chapter \ref{ch:minerva} \texttt{YOLO} treats the task of object detection as a regression problem, which overcomes the need of having two separate models that are respectively responsible for object localization and object classification. As no region proposals have to be estimated, \texttt{YOLO}-like networks \cite{redmon2016you,redmon2017yolo9000,huang2018yolo} are typically more time efficient compared to \texttt{Fast-CNN} based methods, although this can sometimes come at the price of overall worse performing detectors \cite{jiao2019survey}.

When it comes to object segmentation, namely the task of classifying each pixel in an image according to its semantic label, several deep learning-based approaches also exist. These approaches are however not only limited to the use of convolutional neural networks exclusively \cite{minaee2021image}, and can therefore also include architectures such as RNNs, Generative Adversarial Networks and Graphical Models. As the task of object segmentation will not be considered throughout this dissertation we will not discuss these approaches here, but refer the reader to a recent survey about the topic instead \cite{minaee2021image}. 


\section{Conclusion}
\label{sec:conclusion01}
In this chapter we have introduced supervised learning and seen what it means to build statistical models that can capture the interaction between input-output observations. We have specifically focused on algorithms that come in the form of artificial neural networks as this is the type of learning technique that, to this date, are by far the most successful ones. Interestingly, their learning capabilities are not limited to supervised learning problems only, which means that artificial neural networks can also be used for machine learning tasks that do not strictly require building an empirical risk minimizer. Among such problems, we mention the ones modeled in reinforcement learning, a branch of machine learning that throughout this dissertation will receive as much attention as supervised learning. We will present reinforcement learning in the next chapter.
