%Chapter 7

\chapter{Concluding Remarks} % Chapter title
\label{ch:upside_down_rl} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 





\section{RL is not cool}
Unfortunately, the intuition of even the most expert practitioner is not enough for identifying which learning parameters work best. As an example, let us consider the previously described Rainbow agent \cite{hessel2018rainbow}. The learning rate that yields successful training is set to $0.0000625$, clearly this is a value that could only be identified after performing an exhaustive grid search over a large set of potential learning rates. If on top of this, we also consider that training a DRL agent is a process that can last from taking days to even weeks \cite{kaiser2019model}, it is clear that  It is, therefore, desirable to develop algorithms that do not require extraordinary computational costs for training.

