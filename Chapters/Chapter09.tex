%Chapter 7

\chapter{Concluding Remarks} % Chapter title
\label{ch:upside_down_rl} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 





\section{RL is not cool}

\begin{itemize}
\item Unfortunately, the intuition of even the most expert practitioner is not enough for identifying which learning parameters work best. As an example, let us consider the previously described Rainbow agent \cite{hessel2018rainbow}. The learning rate that yields successful training is set to $0.0000625$, clearly this is a value that could only be identified after performing an exhaustive grid search over a large set of potential learning rates. If on top of this, we also consider that training a DRL agent is a process that can last from taking days to even weeks \cite{kaiser2019model}, it is clear that  It is, therefore, desirable to develop algorithms that do not require extraordinary computational costs for training.
\item However, if an agent can bypass the need to rely on either one of these elements, then instability can already be prevented. Several work has addressed the `Deadly Triad" of DRL \cite{van2018deep_triad,hernandez2019understanding,fedus2020revisiting} to answer the natural question \textit{`which element can be given up when developing DRL algorithms?"} So far, the community seems to agree that giving up on function approximators is clearly not possible. As discussed in Sec. \ref{sec:function_approximators} even linear functions can already play a crucial role in developing algorithms that deal with large and complex MDPs. On the other hand, it is not yet known what to give up between off-policy learning and bootstrapping; what is known however, is that also because of the `Deadly Triad", successfully training DRL solutions can be computationally very expensive since long training times are required for training agents which constantly try not to diverge. 


