%Chapter 7

\chapter{Concluding Remarks} % Chapter title
\label{ch:upside_down_rl} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 

\begin{remark}{Outline}
This is the final chapter of this dissertation which is divided in two parts: we start by answering the research questions that were presented at the beginning of this work in Chapter \ref{ch:introduction} in Sec. \ref{sec:answers}. Each question is answered with respect to the research that has been presented throughout the earlier chapters of this work and is followed by a brief critical conclusion. We then move to the second part of this chapter presented in Sec. \ref{sec:critical_discussion}, where we will critically analyze how over the last decade, the fields of supervised learning and reinforcement learning have been affected by the rise of deep neural networks and describe what the future of both research fields could look like in the coming years. 
\end{remark}


\section{Answers to the Original Research Questions}
\label{sec:answers}

We now answer the research questions that we have originally introduced in Chapter \ref{ch:introduction} and that have served as inspiration for all the work presented throughout this thesis.

\begin{enumerate}
	\item \textit{"Can convolutional neural networks be transferred and trained across different source and target domains? if so which target domains could be of interest for investigating their transfer learning properties?"}
	
	The research presented in Chapters \ref{ch:tl_natural_to_non_natural}, \ref{ch:minerva} and \ref{ch:tl_lth} shows that when it comes to supervised learning problems, convolutional neural networks exhibit strong transfer learning properties. In Chapter \ref{ch:tl_natural_to_non_natural} we have in fact seen that five popular neural architectures, originally designed for tackling computer vision problems on datasets containing natural images, can be transferred for targetting classification problems that come from the field of digital heritage. Furthermore, we have also empirically shown that all such pre-trained architectures, are able to learn features on datasets of natural images that generalize to the non natural image domain. Similar conclusions can also be drawn from the results presented in Chapter \ref{ch:minerva}, where we have shown that the good transfer learning properties of convolutional neural networks go beyond the computer vision task of classification, and also hold for object detection problems. Similarly to the research presented in Chapter \ref{ch:tl_natural_to_non_natural} we have again considered the field of digital heritage as target domain, as it offers numerous potentially interesting practical applications such as e.g., the one described by the MINERVA dataset. In Chapter \ref{ch:tl_lth} we have then seen that an equally important target domain for exploiting the transfer learning properties of pre-trained image classifiers is that of digital pathoology, as it is a field that is potentially characterized by a lack of appropriate training data, a hurdle that can strongly limit the training process of a convolutional neural network.

	While all the research presented in the second part of this thesis provides strong evidence in favour of transferring, and potentially fine-tuning, pre-trained convolutional neural networks, the same can however not be said for the results obtained in Chapter \ref{ch:dqn_transfer}, where we have instead seen that the transfer learning potential of such models can be much more limiting when it comes to the reinforcement learning domain. 

	
	\item \textit{"What Transfer Learning training strategy should be adopted to maximize the performance of pre-trained networks?"}

	As presented in Chapter \ref{ch:transfer_learning}, there are two main approaches for performing transfer learning in the context of convolutional neural networks: an off-the-shelf feature extraction approach, and a fully fine-tuning approach. The research presented in Chapter \ref{ch:tl_natural_to_non_natural} clearly shows that when it comes to image classification problems, the latter training strategy results in significantly better final performance, as it allows networks to better adapt to the target domain, and therefore learn new feature representations that are relevant for the target task. The results of this study served as inspiration for the research presented in Chapter \ref{ch:minerva} where a fine-tuning training strategy was preferred over an off-the-shelf approach when tackling object detection problems and, in the end, resulted in models that were able to successfully detect musical instruments in paintings. Surprisingly, however, in Chapter \ref{ch:dqn_transfer} we have then seen that a fine-tuning transfer learning approach can be detrimental in the context of model-free deep reinforcement learning, as deep reinforcement learning agents transfer very poorly across tasks and even to themselfes. In the case of the latter, however, this only happens if a fine-tuning training strategy is adopted and not if the networks are used as simple feature extractors.   

Despite the negative performance presented in Chapter \ref{ch:dqn_transfer} we overall still believe that if enough computational resources are available, pre-trained convolutional neural networks should always be fine-tuned, especially when it comes to supervised learning problems.

	\item \textit{"Can Transfer Learning be a valuable tool for better understanding convolutional neural networks?"}
	
	Throughout this thesis we have argued that convolutional neural networks should be studied from a transfer learning perspective, not only because this would allow practictioners to know whether such algorithms could be deployed to real world applications, but also because their transfer learning properties could deliver novel insights into their inner properties. The research presented in Chapters \ref{ch:tl_lth} and \ref{ch:dqn_transfer} is a clear example that shows that a better understanding of convolutional neural networks can be obtained thanks to transfer learning. In Chapter \ref{ch:tl_lth} we have used transfer learning as a tool for better understanding the phenomenon of the Lottery Ticket Hypothesis (LTH). This allowed us to show that pruned convolutional neural networks winners of the LTH contain inductive biases that are generic at least to some extent, and therefore gain a deeper understanding of this deep learning phenomenon. In Chapter \ref{ch:dqn_transfer} we have instead discovered that convolutional neural networks transfer very poorly when they get trained for solving reinforcement learning tasks in a model-free deep reinforcement learning setting. With the intent of understanding why their transfer learning potential was not on par with the one that was extensively observed in Chapters \ref{ch:tl_natural_to_non_natural} and \ref{ch:minerva}, we have gained novel insights that allowed us to show that models commonly denoted as Deep-Q Networks go through two very distinct training phases. 

We therefore believe that transfer learning is an extremely valuable tool for better understanding neural networks, as it allows to study their training process from a perspective which is unique and that goes beyond that of models that get trained from scratch.  

	
	\item \textit{"Do different machine learning paradigms result in convolutional neural networks with different transfer learning properties?"}

	Based on the significant gap in terms of performance between the results obtained in the second part of this dissertation, and the results obtained in the third part of this thesis, we strongly believe that the answer to this research question is \textit{yes}. Specifically, we have seen that as long as convolutional neural networks are trained for solving supervised learning tasks, then they will very likely exhibit strong transfer learning properties (see Chapters \ref{ch:tl_natural_to_non_natural} and \ref{ch:minerva}). However, if such models will be used for tackling deep reinforcement learning problems in a model-free reinforcemnt learning context, then their transfer learning potential will be much more limited. The reason of this is that when it comes to supervised learnig, convolutional neural networks will only have to serve as feature extractors, whereas the same cannot be said for model-free deep reinforcement learning, where such models have also to serve as optimal value function approximators. 

The transfer learning potential of convolutional networks is therefore highly dependant from the machine learning problem at hand, although we also believe that the poor transfer learning properties observed in Chapter \ref{ch:dqn_transfer} are inherent to model-free deep reinforcement learning algorithms only.


\end{enumerate}


\section{Critical Discussion \& Future Perspectives}
\label{sec:critical_discussion}

We now present a critical discussion which addresses some of the limitations that currently characterize the fields of deep supervised and deep reinforcement learning. 


\subsection{Deep Supervised Learning}
\label{sec:supervised_learning}



\subsection{Deep Reinforcement Learning}
\paragraph{Tedious Hyperparameters}

Over the last decade, Deep Reinforcement Learning (DRL) has undoutably gained a lot of attention from the machine learning community. The number of successful applications showcasing the potential of DRL algorithms are in fact countless, and range from agents achieving super-human performance on popular boardgames such as chess and go, to neural networks able to autonomously navigate stratospheric balloons. To an untrained eye, or more simply to a RL practitioner with few years of experience, successfully training a DRL agent might look like a straightforward task. However, behind the largely acclaimed accomplishments praised by the DRL literature, there is an equivalently large, and mostly hidden, process of hyperparameter tuning which is essential for successfully training a neural network. Throughout this thesis, convolutional neural networks have been trained both in a supervised learning context as well as in a reinforcement learning one, and we have in first person experienced how unrobust and oversensitive such algorithms can be when targeting the latter type of problems. This susceptability, on the contrary, was never encountered when tackling supervised learning tasks. We believe that at the present moment, despite all of its remarkable achievements, DRL cannot yet be considered as a successful application of convolutional neural networks. As long as a state of the art Rainbow agent \cite{hessel2018rainbow} can only get successfully trained if its learning rate is set to the unintuitive value of $0.0000625$, and a DRL practitioner has to wait for weeks before being able to see a DQN agent play certain games of the Atari Arcade Learning Environment \cite{kaiser2019model}, we believe that DRL is far from being solved.


\paragraph{The Deadly Triad}
In Chapters \ref{ch:reinforcement_learning} and \ref{ch:dqv_family_of_algorithms} we have mentioned the Deadly Triad of Deep Reinforcement Learning, a combination of three elements, which if present within the same learning agent can result into algorithms that diverge and are unable to learn an approximation of an optimal value function. As one of the elements of the Deadly Triad is that of a function approximator, it naturally follows that the stability of deep reinforcement learning algorithms will constantly be questioned by the deep learning community which will regularly introduce novel neural architectures in the coming years. This raises the natural question \textit{`which element of the triad should eventually be given up when developing DRL algorithms?"}. So far, the community seems to agree that giving up on function approximators is clearly not possible as neural networks will always play a crucial role in the development of future DRL algorithms \cite{van2018deep_triad,hernandez2019understanding,fedus2020revisiting} thanks to their properties that we discussed in Sec. \ref{sec:function_approximators} of Chapter \ref{ch:reinforcement_learning}. We agree with this point of view and therefore believe that either off-policy learning or bootstrapping should be given up: if the end goal is that of developing algorithms that do not diverge whilst training we believe that it is the off-policy learning element of the triad which should be withdrawn during the development of novel DRL techniques. While this could come at the price of restricting the number of possible policies that could be learned by an agent, we believe that this is a problem which could be controlled as long as agents will be combined with proper exploration policies. The DQV-Learning algorithm presented in Chapter \ref{ch:dqv_family_of_algorithms} is an example of a DRL algorithm that can avoid divergence through on-policy learning, while at the same time making use of a neural network trained through temporal-difference learning. 


\paragraph{Reinforcement Learning Upside Down}

It is clear that the combination of reinforcement learning algorithms and deep neural networks can present some significant limitations. Throughout this dissertation we have mainly focused on the aforementioned Deadly Triad phenomenon and on the poor transfer learning properties of Deep-Q Networks which both can limit the development of robust and general agents. Next to these issues, DRL algorithms are characterized by numerous other problems as well (the discussion of which is out of the scope of this thesis) and that all mainly arise because tabular RL algorithms are combined with function approximators \cite{}. Although the DRL community has been able to address most of such issues successfully, we believe that the way optimal control problems are currently being solved by the DRL community might need revision. In fact, we argue that most of the current DRL research focuses on introducing solutions that, albeit valuable, are only able to solve very specific and limited problems to a (sometimes very) small extent. As a result, very few DRL practitioners have been critically questioning the long term value of modern DRL algorithms, and the way RL problems are currently being addressed. An exception to this common trend, however, has recently been introduced by \citet{} who proposed the idea of `Upside-Down Reinforcement Learning" (UDRL). In UDRL the main goal is to solve typical Markovian control problems via classic supervised learning techniques, and to replace common RL concepts such as value function approximation and policy search, through maximum likelihood estimation techniques. In principle this should be done by learning a mapping from states to actions (see \cite{} for all the mathematical details), which could overcome the need of learning an optimal value function or stochastic policy. We agree with Schmidhuber's ideas, and support the goal of potentially solving optimal control problems via supervised learning techniques. In fact we strongly believe that DRL in its current form is already reducing reinforcement learning problems to supervised learning problems. As an example let us consider the role of the experience replay memory buffer reviewed in Chapter \ref{ch:reinforcement_learning} and adopted by the DQV and DQV-Max Learning algorithms presented in Chapter \ref{ch:dqv_family_of_algorithms}. We have seen that the role of experience replay is that of storing RL trajectories which can in a later moment be sampled and used for minimizing the objective function of a Deep-Q Network. By taking a critical look at this buffer, we can see that it is in large part equivalent to the datasets that are typically used in supervised learning: it needs to store a large amount of RL trajectories which can then be used for modeling the task of learning a value function as a regression problem where $\mathcal{Y}$ is modeled by the space of all TD-errors stored within the buffer. The idea of constructing a dataset of previous trajectories is very far from the ideal RL setting reviewed at the beginning of Chapter \ref{ch:reinforcement_learning}, where we have indeed seen that a RL agent should be able to learn in an online fashion based on the last trajectory only. We therefore believe that the successess obtained by DRL stem from the fact that RL problems have been modeled as supervised learning ones, which as discussed in Sec. \ref{sec:supervised_learning} are well suited for deep neural networks.


\begin{takeaway}{Takeaway of Part III}

\end{takeaway}







