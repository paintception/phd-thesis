\chapter{On the Transferability of Convolutional Networks}
\label{ch:tl_natural_to_non_natural}

\begin{remark}{Contributions and Outline} 
	This chapter contributes to the field of (Deep) Transfer Learning (TL) by investigating whether popular neural networks that come as pre-trained on datasets containing natural images can perform equally well once they are used on non-natural datasets. Specifically, we explore whether these models can be used for tackling three different art classification problems. Furthermore, assuming this is the case, we also explore whether it is possible to improve on such performance. The chapter is structured as follows: we start by providing the reader with some background information in Sec. \ref{sec:introduction}. In Sec. \ref{sec:methods} we present a brief theoretical reminder of the field of TL, a description of the datasets that we have used and the methodological details about the experiments that we have performed. In Sec. \ref{sec:results} we present and discuss our results. A summary of the main contributions of this work ends the chapter in Sec. \ref{sec:conclusion}.

\vspace{5mm}

\textit{This chapter is based on the publication \citet{sabatelli2018deep}.}
\end{remark}


% ---- Section 1 ----
\section{Introduction and Related Work}
\label{sec:introduction}

Over the past decade Deep Convolutional Neural Networks (DCNNs) have become one of the most used and successful algorithms in Computer Vision (CV) \cite{donahue2014decaf, ma2015multimodal,tome2016deep}. Due to their ability to automatically learn representative features by incrementally down sampling the input via a set of non linear transformations, these kind of neural networks have rapidly established themselves as the state of the art algorithm on a large set of CV problems. Within different CV testbeds large attention has been paid to the ImageNet challenge \cite{deng2009imagenet}, a CV benchmark that aims to test the performance of different image classifiers on a dataset that contains one million natural images distributed over thousand different classes. The availability of such a large dataset, combined with the possibility of training deep neural networks in parallel over several \texttt{GPUs} \cite{krizhevsky2012imagenet}, has lead to the development of a large set of different neural architectures that have continued to outperform each other over the years \cite{simonyan2014very,szegedy2016rethinking,chollet2016xception,he2016deep,huang2017densely}.     
A promising research field in which the classification performances of such DCNNs can be exploited is that of \textit{Digital Heritage} \cite{parry2005digital}. Due to a growing and rapid process of digitization, museums have started to digitize large parts of their cultural heritage collections, leading to the creation of several digital open datasets \cite{allen2000collaboration, mensink2014rijksmuseum}. The images constituting these datasets are mostly matched with descriptive metadata  which, as presented by \citet{mensink2014rijksmuseum}, can be used to define a set of challenging machine learning tasks. However, the number of samples in these datasets is far smaller than those in, for instance, the ImageNet challenge and this can become a serious constraint when trying to successfully train deep networks from scratch.

The lack of available training data is a well known issue in the deep learning community and is one of the main reasons that has led to the development of the research field of Transfer Learning (TL). The main idea of TL consists of training a machine learning algorithm on a new task (e.g. a classification problem) while exploiting knowledge that the algorithm has already learned on a previously related task (a different classification problem). This machine learning paradigm has proved to be extremely successful in deep learning, where it has been shown how popular models that were trained on many large datasets \cite{huang2007labeled, stallkamp2011german}, were able to achieve very promising results on classification problems from heterogeneous domains, ranging from medical imaging \cite{tajbakhsh2016convolutional} or gender recognition \cite{van2015deep} over plant classification \cite{reyes2015fine} to galaxy detection \cite{ackermann2018using}.      

In this work we explore whether the TL paradigm can be successfully applied to three different art classification problems. We use four neural architectures that have obtained strong results on the ImageNet challenge in recent years and we investigate their performance when it comes to attributing the \textit{authorship} to different artworks, recognizing the \textit{material} which has been used by the artists in their creations, and identifying the \textit{artistic category} these artworks fall into. We do so by comparing two possible approaches that can be used to tackle the different classification tasks. The first one, known as off the shelf classification \cite{razavian2014cnn}, simply retrieves the features that were learned by the networks on other datasets and uses them as input for a new classifier. In this scenario the weights of the model do not change during the training phase, and the final, top-layer classifier is the only component of the architecture which is actually trained. This changes in our second explored approach, known as fine tuning, where the weights of the original network are ``unfrozen'' and the neural architectures are trained together with the final classifier. 

\citet{kornblith2018better} have shown the benefits that this particular pre-training approach has. In particular, DCNNs which have been trained on the ImageNet challenge typically lead to superior results when compared to the same architectures trained from scratch. However, this is not necessarily beneficial and in some cases networks that are randomly initialized are able to achieve the same performance as ImageNet pre-trained models. However, none of the results presented in \cite{kornblith2018better} report experiments on datasets containing heritage objects, it is thus still an open question how such pre-trained DCNNs would perform in such a classification scenario. In the rest of this chapter we report results that extensively study the performance of such neural networks; at the same time we also assess whether better TL performance can be obtained when using neural networks that, in addition to the ImageNet dataset, have additionally been pre-trained on a large artistic collection.  


\section{Methods}
\label{sec:methods}

We now present the methods that underpin our research. We start by giving a brief formal reminder of TL. We then introduce the three classification tasks under scrutiny, together with a brief description of the datasets. Finally, we present the neural architectures that we have used for our experiments. 

\subsection{Transfer Learning}
\label{subsec: tl}

A supervised learning (SL) problem can be identified by three elements: an input space ${\cal X}_t$, an output space ${\cal Y}_t$, and a probability distribution $p_t(x,y)$ defined over ${\cal X}_t\times {\cal Y}_t$ (where $t$ stands for 'target', as this is the main problem we would like to solve). The goal of SL is then to build a function $f:{\cal X}_t\rightarrow{\cal Y}_t$ that minimizes the expectation over $p_t(x,y)$ of a given loss function $\ell$ assessing the predictions made by $f$:
\begin{equation}\label{loss}
  E_{(x,y)\sim p_t(x,y)} \{\ell(y,f(x))\},
\end{equation}
when the only information available to build this function is a learning sample of input-output pairs $LS_t=\{(x_i,y_i)|i=1,\ldots,N_t\}$ drawn independently from $p_t(x,y)$. In the general transfer learning setting, one assumes that an additional dataset $LS_s$, called the source data, is available that corresponds to a different, but related, SL problem. More formally, the source SL problem is assumed to be defined through a triplet $({\cal X}_s,{\cal Y}_s,p_s(x,y))$, where at least either ${\cal X}_s\neq {\cal X}_t$, ${\cal Y}_s\neq {\cal Y}_t$, or $p_s\neq p_t$. The goal of TL is then to exploit the source data $LS_s$ together with the target data $LS_t$ to potentially find a better model $f$ in terms of the expected loss (\ref{loss}) than when only $LS_t$ is used for training this model. Transfer learning is especially useful when there is a lot of source data, whereas target data is more scarce.

Depending on the availability of labels in the target and source data and on how the source and target problems differ, one can distinguish different TL settings \cite{pan2010survey}. In what follows, we assume that labels are available in both the source and target data and that the input spaces ${\cal X}_t$ and ${\cal X}_s$, that both correspond to color images, match. Output spaces and joint distributions will however differ between the source and target problems, as they will typically correspond to different classification problems (ImageNet object recognition versus art classification tasks). Our problem is thus an instance of \textit{inductive transfer learning} \cite{pan2010survey}. While several inductive transfer learning algorithms exist, hereafter we focus on model transfer techniques, where information between the source and target problems is exchanged in the form of a neural network that comes as pre-trained on the source data. Although potentially suboptimal, this approach has the advantage of being more computationally efficient, as it does not require to train a model using both the source and the target data.


\subsection{Datasets and Classification Challenges}
\label{subsec:datasets}

For our experiments we use two datasets which come from two different heritage collections. The first one contains the largest number of samples and comes from the Rijksmuseum in Amsterdam\footnote{\url{https://staff.fnwi.uva.nl/t.e.j.mensink/uva12/rijks/}}. On the other hand, our second `Antwerp' dataset is much smaller. This dataset presents a random sample that is available as open data from a larger heritage repository: DAMS (Digital Asset Management System)\footnote{\url{https://dams.antwerpen.be/}}. This repository can be searched manually via the web-interface or queried via a Linked Open Data API. It aggregates the digital collections of the foremost GLAM institutions  (Galleries, Libraries, Archives, Museums) in the city of Antwerp in Belgium. Thus, this dataset presents a varied and representative sample of the sort of heritage data that is nowadays being collected at the level of individual cities across the globe. While it is much smaller, its coverage of cultural production is similar to that of the Rijksmuseum dataset and presents an ideal testing ground for the transfer learning task under scrutiny here.

Both image datasets come with metadata encoded in the Dublin Core metadata standard \cite{weibel1998dublin}. We selected three well-understood classification challenges: (1) ``material classification'' which consists in identifying the material the different heritage objects are made of (e.g paper, gold, porcelain, ...) ;  (2) ``type classification'' in which the neural networks have to classify in which artistic category the samples fall into (e.g. print, sculpture, drawing, ...), and finally (3) ``artist classification'', where the main goal is to appropriately match each sample of the dataset with its creator (from now on we refer to these classification tasks as challenge 1, 2 and 3 respectively). As reported in Table \ref{table:dataset_overview} we can see that the Rijksmuseum collection is the dataset with the largest amount of samples per challenge ($N_t$) and the highest amount of labels to classify ($Q_t$). Furthermore it is also worth noting that there was no metadata available when it comes to the first classification challenge for the Antwerp dataset (as marked by the $\times$ symbol), and that there are some common labels between the two heritage collections when it comes to challenge 2. A visualization reporting some of the images that are present in both datasets is shown in Figure \ref{fig:datasets}.


\input{./Tables/Chapter04/table_1.tex}


\begin{figure}
\centering
  \includegraphics[width=10cm]{./Images/Chapter04/datasets.jpg}\vspace{-1cm}
  \caption{A visualization of the images that are used for our experiments. It is possible to see how the samples range from images representing plates made of porcelain to violins, and from Japanese artworks to a more simple picture of a key.}
  \label{fig:datasets}
\end{figure}

We use $80\%$ of the datasets for training while the remaining $2 \times 10\%$ is used for validation and testing respectively. Furthermore, we ensure that only classes which occur at least once in all the splits are used for our experiments. Naturally, in order to keep all comparisons fair between neural architectures and different TL approaches, all experiments have been performed on the exact same data splits which, together with the code used for all our experiments, are publicly released to the CV community \footnote{\url{https://github.com/paintception/Deep-Transfer-Learning-for-Art-Classification-Problems}}. 

\subsection{Convolutional Networks and Classification Approaches}
\label{subsec: neural_nets}

For our experiments we use four pre-trained DCNNs that have all obtained state of the art results on the ImageNet classification challenge. The neural architectures are VGG19 \cite{simonyan2014very}, Inception-V3 \cite{szegedy2016rethinking}, Xception \cite{chollet2016xception} and ResNet50 \cite{xie2017aggregated}. We use the implementations of the networks that are provided by the \texttt{Keras} Deep Learning library \cite{chollet2015keras} together with their appropriate \texttt{Tensorflow} weights \cite{abadi2016tensorflow} that come from the \texttt{Keras} official repository as well. Since all architectures have been built in order to deal with the ImageNet dataset we replace the final classification layer of each network with a new one. This final layer simply consists of a new \textit{softmax} output, with as many neurons as there are classes to classify, which follows a 2D global average pooling operation. We rely on this dimensionality reduction step because we do not add any fully connected layers between the last convolution layer and the \textit{softmax} output. Hence, in this way we are able to obtain a feature vector, $\mathscr{X}$, out of the rectified activation feature maps of the network that can be properly classified. Since all experiments are treated as a multi-class classification problem all networks minimize the \textit{categorical crossentropy} function loss function.

We investigate two possible classification approaches that are based on the previously mentioned pre-trained architectures. The first one, denoted as off the shelf classification, only trains a final \textit{softmax} classifier on $\mathscr{X}$, which is retrieved from the different models after performing one forward pass of the image through the network \footnote{Please note that instead of a \textit{softmax} layer any kind of machine learning classifier can be used instead. We experimented with both Support Vector Machines (SVMs) and Random Forests but since the results did not significantly differ between classifiers we decided to not include them here.}. This approach is intended to explore whether the features that are learned by the network on the ImageNet dataset are informative enough in order to properly train a machine learning classifier on the previously introduced art classification challenges. If this would be the case, such pre-trained models could be used as appropriate feature extractors without having to rely on expensive \texttt{GPU} computations for training. Naturally, they would only require the training of the final classifier without having to compute any backpropagation operations over the entire network. From now on we will refer to these networks with $\theta^{-}$. 

Our second approach is generally known as fine tuning and differs from the previous one by the fact that together with the final \textit{softmax} output the entire network is trained as well. This means that unlike the off the shelf approach, the entirety of the neural architecture gets ``unfrozen'' and is optimized during training. The potential benefit of this approach lies in the fact that the models get independently trained on samples coming from the artistic datasets, and therefore their classification predictions will not be restricted to what the networks previously learned on the ImageNet dataset only. Evidently, such an approach is computationally more demanding. We refer to these networks as $\theta^{i}$, where $i$ stands for the source task these models have been trained on, namely the ImageNet dataset. %We visually present a simplified representation of the classification approaches considered in this chapter together with how they differ from not pre-trained model in Fig. \ref{fig:network_training_approaches}.


In order to maximize the performance of all models we follow some of the recommendations presented by \citet{masters2018revisiting} and train the networks with a relatively small batch size of $32$ samples. We do not perform any data augmentation operations besides a standard pixel normalization to the $[0, 1]$ range and a re-scaling operation which resizes the images to the input size that is required by the different models. Regarding the stochastic optimization procedures of the different classifiers, we use two different optimizers, that after preliminary experiments, turned out to be the best performing ones. For the off the shelf approach we use the RMSprop optimizer \cite{tieleman2012lecture} which has been initialized with its default hyperparameters (learning rate = $0.001$, a \textit{momentum} value $\rho = 0.9$ and $\epsilon =1e-08$). On the other hand, when we fine tune the DCNNs we use the standard (and less greedy) Stochastic Gradient Descent (SGD) algorithm with the same learning rate, $0.001$, and a \textit{Nesterov Momentum} value set to $0.9$.
Training has been controlled by the \textit{Early Stopping} method \cite{caruana2001overfitting} which interrupted training as soon as the validation loss did not decrease for $7$ epochs in a row. The model which is then used on the testing set is the one which obtained the smallest validation loss while training.

To the best of our knowledge, the results presented in this chapter are the first ones that systematically asses to which extent models pre-trained on the ImageNet dataset can be used as valuable architectures when tackling art classification problems. Furthermore, we also present the first results that investigate it is also not known whether the fine tuning approach can yield better results than the off the shelf one, and if using such pre-trained models would yield better performance than training the same architectures from scratch as observed by \citet{kornblith2018better}.

% END METHODS
%==================================================================================================================
% BEGIN RESULTS

\section{Results}
\label{sec:results}

Our experimental results are divided in two different sections, depending on which kind of dataset has been used. We first report the results that we have obtained when using architectures that were pre-trained on the ImageNet dataset only, and aimed to tackle the three classification problems of the Rijksmuseum dataset that were presented in Section \ref{subsec:datasets}. We report these results in Section \ref{subsec: natural_to_art} where we explore the benefits of using the ImageNet dataset as the TL source data, and how well such pre-trained models generalize when it comes to the artistic domain. We then present the results from classifying the Antwerp dataset, using models that are both pre-trained on the ImageNet dataset and on the Rijksmuseum collection in Section \ref{subsec: from_one_to_another}. We investigate whether these neural architectures, which have already been trained to tackle art classification problems before, perform better than the ones which have been trained on the ImageNet dataset only.    

All results show comparisons between the off the shelf classification approach and the fine tuning scenario. In addition to that, in order to establish the potential benefits that TL from ImageNet has over training a model from scratch, we also report the results that have been obtained when training a network with weights that have been initially sampled from a ``He-Uniform'' distribution \cite{he2015delving}. Since we take advantage of the work presented by \citet{bidoiadeep} we use the Inception-V3 architecture. We refer to it in all figures as Scratch-V3 and always visualize it with a solid orange line. Figures \ref{fig:rijks_material} and \ref{fig:type_and_artist} report the performance in terms of accuracy that the models have obtained on the validation sets. While the performances that the neural architectures have obtained on the final testing set are reported in Tables \ref{tab:Rijksmuseum_Dataset} and \ref{tab:Antwerpen_dataset}. 


\subsection{From Natural to Art Images}
\label{subsec: natural_to_art}

The first results that we report have been obtained on the ``material'' classification challenge. We believe that this can be considered as the easiest classification task within the ones that we have introduced in Section \ref{subsec:datasets} for two main reasons. First, the number of possible classes the networks have to deal with is more than five times smaller when compared to the other two challenges. Furthermore, we also believe that this classification task is, within the limits, the most similar one when compared to the original ImageNet challenge. Hence, the features that might be useful in order to classify the different natural images on the latter classification testbed might be not too dissimilar from the ones that are needed to properly recognize the material that the different samples of the Rijksmuseum collection are made of. If this would be the case we would expect a very similar performance between the off the shelf classification approach and the fine tuning one.
Comparing the learning curves of the two classification strategies in Figure \ref{fig:rijks_material}, we actually observe that the fine tuning approach leads to significant improvements when compared to the off the shelf one, for three architectures out of the four tested ones. Note however that, in support of our hypothesis, the off the shelf approach can still reach high accuracy values on this problem and is also competitive with the DCNN trained from scratch. This suggests that features extracted from networks pretrained on ImageNet are relevant for material classification.

When comparing the learning curves of the two classification strategies reported in Figure \ref{fig:rijks_material}, we can observe that the fine tuning approach leads to significant improvements when compared to the off the shelf one, for three architectures out of the four tested ones. Note however that, in support of our hypothesis, the off the shelf approach can still reach high accuracy values on this problem and is also competitive with the network that is trained from scratch. This suggests that features extracted from pretrained ImageNet models are relevant for material classification.

\input{Results/Chapter04/tex/results_1.tex}

We can also observe that the ResNet50 architecture is the architecture which, when fine tuned, performs overall best when compared to the other three models. This happens despite it being the network that initially performed worse as a simple feature extractor in the off the shelf experiments. As reported in Table \ref{tab:Rijksmuseum_Dataset} we can see that this kind of behavior reflects itself on the separated testing set as well, where it obtained the highest testing set accuracy when fine tuned ($92.95\%$), and the lowest one when the off the shelf approach was used ($86.81\%$). It is worth noting that the performance between the different neural architectures do not strongly differ between each other once they are fine tuned, with all models performing around $\approx 92\%$ on the final testing set. Furthermore, special attention needs to be given to the VGG19 architecture, which does not seem to benefit from the fine tuning approach as much as the other architectures do. In fact, its off the shelf performance on the testing set ($92.12\%$) is very similar to its fine tuned one ($92.23\%$). This suggests that this neural architecture is the only one which, in this task, and when pre-trained on ImageNet, can successfully be used as a simple feature extractor without having to rely on complete retraining. 

When analyzing the performance of the different neural architectures on the ``type'' and ``artist'' classification challenges (respectively the left and right plots reported in Figure \ref{fig:type_and_artist}), we observe that on these problems fine tuning strategy leads to even more significant improvements when compared to what we observed in the previous experiment. The results obtained on the second challenge show again that the ResNet50 architecture is the architecture which leads to the worse results if the off the shelf approach is used (its testing set accuracy is as low as $71.23\%$) and similarly to what has been observed before, it then becomes the best performing model when fine tuned, with a final accuracy of $91.30\%$. Differently from what has been observed in the previous experiment, the VGG19 architecture, despite being the network performing best when used as off the shelf feature extractor, this time performs significantly worse than when it is fine tuned, which highlights the benefits of this latter training approach. Similarly to what has been observed before, our results are again not significantly in favor of any fine tuned neural architecture, with all final accuracies being around $\approx 91\%$.

If the classification challenges that we have analyzed so far have highlighted the significant benefits of the fine tuning approach over the off the shelf one, it is also important to note that the latter approach is still able to lead to satisfying results. In fact, a final accuracy of $92.12\%$ has been obtained when using the VGG19 architecture on the first challenge and a classification rate of $77.33\%$ was reached by the same architecture on the second challenge. Despite the latter accuracy being very far in terms of performance from the one obtained when fine tuning the network ($90.27\%$), these results still show that models pre-trained on ImageNet do learn particular features that can also be used for classifying the ``material'' and the ``type'' of heritage objects. However, when analyzing the results from the ``artist'' challenge, we can see that this is partially not the case anymore.


\input{Results/Chapter04/tex/results_2.tex}

When considering the third classification challenge we can observe that the Xception, ResNet50, and Inception-V3 architectures all perform extremely poorly if not fine tuned, with the latter two models not being able to even reach a $10\%$ classification rate. Better results are obtained when using the VGG19 architecture, which reaches a final accuracy of $38.11\%$. Most importantly, the performance of each model are again significantly improved when the networks are fine tuned. As already observed in the previous experiments, ResNet50 outperforms the other architectures on the validation set. However, on the test set (see Table \ref{tab:Rijksmuseum_Dataset}), the overall best performing network is Inception-V3 (with a final accuracy of $51.73\%$), which suggests that ResNet50 suffered from overfitting. It is important to state two major important points about this set of experiments. The first one relates to the final classification accuracy that is obtained by all models, and that at first sight might seem disappointing. While it is true that these classification rates are significantly lower when compared to the ones obtained in the previous two experiments, it is important to highlight how a large set of artists present in the dataset are associated to an extremely limited amount of samples. This reflects a lack of appropriate training data which does not allow the models to learn all the features that are necessary for successfully dealing with this particular classification challenge. In order to do so, we believe that more training data is required. Moreover, it is worth pointing out that despite performing very poorly when used as off the shelf feature extractors, ImageNet pre-trained models do still perform better once they are fine tuned than a model that is trained from scratch. This suggests that these networks do learn potentially representative features when it comes the classification of artists, but in order to properly classify them, the model need to be fine tuned.

\input{Tables/Chapter04/table_2.tex}

\subsection{Discussion}
\label{subsec: RijksDiscussion}
In the previous section, we have investigated whether four different architectures pre-trained on the ImageNet dataset can be successfully used to address three art classification problems. We have observed that this is particularly the case when it comes to classifying the material and the type, where in fact, the off the shelf approach already yielded satisfactory results. However, most importantly, we have also shown that the performance of all models can be significantly improved when the networks are fine tuned, and that an ImageNet initialization is beneficial when compared to training a randomly initialized network from scratch. Furthermore, we have discovered that the pre-trained DCNNs fail if used as simple feature extractors when having they are used for attributing the authorship to the different heritage objects. In the next section, we explore the performance of fine tuned models that are trained for tackling two of the already seen classification challenges on a different heritage collection. For this problem, we will again compare the off the shelf approach with the fine tuning one.


\subsection{From One Art Collection to Another} 
\label{subsec: from_one_to_another}

Table \ref{tab:Antwerpen_dataset} compares the results that we obtained on the Antwerp dataset when using ImageNet pre-trained models ($\theta^{i}$) versus the same architectures that were fine tuned on the Rijksmuseum dataset ($\theta^{r}$). While looking at the performance of the different neural architectures two interesting results can be highlighted. First, models which have been fine tuned on the Rijksmuseum dataset outperform the ones pre-trained on ImageNet in both classification challenges. This happens to be the case both when the networks are used as simple feature extractors and when they are fine tuned. On the ``type'' classification challenge, this result is not surprising since, as discussed in Section \ref{subsec:datasets}, the types corresponding to the heritage objects of the two collections partially overlap. This is more surprising on the ``artist'' classification challenge however, since there is no overlap at all between the artists of the Rijksmuseum and the ones from the Antwerp dataset.
A second interesting result, which is consistent with the results presented in the previous section, revolves around the observation that it is always beneficial to fine tune the networks over just using them as off the shelf feature extractors. Once the models get fine tuned on the Antwerp dataset, these DCNNs, which have also been fine tuned on the Rijksmuseum dataset, outperform the architectures that were pre-trained on ImageNet only. This happened to be the case for both classification challenges and for all considered architectures, as reported in Table \ref{tab:Antwerpen_dataset}. This demonstrates how beneficial it is for the models to have been trained on a similar source task and how this can lead to significant improvements both when the networks are used as feature extractors and when they are fine tuned. 

\input{Tables/Chapter04/table_3.tex}

\subsection{Selective Attention}

The benefits of the fine tuning approach over the off the shelf one are clear from our previous experiments. Nevertheless, we do not have any insights yet as to what exactly allows fine tuned models to outperform the architectures which are pre-trained on ImageNet only. In order to provide an answer to that, we investigate which pixels of each input image contribute the most to the final classification predictions of the networks. We do this by using the ``VisualBackProp'' algorithm presented by \cite{bojarski2016visualbackprop}, which is able to identify which feature maps of the networks are the most informative ones with respect to their final prediction. Once these feature maps are identified, they get backpropagated to the original input image, and visualized as a saliency map according to their weights. The higher the activation of the filters, the brighter the set of pixels covered by these filters are represented.

The results that we have obtained provide interesting insights about how fine tuned models develop novel selective attention mechanisms over the images, which are very different from the ones that characterize the networks that are pre-trained on ImageNet. We report the existence of these mechanisms in Figure \ref{fig:saliency_maps} where we visualize the different saliency maps between a model pre-trained on ImageNet and the same neural architecture which has been fine tuned on the Rijksmuseum collection. In Figure \ref{fig:saliency_maps} we visualize which sets of pixels allow the fine tuned model to successfully classify an artist of the Rijksmuseum collection that the same architecture was not able to initially recognize. It is possible to notice that the saliency maps of the latter architecture either correspond to what is more similar to a natural image, as represented by the central image of the first row of plots, or even to what appear to be non informative pixels at all, as shown by the second image in the second row. However, when considering the fine tuned model we clearly observe that these saliency maps change. In this case the network attends towards the set of pixels that represent people in the bottom, suggesting that this is what allows the model to appropriately recognize the artist of the considered artwork.

\begin{figure*}[!htb]
\centering
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/example_1.jpg}
\endminipage
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/imagenet_saliencies_1.jpeg}
\endminipage
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/rijksnet_saliencies_1.jpeg}
\endminipage

\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/example_2.jpg}
\endminipage
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/imagenet_saliencies_2.jpeg}
\endminipage
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/rijksnet_saliencies_2.jpeg}
\endminipage

\caption{Add caption}
\label{fig:saliency_maps}
\end{figure*}


These observations can be related to parallel insights in authorship attribution research \cite{stamatatos:2009}, an established task from Natural Language Processing that is highly similar in nature to artist recognition. In this field, preference is typically given to high-frequency function words (articles, prepositions, particles etc.) over content words (nouns, adjectives, verbs, etc.), because the former are generally considered to be less strongly related to the specific content or topic of a work. As such, function words or stop words lend themselves more easily to attribution across different topics and genres. In art history, strikingly similar views have been expressed by the well-known scholar Giovanni Morelli (1816-1891), who published seminal studies in the field of artist recognition \cite{wollheim:1972}. In Morelli's view too, the attribution of a painting could not happen on the basis of the specific content or composition of a painting, because these items were too strongly influenced by the topic of a painting or the wishes of a patron. Instead, Morelli proposed to base attributions to so-called \emph{Grundformen} or small, seemingly insignificant details that occur frequently in all paintings and typically show clear traces of an artist's individual style, such as ears, hands or feat, a painting's function words, so to speak. The saliency maps above reveal a similar shift in attention when the ImageNet weights are adapted on the Rijksmuseum data: instead of focusing on higher-level content features, the network shifts its attention to lower layers in the network, seemingly focusing on insignificant details, that nevertheless appear crucial to perform artist attribution.


\section{Conclusion} 
\label{sec:conclusion}
 
This paper provides insights about the potential that the field of TL has for art classification. We have investigated the behavior of DCNNs which have been originally pre-trained on a very different classification task and shown how their performances can be improved when these networks are fine tuned. Moreover, we have observed how such neural architectures perform better than if they are trained from scratch and develop new saliency maps that can provide insights about what makes these DCNNs outperform the ones that are pre-trained on the ImageNet dataset. Such saliency maps reflect themselves in the development of new features, which can then be successfully used by the DCNNs when classifying heritage objects that come from different heritage collections. It turns out that the fine tuned models are a better alternative to the same kind of architectures which are pre-trained on ImageNet only, and can serve the CV community which will deal with similar machine learning problems.

As future work, we aim to investigate whether the results that we have obtained on the Antwerp dataset will also apply to a larger set of smaller heritage collections. Furthermore, we want to explore the performances of densely connected layers \cite{huang2017densely} and understand which layers of the currently analyzed networks contribute the most to their final classification performances. This might allow us to combine the best parts of each neural architecture into one single novel DCNN which will be able to tackle all three classification tasks at the same time. 


