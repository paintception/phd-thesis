\chapter{Novel Datasets for Transfer Learning}

\begin{remark}{Outline}

This chapter ...

\vspace{5mm}
\textit{This chapter is based on the publication \citet{sabatelli2021advances}.}
\label{ch:minerva_paper}


\end{remark}


\section{Challenges of Modern Computer Vision}
\label{sec:cv_challenges}

If it is true that the results presented in the previous chapter show that it is possible to successfully transfer pre-trained convolutional neural networks to non natural image datasets, it is equally true that some limitations might still need to be addressed. One above all, is the need of fine-tuning the networks instead of simply using them as feature extractors. As demonstrated by the experiments performed on the third classification problem of the Rijksmuseum dataset, it is clear that pre-trained models might only learn features that are relevant for their respective source task $\mathcal{T}_S$ (ImageNet), which therefore might result into unsatisfying performance when an off the shelf training strategy is used. While this is a result that does not come as a surprise, as it would be unreasonable to expect pre-trained networks to act as universal feature extractors, this limitation can still have some important practical implications, since it can prevent the deployment of computer vision systems outside the domain of natural images. As a very practical example let us consider the first image presented in Fig. \ref{fig:fails} and the computer vision task of object detection. We tackle this task with an object detector that is pre-trained on natural images only and that therefore, has never seen any images coming from a domain other than the source domain $\mathcal{D}_S$. From the performance of the model it is clear that only one out of the two predictions made by the network is appropriate, as it fails in detecting the musical instrument depicted in the image by wrongly classifying it as a `frisbee". While certainly reasonable and fully justifiable, this kind of performance is the result of some limitations that currently characterize modern Computer Vision (CV) which we summarize as follows:

\begin{itemize}
	\item \textcolor{RoyalBlue}{Photorealism and Data Scarcity:} it is well known that modern CV strongly gravitates towards photorealistic material, since most of the datasets that are used in the field are representative of digitized, or born-digital, versions of photographs. Nevertheless, datasets like MNIST, CIFAR-10/100 and the already mentioned ImageNet play a crucial role in today's rapid development of the field, as they are constantly used as benchmarks by the community. While certainly suitable for defining different challenging CV tasks, it is worth noting however, that these datasets are also only partially representative of the physical world, as they do not actively attempt to distort the reality they depict. Unfortunately, datasets going beyond the photorealistic domain are either much rarer, or are simply not as popular as their photorealistic counterparts, a limitation that results into pre-trained models that fail in performing well when used outside from the natural world (see again first image of Fig. \ref{fig:fails}).  

	\item \textcolor{RoyalBlue}{Modern Training Classes:} the performance depicted in the first image of Fig. \ref{fig:fails} can largely be attributed to the fact that the model used for detecting the objects in the image has never been explicitly trained on images of musical instruments. As a result its predictions can only tend to be representative of the classes that have governed the training process. While this is a behavior that has only to be expected, it can still serve as a surrogate for highlighting a crucial limitation of modern object detection datasets: datasets are not as diversified and heterogeneous as one might expect. As an example let us consider the popular Pascal-Voc \cite{everingham2010pascal} and MS-COCO \cite{lin2014microsoft} datasets. The first one tackles the detection of 20 classes, out of which more than a third constitute different kinds of transportation systems, such as `trains", `boats",`motorcycles" and `cars". The latter, albeit more complex, mostly represents objects that are representative of the highly technological world we currently live in, with classes such as `microwave", `laptop" and `remote control". In practice this results into models that gravitate towards detecting objects in an image which are modern, a behavior that hurts not-technological classes such as the `person" one which should be detected in the second image of Fig. \ref{fig:fails}.     

	\item \textcolor{RoyalBlue}{Model Robustness:} the aforementioned limitation also results into models that learn features that are hardly general enough for successfully tackling different representations of the same class. As an example let us consider the last image of Fig. \ref{fig:fails}: we can clearly see that a model pre-trained on MS-COCO successfully detects the persons represented in the paintings only as long as their pose corresponds to a pose that can easily be found in the images depicting persons in photorealistic datasets. As soon as a person is depicted in a pose that is different from the one that usually characterizes a person in a photorealistic dataset (sitting or standing), then a pre-trained network mistakenly detects it as an animal.   
\end{itemize}

\begin{figure}[ht!]
\centering
  \includegraphics[width=\linewidth]{./Images/Chapter05/fails}
  \caption{}
  \label{fig:fails}
\end{figure}

In this chapter we take inspiration from these limitations and use them as a surrogate for introducing novel datasets that can be used as a benchmark for CV researchers. The purpose of such datasets is twofold: on the one hand they represent, at least in part, a solution to the aforementioned issues that currently characterize CV, while on the other hand, they allow us to continue studying the transfer learning properties of convolutional neural networks which we started in the previous chapter.  

\section{The MINERVA Dataset}
\label{sec:minerva_dataset}

We now introduce MINERVA, a novel annotated dataset that can be used for object detection. More specifically, the main task that we present is that of the detection of musical instruments in non-photorealistic, unrestricted image collections from the artistic domain. We start by providing some background information about the field of music iconography which allows us to motivate why being able to identify musical instruments is not only a challenging CV task but is also of high practical interest. We then describe how the images of MINERVA have been first collected and then annotated. We end this section by quantitatively characterizing the dataset from a machine learning perspective.    

\subsection{Music Iconography}


\subsection{Data Collection}
The images constituting MINERVA come from three different data sources which allow the dataset to be highly varied and unrestricted. In fact its images cover a large range of periods, genres and materials and are both of photorealistic and not-photorealistic nature as visually represented in Fig. \ref{fig:minerva_dataset}. The three data sources are the following:
\begin{itemize}
	\item RIDIM: which stays for \textit{Repertoire International d'Iconographie Musicale} is an international digital inventory for musical iconography that functions as a reference image database. Developed and curated by \citet{green2013ridim} it has been designed to facilitate the discovery of music-related artworks. Among the three different considered data sources, the images coming from the RIDIM collection are the ones of highest quality in terms of resolution.
	\item RMFAB/RMAH: which stays for \textit{Royal Museums of Fine Arts of Belgium} and \textit{Royal Museums of Art and History}. These images come from a larger pool of digitized images which have been manually selected on the basis whether they included depictions of musical instruments or not. Among the different data sources, the amount of images coming from RMFAB/RMAH within MINERVA is is the lowest when compared to the other two data sources. These images are of midrange resolution.
	\item Flickr: is a well known image hosting service from which we downloaded a large dataset of images depicting musical instruments in the visual arts pre-dating 1800. Most of the images present within MINERVA come from Flickr, although their resolution is not always on par with the previous two data sources.  
\end{itemize}

Once all these images have been collected we have started the labeling process.

\subsection{Annotation Process}

We manually annotated almost $10000$ instruments by using the conventional method of rectangular bounding boxes. To this end we have used the open-source \texttt{Cytomine} software \cite{maree2016collaborative}, a rich web environment that allows highly collaborative analysis of multi-gigapixel imaging data. Originally developed for facilitating the task of image annotation in biomedical informatics, \texttt{Cytomine} has already been widely used for the annotation and creation of several datasets \cite{mormont2018comparison}. However, it is worth noting that its use within the present study is among the very first ones which uses the software outside the context of large-scale biomaging data. Regarding the labeling process itself, all the individual instruments within MINERVA have been unambiguously identified and labeled by using their MIMO codes. The MIMO (Musical Instrument Museums Online) initiative is an international consortium, well known for its online database of musical instruments, aggregating data and metadata from multiple heritage institutions \cite{dolan2017mimo}. An important contribution of \citet{dolan2017mimo} is the development of a uniform metadata documentation standard for the field, including a multilingual vocabulary that can be used for identifying musical instruments in an interoperable manner. We have followed this metadata standard and manually labeled the previously collected images within \texttt{Cytomine} as visually represented in Fig. \ref{}.

\subsection{Versions and Splits}

MINERVA comes in four, different, increasingly complex, versions: \texttt{Minerva-0} which is arguably the easiest version of the dataset where the target task $\mathcal{T}_T$ simply consists in detecting whether a musical instrument is present within an image or not. We therefore do not yet consider the task of predicting the class of the detected instrument. The second version of the dataset is \texttt{Minerva-Hypernym} where the goal is that of detecting all the images present within \texttt{Minerva-0} and classify them according to their hypernym categories. All instruments present within MINERVA correspond to $5$ different hypernyms which define them as: `stringed instruments", `wind instruments", `percussion instruments", `keyboard instruments" and `electronic instruments'. The last two versions of MINERVA are \texttt{Minerva-5} and \texttt{Minerva-10} where the goal is to detect and classify the instruments depicted in the images according to the top 5 or top 10 most occurring classes. These classes are: `Lute", `Harp", `Violin", `Trumpet", `Shawn", `Bagpipe", `Organ", `Horn", `Rebec" and `Lyre". Naturally, in \texttt{Minerva-5} we only consider the first $5$ of such classes, whereas in \texttt{Minerva-10} we consider all $10$ of them. Each version of the dataset comes with its own training, validation and testing splits, where we offer the guarantee that at least one of the instrument classes in the task is represented in each of the splits. Additionally, the splits are stratified so that the class distribution is approximately the same in each split. The number of images per split in each version is summarized in Table \ref{table:minerva_splits} where $N_t$ corresponds to the amount of images present within the split, whereas $I_t$ denotes the number of total instruments. The hypernym version of the dataset is not reported as it shares the same images and splits as \texttt{Minerva-0} (they both contain all instruments), however a distribution of the hypernym classes within \texttt{Minerva-Hypernym} is reported in Fig. \ref{fig:hypernym_distribution}. All splits have been created with the \texttt{scikit-learn} software \cite{pedregosa2011scikit} by using $50\%$ of the images for training, and the remaining $50\%$ for validation and testing ($25\%$ respectively). 

\begin{table}
\input{./Tables/Chapter05/minerva_splits.tex}
\caption{An overview reporting how many images $N_t$ and instruments $I_t$ are present within the splits of the \texttt{Minerva-0, Minerva-5} and \texttt{Minerva-10} versions of the MINERVA dataset.}
\label{table:minerva_splits}
\end{table}

\begin{figure}[htb!]
	\input{./Tables/Chapter05/hypernym_distribution.tex}
	\caption{A visual representation of the distribution of the hypernym classes that are present within \texttt{Minerva-0} and that define the \texttt{Minerva-Hypernym} benchmark.}
	\label{fig:hypernym_distribution}
\end{figure}


\section{Benchmarking}
\label{sec:benchmarking}

While MINERVA has been created with the primary intention of serving as a novel dataset for object detection, it can nevertheless still be used for testing the classification performance of convolutional neural networks as well. Although this task is certainly easier than the one of object detection, it is still of high interest as it provides a novel benchmark for further characterizing the transfer learning properties of neural networks that we started studying in the previous chapter. Therefore, we hereafter report results both for \textcolor{RoyalBlue}{classification} experiments as for \textcolor{RoyalBlue}{object detection} experiments. In the first section we consider the target task $\mathcal{T}_T$ of classifying the bounding boxes that have been annotated in MINERVA as standalone images, while in the second section we aim at both detecting and classifying the content of the potentially detected bounding boxes. We hereafter describe the experimental protocol used for both sets of experiments into detail.

\subsection{Classification}
The experimental setup used for the classification experiments largely builds on top of the study that we presented in Chapter \ref{ch:tl_natural_to_non_natural}. We continue to explore whether popular neural architectures, that have obtained state of the art results on the ImageNet benchmark, are able to perform equally well when they are trained on datasets of non natural images. To this end, we again consider the well known VGG19 \cite{simonyan2014very}, InceptionV3 \cite{szegedy2016rethinking} and ResNet50 \cite{xie2017aggregated} neural architectures. As done in the previous chapter we keep investigating the effect that different weight initialization strategies have on the final performance of the networks, with the goal of further characterizing the potential benefits that can come from adopting transfer learning. Specifically, we train the three considered neural architectures by following three different initalization strategies: `random" which simply initializes the model's parameters after following a initalization strategy, `ImageNet" which instead uses the weights that have been obtained after training the networks on the ImageNet source task $\mathcal{T}_S$, and `RijksNet" which are the models we trained both on the ImageNet dataset and on the Rijksmuseum collection, and that were also used for the final experiments of the previous chapter. For this study we only fine tune the networks, while we do not explore their performance when they are used as off the shelf feature extractors. We train all networks with the Adam optimizer \cite{kingma2014adam} and by using an initial learning rate of $0.001$. As already done for the previous study, we again controlled the training process by using early stopping and by interrupting the training regime as soon as the validation loss did not decrease for five epochs in a row. Naturally, all networks minimize the categorical crossentropy loss function.

\subsection{Object Detection}

For this set of experiments we explore the performance of a YOLO object detector \cite{}, a popular neural architecture that has obtained state of the art results on the MS-COCO object detection benchmark. YOLO treats the task of object detection as a standard regression problem by dividing an image into a $S\times S$ grid and by predicting for each grid cell $B$ bounding boxes and $C$ class probabilities. The main assumption behind YOLO is that any of the $S\times S$ cells contains at most the center of one single object, therefore for every image, cell index $i=1,...,S\times S$, predicted box $j=1,...,B$ and class index $c=1,...,C$ we have the following components: 

\begin{itemize}
	\item $\mathbb{1}_{i}^\text{obj}$ which is $1$ if there is an object in cell $i$, and $0$ otherwise;
	\item $\mathbb{1}_{i,j}^\text{obj}$ which is $1$ if there is an object in cell $i$ and predicted box $j$ that is the most fitting one, whereas is $0$ otherwise; 
	\item $p_{i,c}$ which is $1$ if there in an object of class $c$ in cell $i$, and 0 otherwise;
	\item $x_i,y_i,w_i,h_i$ which are the coordinates of an annotated bounding box that are defined only if $\mathbb{1}_{i}^{\text{obj}}=1$;
	\item $c_{i,j}$ which is the IoU between the predicted box and the ground truth target.
\end{itemize}

At training, YOLO computes the value of the $\mathbb{1}_{i,j}^{\text{obj}}$ for each image together with the respective $c_{i,j}$, and then minimizes the following multi-part loss function:

\begin{aligned}
& \lambda_\text{coord} \sum_{i=1}^{S \times S} \sum_{j=1}^B \mathbb{1}_{i,j}^\text{obj} \left( (x_i - \hat{x}_{i,j})^2 + (y_i - \hat{y}_{i,j})^2 + (\sqrt{w_i} - \sqrt{\hat{w}_{i,j}})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_{i,j}})^2\right)\\\\
& + \lambda_\text{obj} \sum_{i=1}^{S \times S} \sum_{j=1}^B \mathbb{1}_{i,j}^\text{obj} (c_{i,j} - \hat{c}_{i,j})^2 + \lambda_\text{noobj} \sum_{i=1}^{S \times S} \sum_{j=1}^B (1-\mathbb{1}_{i,j}^\text{obj}) \hat{c}_{i,j}^2  \\\\
& + \lambda_\text{classes} \sum_{i=1}^{S \times S} \mathbb{1}_i^\text{obj} \sum_{c=1}^C (p_{i,c} - \hat{p}_{i,c})^2 
\end{aligned}

where $\hat{p}{i,c}$, $\hat{x}{i,j}$, $\hat{y}{i,j}$, $\hat{w}{i,j}$, $\hat{h}{i,j}$ and $\hat{c}{i,j}$ are the predictions of the network.

In our experiments we use the YOLO-V3 version of the network which we initialize with the weights that are obtained after training the network on the MS-COCO dataset. Regarding the stochastic optimization procedure we use two different optimizers: we train the network with the Adam optimizer for the first 10 epochs, while we then use the RMSprop optimizer for the remaining training epochs, which are again controlled through early stopping. To assess the final performance of the model, we follow an evaluation protocol that is typical for object detection problems in CV \cite{lin2014microsoft}. Each detected bounding box is compared to the bounding box which has been annotated on the \texttt{Cytomine} platform. We only consider bounding boxes for which the confidence level is $\geq 0.05$, following the protocol established by \citet{everingham2010pascal}. We then compute the "Intersection over Union" (IoU) for measuring how much the detected bounding-boxes differ from the ground-truth ones. To assess whether a prediction can be considered as a true positive or a false positive, we define two, increasingly restrictive metrics: first, IoU $\geq10$ and, secondly, IoU $\geq50$. This approach is inspired by the work of \citet{gonthier2018weakly}, where the authors report results for both IoU thresholds when assessing the performance of their weakly supervised learning system on the IconArt dataset.

\section{Results}
\label{sec:results}

\subsection{Quantitative Analysis}
\paragraph{Classification}

We start by discussing the results obtained with our classification experiments. The performance of all models is reported in Tables \ref{table:minerva_no_tl_results}, \ref{table:minerva_tl_results} and \ref{table:minerva_rijks_results}, where we present the accuracy that the networks have obtained on the different Minerva testing sets, together with their respective F-1 scores. To this end for a given class $c$, a ground truth label $y$ and a model's prediction $\hat{y}$, let us introduce the notions of precision and recall. The first is computed as:
\begin{equation}
	\text{P}(c)=p(y=c|\hat{y}=c)=\frac{TP}{TP+FP},
\end{equation}
while the latter as
\begin{equation}
	\text{R}(c)=p(y=c|\hat{y}=c)=\frac{TP}{TP+FN}.
\end{equation}
Both quantities can be used for computing the F-1 score as follows:
\begin{equation}
	\text{F-1}(c)=2 \cdot \frac{\text{P}(c)\cdot\text{R}(c)}{\text{P}+\text{R}}.
\end{equation}
Similarly to the results reported in Chapter \ref{ch:tl_natural_to_non_natural} we again report the best performing architecture in a green cell, while the second best performing model in a yellow one.  

We can start by observing that among all the results presented in the three different tables, the best performing models are either the ones reported in Table \ref{table:minerva_tl_results}, or the ones presented in Table \ref{table:minerva_rijks_results}. This confirms the results that were presented in the previous chapter: fine tuning pre-trained models yields significantly better results that training models from scratch, even when the source and the target tasks can be particularly different (as is the case for musical instruments classification). While the results of this study confirm the conclusions that were drawn at the end of Chapter \ref{ch:tl_natural_to_non_natural}, they also provide some additional insights that were not observed before. First, it appears that the best performing architecture is not ResNet50 anymore, but rather the arguably older InceptionV3, a result which seems to suggest that there is no overall best performing architecture for all target tasks $\mathcal{T}_T$, and that the best architecture is highly problem dependent. Second, and perhaps arguably more surprising, we can also see that differently from what was observed in the last experiment in Chapter \ref{ch:tl_natural_to_non_natural}, it appears to be more beneficial to transfer models pre-trained on ImageNet only, instead of models that are additionally trained on the Rjksmuseum collection. Indeed, as can be observed by the results presented in Tables \ref{table:minerva_tl_results} and \ref{table:minerva_rijks_results}, the latter pre-training strategy outperforms the first one only when the ResNet50 and the InceptionV3 architectures are trained on the \texttt{Minerva-5} benchmark. These results can be explained as follows: in Chapter \ref{ch:tl_natural_to_non_natural} the target task $\mathcal{T}_T$ tackled with a model pre-trained on the Rijksmuseum collection corresponded to the original source task $\mathcal{T}_S$ (classification of `types" and `artists"). In these experiments however, albeit coming from similar domains, the considered source task $\mathcal{T}_S$ and target task $\mathcal{T}_T$ are unrelated, which could work in favor of an arguably more general ImageNet weight initialization.


\begin{table}
	\caption{Results obtained when classifying the bounding boxes of the three different MINERVA benchmarks with models that do not come as pre-trained on any sort of source task $\mathcal{T}_S$. We can see that their performance is significantly worse than the one that is obtained when the same models come as pre-trained (see Table \ref{table:minerva_tl_results} and Table \ref{table:minerva_rijks_results}).}
\resizebox{\columnwidth}{!}{%
	\input{./Tables/Chapter05/random_classification_results.tex}%
}
\label{table:minerva_no_tl_results}
\end{table}



\begin{table}
	\caption{Results obtained when classifying the bounding boxes of the three different MINERVA benchmarks after adapting transfer learning and considering the ImageNet dataset as the only source task $\mathcal{T}_S$. We observe that, compared to the results presented in Table \ref{table:minerva_no_tl_results}, this approach yields significant benefits, therefore confirming the results presented in Chapter \ref{ch:tl_natural_to_non_natural}.}
\resizebox{\columnwidth}{!}{%
	\input{./Tables/Chapter05/imagenet_classification_results.tex}%
}
\label{table:minerva_tl_results}
\end{table}


\begin{table}
	\caption{Results obtained when classifying the bounding boxes of the three different MINERVA benchmarks after adapting transfer learning and considering the ImageNet and the Rijksmuseum collection as source domains $\mathcal{D}_S$. Similarly to what was observed in Table \ref{table:minerva_tl_results}, we can again see that transfer learning yields significant benefits although this weight initialization strategy does not outperform the more common ImageNet one.}
\resizebox{\columnwidth}{!}{%
	\input{./Tables/Chapter05/rijksnet_classification_results.tex}%
}
\label{table:minerva_rijks_results}
\end{table}


\paragraph{Object Detection}


\begin{table}
	\caption{Average Precision ($\%$) obtained when fine-tuning a pre-trained YOLO-V3 object detector on the \texttt{Minerva-Hypernyms} dataset. We can observe that satisfying results are obtained for both IoU thresholds when it comes to the detection of stringed instruments, whereas detecting the remaining four hypernyms of MINERVA appears to be much more challenging.}
\resizebox{\columnwidth}{!}{%	
	\input{./Tables/Chapter05/hypernyms_detection.tex}%
}
\label{table:minerva_detection_hypernyms}
\end{table}


\begin{table}
	\caption{Average Precision ($\%$) obtained on the \texttt{ Minerva-5} benchmark. We can observe that the fine-tuned model successfully detects `Harps", `Lutes" and `Violins", whereas the detection of `Shawns" and `Trumpets" can be improved.}
\resizebox{\columnwidth}{!}{%	
	\input{./Tables/Chapter05/minerva_5_detection.tex}%
}
\label{table:minerva_5_detection}
\end{table}


\begin{table}
	\caption{Average Precision ($\%$) obtained on the \texttt{Minerva-10} benchmark. Similarly to what was presented in Table \ref{tab:minerva_5_detection}, we can again observe that the model successfully detects the first three most occurring instruments within the dataset, whereas it appears to perform poorly on the remaining instrument classes.}
\resizebox{\columnwidth}{!}{%	
	\input{./Tables/Chapter05/minerva_10_detection.tex}%
}
\label{table:minerva_10_detection}
\end{table}





\begin{figure}[htb!]
	\scalebox{0.8}{\input{./Tables/Chapter05/detection_experiment_2.tex}}
	\caption{}
	\label{fig:detection_experiment_2}
\end{figure}



\begin{figure}[htb!]
	\scalebox{0.8}{\input{./Tables/Chapter05/detection_experiment_3.tex}}
	\caption{}
	\label{fig:detection_experiment_3}
\end{figure}


\begin{figure}[htb!]
	\scalebox{0.8}{\input{./Tables/Chapter05/detection_experiment_4.tex}}
	\caption{}
	\label{fig:detection_experiment_4}
\end{figure}




\subsection{Qualitative Analysis}
We now characterize the performance of the aforementioned fine-tuned models from a qualitative perspective. 

\paragraph{Object Classification}


\paragraph{Object Detection}
Regarding the models trained for object detection, we visually investigate the quality of the predictions on the IconArt dataset \cite{gonthier2018weakly}, a database of $\approx 6000$ paintings that has been collected with the aim of detecting classes that are specific to the analysis of artworks. Among such classes IconArt tackles the detection of `angels", `Jesus" and `Mary", or more simply `ruins". IconArt, however, does not come with any ground truth labels when it comes to the task of instruments detection, as the dataset has been built for different purposes. Yet, musical instruments might still be depicted within its images and trying to detect them corresponds to a nice proof of concept that can show the benefits of deploying Minerva pre-trained models to different artistic collections. In Fig. \ref{fig:wikiart_detections} we show some successful examples of detections that were obtained after testing the performance of a YOLO-V3 model that was fine-tuned on the \texttt{Minerva-Hypernym} benchmark. We see that the model is indeed able to successfully detect musical instruments within this new artistic collection, a result that can be exploited by art historians that are interested in the study of musical instruments .   

\begin{figure}[ht!]
\centering
  \includegraphics[width=\linewidth]{./Images/Chapter05/wikiart_detections}
  \caption{Some examples of successful detections that have been obtained on the IconArt dataset with a model fine-tuned on the \texttt{Minerva-Hypernym} benchmark.}
  \label{fig:wikiart_detections}
\end{figure}

While these results are certainly nice and encouraging, it is arguably of even larger interest analyzing the model's detections which are erroneous. To this end, we have manually identified the incorrect model's predictions, and grouped them into three different categories. This process resulted into novel insights that, at least in part, explain the performance of the models which we have quantitatively assessed in the previous section. First, we have noticed that the model strongly gravitates towards the detection of `stringed instruments" (a result which was already observed in Fig. \ref{fig:detection_experiment_1}). We believe that such detections are driven by the large presence of dual, conic contour curves of naked women's bodies that are reminiscent of the resonance box of guitar-like instruments (see Fig. \ref{fig:false_positives_1}). 

Second we have observed that long, often martial objects such as swords, arrows and spears are mistakenly detected as `wind instruments" as the shape between such objects and the one of instruments like `shawns" is indeed very similar and hard to distinguish even for humans (see Fig. \ref{fig:false_positives_2})



\begin{figure}[ht!]
\centering
  \includegraphics[width=\linewidth]{./Images/Chapter05/false_positives_1}
  \caption{Examples of false detections of `stringed instruments" within some images representing `nudity" that are part of the IconArt dataset.}
  \label{fig:false_positives_1}
\end{figure}


\begin{figure}[ht!]
\centering
  \includegraphics[width=\linewidth]{./Images/Chapter05/false_positives_2}
  \caption{}
  \label{fig:false_positives_2}
\end{figure}


\begin{figure}[ht!]
\centering
  \includegraphics[width=\linewidth]{./Images/Chapter05/false_positives_3}
  \caption{}
  \label{fig:false_positives_3}
\end{figure}





\section{Discussion and Critical Analysis}
\label{sec:discussion}






\section{Future Work: towards more benchmarks}
\label{sec:future_work}
