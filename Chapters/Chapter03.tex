\chapter{Transfer Learning}
\label{ch:transfer_learnimg}

\begin{remark}{Outline}
	We now present Transfer Learning (TL), a machine learning methodology that aims at creating algorithms that are capable of retaining and reusing previously learned knowledge when getting trained on new, unseen problems. Most of the contributions presented within this dissertation are motivated by TL, therefore, we now introduce the reader to this specific learning paradigm with the goal of providing him/her with all the preliminary knowledge that is necessary for fully understanding the research that will be presented in the coming chapters. We start with a gentle introduction to TL in Sec. \ref{sec:tl_introduction} where we present the main concepts underlying TL and explain why it is desirable to have machine learning models that are transferable. We then show in Sec. \ref{sec:rationale} some practical, high level, examples that visually represent the benefits that come from adopting TL strategies in machine learning. We will then provide more rigorous, mathematical definitions of TL in Sec. \ref{sec:definitions} where we will characterize TL both for supervised learning as for reinforcement learning. In Sec. \ref{sec:literature_review} we thoroughly review how TL has already been studied by the deep learning community, and we will again focus on supervised learning and on reinforcement learning. We end this chapter with Sec. \ref{sec:relevance} where we summarize the most relevant TL concepts that underpin the contributions that will be presented throughout in this dissertation.  
\end{remark}

\section{Introduction}
\label{sec:tl_introduction}

\begin{itemize}
	\item cogsci perspective on nature of intelligence
	\item practical benefits real world applications
	\item computational costs 
	\item tl as a tool for understanding neural networks
\end{itemize}



\section{Rationale of Transfer Learning}
\label{sec:rationale}

Before mathematically defining TL we start by building some intuition and visually represent how one can observe the benefits of TL in practice. We assume that we would like to solve a certain task, and that we can choose between two different models: a model that has never dealt with any kind of task before, and a second model, which is identical to the first one with the major difference being that it has already seen a similar task in the past. Because of their different nature, we refer to the first model as a model that will be trained from scratch, and to the second model as a pre-trained model. Ideally, as mentioned in the previous section, we would like the latter model to perform better on the considered task that the first one. Yet, how can we asses if one model is better than the other one? As initially presented by \citet{langley2006transfer} and later generalized by \citet{lazaric2012transfer} we would like the performance of a pre-trained model to result into three possible improvements. If at least one of these improvements is observed while training, we can then consider the pre-trained model to be better than the scratch model. These improvements are the following:
\begin{itemize}
	\item \textcolor{RoyalBlue}{Learning Speed Improvements}: in this scenario the performance between a pre-trained model and a model trained from scratch is identical by the time training is finished, however when this kind of improvement appears we observe that the pre-trained model converges faster than the model trained from scratch. An example of this TL improvement is presented in the first plot of Fig. \ref{fig:tl_examples}. The goal is to train a model such that by the end of training its performance reaches a value of $200$. We can clearly see that both models manage to converge to this desired performance value, but that the pre-trained model manages to converge already after $\approx 50$ training iterations, whereas the model trained from scratch requires more than $200$ training iterations to perform similarly. Also note that the performance of both models at the beginning of training is identical, with both models starting with an initial performance of $\approx20$.

	\item \textcolor{RoyalBlue}{Jumpstart Improvements}: similarly to the previous case, also in this scenario there are no significant differences between the performance of a pre-trained model and the performance of a random model by the end of training. However this changes when we consider the very first training iteration. If jumpstart improvements appear, we can usually observe that when both models start their training process, the performance of the pre-trained model is much closer to the one that will be obtained by the end of training than the one of the scratch model. We visually report an example of this scenario in the third plot of Fig. \ref{fig:tl_examples}. In this case the goal is to train a model such that by the end of training its performance will be of $\approx -100$. We can clearly see that by the end of training both models are able to successfully achieve this goal, but that at the very first training iteration, the performance of the pre-trained model is significantly closer to the desired final performance ($\approx -250$) than the one of the scratch model ($\approx -450$). 
	\item \textcolor{RoyalBlue}{Asymptotic Improvements}: when this TL improvement appears, the final performance of a pre-trained model is significantly higher than the one of a model trained from scratch. It is worth noting that similarly to what happens when learning speed improvements are observed, also in this case the performance of both models is identical when training begins, and that this TL improvement only presents itself after several training iterations. A visualization of this TL improvement can be observed in the last plot of Fig. \ref{fig:tl_examples} where we can observe that for the first $\approx 20$ training iterations there are no differences in terms of performance between a pre-trained model and a model trained from scratch. However the more training iterations are performed, the more the pre-trained model starts outperforming the model trained from scratch, reaching a final performance that is almost twice as good by the $100\text{th}$ training iteration.
\end{itemize}

\input{./Results/Chapter03/tex/tl_examples.tex}

While all the improvements presented in Fig. \ref{fig:tl_examples} are all highly desirable, it is  worth noting that some of them can be more preferable than others. In fact, the potential benefits of TL highly depend from the problem at hand. As an example, let us consider a training situation where the main goal is that of minimizing the overall training time of a model. In this particular case, jumpstart and learning improvements are more desirable than asymptotic improvements, since the latter improvement might not result into a model that converges to a desired solution faster. On the other hand, if the main objective is that of training a model which performs as best as possible, than evidently, asymptotic improvements are preferred. It is also worth noting that the examples presented in Fig. \ref{fig:tl_examples} are not mutually exclusive, and that the benefits of TL can present themselves as a combination of improvements, rather than in the form of a single, isolated, improvement. 

Now that we have introduced the key ideas behind TL, and presented why adopting TL training strategies is beneficial we move to formally characterizing this machine learning paradigm both from a supervised learning perspective as from a reinforcement learning one.  


\section{Mathematical Definitions}
\label{sec:definitions}

As is common throughout this thesis we start by focusing on the supervised learning case.

\subsection{Supervised Learning}
The first two definitions that we provide are borrowed from \citet{zhuang2020comprehensive} and are the ones of \textcolor{RoyalBlue}{domain} and \textcolor{RoyalBlue}{task}. The first one is defined as follows:
\begin{definition}
	A domain $\mathcal{D}$ is the combination between an input space $\mathcal{X}$ and a marginal distribution $P(X)$, $\mathcal{D} = \{\mathcal{X},P(X)\}$ where $X$ denotes an instance set defined as $X=\{\vec{x}|\vec{x_i}\in \mathcal{X}, i =1, \cdots, n \}$.
\end{definition}
Examples of domains can be natural images, time-series data, biomedical markers and so on. In supervised learning we know that each domain is associated to its respective task, which is representative of the problem we would like to solve. A task is defined as follows:
\begin{definition}
	A task $\mathcal{T}$ consists of a label space $\mathcal{Y}$ and a decision function $f$, i.e., $\mathcal{T}=\{\mathcal{Y},f\}$. The decision function $f$ is implicit and can only be learned by sampling data from $\mathcal{X}$ which comes in the form of $\{x_i,y_i\}$ pairs where $x_i\inX$ and $y_i \in \mathcal{Y}$.
\end{definition}
Examples of possible decision functions $f$ can be classifiers which categorize natural images in their respective classes, or regressors that are able to predict future values in a time-series. 
The key concept underlying TL is that, differently from the common supervised learning scenario
where the only information that is available to a model are one domain and one task, we now have access to at least two domains. The one that corresponds to the task we would like to solve, called the \textcolor{RoyalBlue}{target-domain} $\mathcal{D}_T$, and an additional, possibly related, domain that comes with the name of the \textcolor{RoyalBlue}{source-domain} $\mathcal{D}_S$. With all these concepts in place we can now define \textcolor{RoyalBlue}{Transfer Learning} as:
\begin{definition}
Given one, or more, observations corresponding to $m^s \in \mathds{N}^{+}$ source domain(s) and tasks(s) (i.e., $\{(\mathcal{D}_{S}_{i}, \mathcal{T}_{S}_{i}|i=1\cdots,m^s)\})$ and some additional observation(s) about $m^T \in \mathds{N}^{+}$ target domain(s) and task(s) (i.e. $\{(\mathcal{D}_{T}_{j},\mathcal{T}_{T}_{j}|j=1,\cdots,m^T)\})$, transfer learning utilizes the knowledge implied in the source domains to improve the performance of the learned decision functions $f^{T}_j(j=1,\cdots,m^T)$ on the target domain(s).
\end{definition}
As pointed out by \citet{pan2009survey} the condition that the source and the target domains might be different $\mathcal{D}_S \neq \mathcal{D}_T$ implies that either their respective input spaces are different as well $\mathcal{X}_S\neq\mathcal{X}_T$, or that their corresponding marginal distributions are different $P_s(X)\neq P_T(X)$. Similarly, if the tasks are different instead $\mathcal{T}_S\neq\mathcal{T}_T$, then either one between the output spaces, or the conditional distributions has to be different ($\mathcal{Y}_S\neq\mathcal{Y}_T$ or $P(Y_S|X_S)\neq P(Y_T|X_T))$.

Based on the consistency between the source and target input spaces $\mathcal{X}$, and the respective output spaces $\mathcal{Y}$, one can categorize TL into three following settings.

\paragraph{Inductive Transfer Learning}
This TL scenario is characterized by the fact that the target task $\mathcal{T}_T$ is different from the source task $\mathcal{T}_S$, while the source domain $\mathcal{D}_S$ and the target domain $\mathcal{D}_T$ might, or might not, be similar. As originally presented in \cite{pan2009survey} we define inductive transfer learning as:
\begin{definition}
	Given a source domain $\mathcal{D}_S$ and a learning task $\mathcal{T}_S$, and a target domain $\mathcal{D}_T$ and a learning task $\mathcal{T}_T$, inductive transfer learning aims to help improving the target predictive function $f_T(\cdot)$ in $\mathcal{D}_T$ by using the knowledge in $\mathcal{D}_S$ and $\mathcal{T}_S$, where $\mathcal{T}_S \neq \mathcal{T}_T$. 
\end{definition}
A key requirement of this type of TL is that some labeled data in the target domain is necessary for inducing the objective predictive function $f_T(\cdot)$. To build some more intuition about this kind of TL let us assume that we would like to train a model on our target task $\mathcal{T}_T$ which corresponds to recognizing what kind of Japanese letter is depicted in an image. Instead of training a model only on a dataset of letters, we instead start from a model that has already been trained to recognize digits, which will therefore be our source task $\mathcal{T}_S$. Note that in this case the source task and the target tasks are evidently different $\mathcal{T}_S \neq \mathcal{T}_T$ (classifying digits vs classifying letters), but that the input space of the source and target domains is the same $\mathcal{X}_S = \mathcal{X}_T$, since it corresponds to black and white images as represented in Fig. \ref{fig:inductive_tl}. Please note that in this example, since we use a model that is pre-trained on images representing digits, we assume that we have had access to some labeled data in the source domain in the past. However the definition of inductive transfer learning does not require labeled data within the source domain to be strictly necessary.

\begin{figure*}
 \minipage{0.45\textwidth}
    \includegraphics[width=\linewidth]{./Images/Chapter03/mnist_dataset.png}
  \endminipage\hfill
  \minipage{0.45\textwidth}%
    \includegraphics[width=\linewidth]{./Images/Chapter03/kuzushiji_mnist.jpg}
  \endminipage\hfill
  \caption{A visualization of two datasets that can be used for performing inductive transfer learning. On the left we represent instances of the popular MNIST dataset \cite{lecun1994mnist}, while on the right instances of the Kuzushiji-MNIST dataset \cite{clanuwat2018deep}. We see that both datasets share the same domain $\mathcal{D}_S = \mathcal{D}_T$ (black and white images), but are associated to different tasks $\mathcal{T}_S \neq \mathcal{T}_T$ (classification of digits vs classification of Japanese letters).}
  \label{fig:inductive_tl}
\end{figure*}


\paragraph{Transductive Transfer Learning}
Originally proposed by \citet{arnold2007comparative}, this type of TL is characterized by the fact that the source and target tasks are the same $\mathcal{T}_S = \mathcal{T}_T$, but their respective domains are different $\mathcal{D}_S \neq \mathcal{D}_T$. It is also possible that the feature spaces between domains are the same but, if that is the case, then the marginal probability distributions are different $P(X_S)\neq P(X_T)$. In its original formulation, \citet{arnold2007comparative} assumed that all unlabeled data in the target domain is available at training time, however we hereafter report the definition of \citet{pan2009survey} who instead relax this condition and require only a subset of unlabeled target data to be seen at training time.
\begin{definition}
	Given a source domain $\mathcal{D}_S$ and a learning task $\mathcal{T}_S$, and a target domain $\mathcal{D}_T$ and a learning task $\mathcal{T}_T$, transductive transfer learning aims to help improving the target predictive function $f_T(\cdot)$ in $\mathcal{D}_T$ by using the knowledge in $\mathcal{D}_S$ and $\mathcal{T}_S$, where $\mathcal{D}_S \neq \mathcal{D}_T$ and $\mathcal{T}_S = \mathcal{T}_T$. 
\end{definition}
As an example of transductive transfer learning let us assume that we would like to train a model to classify which type of clothing is depicted in an image. This time, differently from the inductive transfer learning case, the images in our target domain $\mathcal{D}_T$ are not black and white images anymore, but rather colored images, therefore they are defined over the RGB domain (see right image of Fig. \ref{fig:transductive_tl}). We now assume that we have access to a pre-trained model which has already been trained to classify the same type of clothes, with the main difference being that the images constituting the source domain $\mathcal{D}_S$ were black and white images (see left image of Fig. \ref{fig:transductive_tl}). If we now train this pre-trained model on our colored dataset we see that our setting fits the transductive transfer learning scenario: our considered domains are different $\mathcal{D}_S \neq \mathcal{D}_T$ (colored vs black and white images), but their respective tasks are the same  $\mathcal{T}_S \neq \mathcal{T}_T$, since a model is always trained to classify types of clothing.

\begin{figure*}
 \minipage{0.45\textwidth}
    \includegraphics[width=\linewidth]{./Images/Chapter03/fashion_mnist}
  \endminipage\hfill
  \minipage{0.45\textwidth}%
    \includegraphics[width=\linewidth]{./Images/Chapter03/colorful_fashion_mnist}
  \endminipage\hfill
  \caption{Two datasets that are representative of transductive transfer learning. On the left we show images coming from the Fashion-MNIST dataset \cite{xiao2017fashion}, while on the right we report instances of the same dataset that are colored. In this case tasks among datasets are shared $\mathcal{T}_S = \mathcal{T}_T$ (classification of clothes), but the respective images come from different domains $\mathcal{D}_S \neq \mathcal{D}_T$ (black and white vs RGB images).}
  \label{fig:transductive_tl}
\end{figure*}

Although this section focuses on supervised learning, we still report for the sake of completeness a definition of unsupervised transfer learning hereafter, and see how this kind of TL is linked to the types of TL that we have analyzed so far

\paragraph{Unsupervised Transfer Learning}
Arguably considered to be the most challenging, and the least explored type of TL, unsupervised transfer learning is characterized by the total absence at training time of labeled data in both the source domain and the target domain. As mentioned by \citet{pan2009survey} very little research work has so far explored this TL paradigm, with the only existing works exploring typical unsupervised learning topics such as clustering \cite{dai2008self, jin2011transferring, qian2015cluster} and dimensionality reduction \cite{wang2008transferred, zhu2013self, zhu2016robust}. Unsupervised transfer learning is defined as follows:
\begin{definition}
	Given a source domain $\mathcal{D}_S$ and a learning task $\mathcal{T}_S$, and a target domain $\mathcal{D}_T$ and a learning task $\mathcal{T}_T$, unsupervised transfer learning aims to help improving the target predictive function $f_T(\cdot)$ in $\mathcal{D}_T$ by using the knowledge in $\mathcal{D}_S$ and $\mathcal{T}_S$, where $\mathcal{T}_S \neq \mathcal{T}_T$ and $\mathcal{Y}_S$ and $\mathcal{Y}_T$ are not observable. 
\end{definition}
Based on this definition we can note that unsupervised transfer learning is more similar to inductive transfer learning than to to transductive transfer learning since we again assume that the source and the target tasks are different $\mathcal{T}_S \neq \mathcal{T}_T$.


\subsection{Reinforcement Learning}
\label{sec:reinforcement_learning_tl}
When it comes to the reinforcement learning setup, the aforementioned definition of TL slightly changes, and becomes arguably less general. Recall from Chapter \label{ch:reinforcement_learning} that in RL, the main goal is that of training an agent such that it becomes able to interact with its environment, a problem that is modeled with Markov Decision Process (MDP). It follows that in the RL context, the previously introduced concept of domain $\mathcal{D}$ (which could come in numerous flavors), now comes in the arguably more strictly defined form of a MDP $\mathcal{M}$. Just like domains, MDPs can either be representative of a source task, $\mathcal{M_S}$, or of a target $\mathcal{M}_T$ task, with the latter case corresponding to the main RL problem we are interested to solve. Also the previously introduced predictive function $f(\cdot)$ is now defined more precisely, since it corresponds to the task of learning an optimal policy $\pi^*$ for $\mathcal{M}_T$. Based on these concepts we give the following definition of TL for reinforcement learning that is adapted from the one proposed by \citet{zhu2020transfer}.

\begin{definition}
	Given a source MDP $\mathcal{M}_S$ and a target MDP $\mathcal{M}_D$, transfer learning in reinforcement learning aims to learn an optimal policy $\pi^{*}$ for $\mathcal{M}_D$ by exploiting some prior knowledge related to $\mathcal{M}_S$, $\mathcal{K}_S$, together with the knowledge that underlies $\mathcal{M}_T$, $\mathcal{K}_T$ such that:
	\begin{align}
		\pi^{*} = \argmax_{\pi} \mathds{E}_{s \sim\mu_{0}^{t},a\sim\pi}\bigl[Q^{\pi}_\mathcal{M}(s,a)\bigr],
	\end{align}
	where $\pi = \zeta(\mathcal{K}_S \sim \mathcal{M}_S, \mathcal{K}_T \sim \mathcal{M}_T): \mathcal{S}^t \rightarrow \mathcal{A}^t$ is a function mapping from the states to actions for $\mathcal{M}_T$ learned thanks to both $\mathcal{K}_S$ and $\mathcal{K}_T$.
\end{definition}

Note that differently from the supervised learning case we are now making explicit use of the concept of knowledge $\mathcal{K}$, which is what we would like to retain when moving from a source MDP $\mathcal{M}_S$ to a target MDP $\mathcal{M}_T$. We do this because RL is a machine learning paradigm that is, arguably, more complex than the supervised learning one. A complexity which stems from the fact that in RL there are concepts such as e.g. rewards and policies, which are, by definition, not present in the supervised learning setup. As a result, $\mathcal{K}$ can come in forms that it cannot take in SL, and correctly identifying which kind of knowledge to transfer between $\mathcal{M}_S$ and $\mathcal{M}_T$ is just as important as developing a method that successfully transfers this knowledge in the first place. As mentioned by \citet{lazaric2012transfer} one can transfer $\mathcal{K}$ coming in the following forms.

\paragraph{Transfer of Instances:} in this scenario $\mathcal{K}$ corresponds to RL trajectories coming in the form $\langle s_t, a_t, r_t, s_{t+1}\rangle$ and that have been collected on one, or possibly multiple, source MDPs $\mathcal{M}_S$. Such trajectories can then be used both in a model-based RL setting, as done by \citet{taylor2008transferring}, or for speeding up the process of learning a value function as described in \cite{lazaric2008transfer} and \cite{laroche2017transfer}. Ideally, transferring RL trajectories should result into algorithms that are highly sample efficient, although it is worth noting that this property, albeit desirable, can constrain the source task $\mathcal{M}_S$ and the target task $\mathcal{M}_T$ to have similar transition models and reward functions. This instance of TL is usually used within the batch RL setup, where gathering experience samples for $\mathcal{M}_T$ can be particularly expensive or time consuming, which is a constraint that does not hold for $\mathcal{M}_S$. The typical challenge then consists in correctly identifying which samples coming from $\mathcal{M}_S$ are the most informative ones for solving $\mathcal{M}_T$ as for example studied by \citet{tirinzoni2018importance}.

\paragraph{Transfer of Parameters:} as we have seen in Chapter \ref{ch:reinforcement_learning} it is often desirable to integrate RL algorithms with parametric function approximators. The main goal is then to train these parametric functions such that they will learn an approximation of an optimal value function or policy. When parameter transfer is performed the main idea is to start solving the problem modeled by the target task $\mathcal{M}_T$ with a function that instead of being initialized with random parameters, is initialized with parameters that have been learned on a certain source task $\mathcal{M}_S$. Examples of knowledge $\mathcal{K}$ that fit this description are the parameters $\theta$ of a pre-trained Deep Q-Network, or the parameters that model an Actor-Critic agent.  
\bigbreak
While both representations are arguably the most popular ones, it is important to mention that as described by \citet{tirinzoni2018transfer} there are alternative possible ways of representing $\mathcal{K}$. Among such ways $\mathcal{K}$ can come in the form of e.g., features \cite{mehta2008transfer,barreto2017successor}, rewards \cite{konidaris2006autonomous,schaal2004estimating} and options \cite{singh2005intrinsically}.

\section{Deep Transfer Learning}
\label{sec:literature_review}

We now present the field of `Deep Transfer Learning" (DTL), a machine learning paradigm that aims at performing TL when a source model comes in the form of a pre-trained convolutional neural network. We start by describing how one can exploit the availability of a pre-trained network when training a model on a desired target task, and then present a thorough literature review which describes the most successfull applications that have so far been achieved by the DTL community.

\subsection{General Framework}
\label{sec:tl_general_framework}

Let us assume we would like to train a neural network on a regression task. Instead of initializing its weights randomly we instead initialize it with weights that have already been optimized on a certain source task. We refer to the parameters of this pre-trained model as $\theta_S$, where $S$ stands again for `source". When training the network on the target task, the main challenge one has to address revolves around deciding up to what extent we are willing to change the parameters $\theta_S$ through stochastic gradient descent. In practice this corresponds to deciding how much of the knowledge contained in $\theta_S$ we would like to retain when training our pre-trained network on $\mathcal{T}_T$. Typically, one can choose between two different approaches:  

\begin{itemize}
	\item \textcolor{RoyalBlue}{Off the Shelf Extraction:} in this setting most of the parameters $\theta_S$ of the pre-trained model are not changed when the network gets trained on the target task. Instead, the only weights that get optimized are the ones that are responsible for the final predictions of the model. In the aforementioned regression example these weights would correspond to the ones which parametrize the final output neuron of the network. Similarly, if we would be dealing with a classification problem, then we would only train the weights that constitute the final softmax layer of the model, since it is the part of the network which is responsible for outputting the predicted output classes. Since this approach does not involve any backpropagation operations, it is particularly desirable when computational resource are limited. In fact, one only needs to compute the forward pass in order to get the predictions from the pre-trained model. However this approach also comes with the major limitation that it does not allow the network to adapt to the target task at all, and assumes that all the knowledge that is required for solving $\mathcal{T}_T$ is already contained within $\theta_S$. For example, in the context of convolutional neural networks, this corresponds to a model that has already learned all the features that are necessary for solving $\mathcal{T}_T$ when getting trained on $\mathcal{T}_S$.   

	\item \textcolor{RoyalBlue}{Fine-Tuning:} when this approach is adopted all the parameters $\theta_S$ that have been learned on the source task get optimized when training the network on the target task. Evidently this training strategy is computationally more expensive than the previous one, since it involves all of the steps that characterize the successful training of neural networks that we discussed in Chapter \ref{ch:supervised_learning}. Despite being computationally more demanding, fine-tuning a network has the significant benefit of allowing a model to be target task specific. In fact, whilst training, part of the knowledge contained within $\theta_S$ can be `forgotten" which will result into a new set of parameters, $\theta_T$, that can perform better on $\mathcal{T}_T$ than $\theta_S$. From a practical perspective, it is important to train the model in such a way that the knowledge contained within $\theta_S$ does not get `forgotten" too quickly, while at the same time ensuring that the network stays flexible enough for successfully learning the target task. One possible way of achieving this is by using small learning rate values for training.      
\end{itemize}


While both approaches come with pros and cons, the latter option is usually preferred if enough computational resources are available. In fact, as we will see in the coming section, the community seems to agree that it often results into better final performance. When DTL strategies are adopted it is usually good practice to compare the performance of a pre-trained model with the performance that is obtained by a model that is initialized randomly, and that gets therefore trained from scratch. Throughout this thesis we will constantly characterize the benefits of adapting TL strategies from this perspective, therefore, to add even more clarity to the concepts presented in this section, we also visually represent them in Fig. \ref{fig:network_training_approaches}. 

\begin{figure*}[!htb]
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/frozen_net.pdf}
\endminipage\hfill
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{./Images/Chapter04/fine_tuning_net.pdf}
\endminipage\hfill
\minipage{0.3\textwidth}%
  \includegraphics[width=\linewidth]{./Images/Chapter04/random_net.pdf}
\endminipage
\caption{A simplified visual representation of the different deep transfer learning training paradigms that are considered throughout this thesis. The first plot represents a model that comes as pre-trained on a source task but which will not update most of its weights during the training stage (represented in gray): the only trainable parameters of this network are the ones that parametrize the final layer of the model and that are represented in green. In the second plot we visually represent a model that is parametrized with weights that have been learned on a certain source task, and that get ``unfrozen'' when the network gets trained on the target task, therefore defining the model as fully trainable. Lastly we visualize a model that does not come as pre-trained on any source task, and that is therefore initialized with random weights instead (represented by the various colours). As mentioned throughout this chapter the main goal of TL is to obtain a model that if trained with the first two approaches results into a final performance that is better than the one that is obtained when training the last depicted network.}
\label{fig:network_training_approaches}
\end{figure*}


\subsection{Literature Review}

\paragraph{Convolutional Networks as Feature Extractors}
As soon as convolutional neural networks started to perform well on popular computer vision benchmarks research investigating whether these networks could be transferred and reused for novel tasks started to bloom. Among the different works exploring this research direction, one of the very first ones is that of \citet{donahue2014decaf} who evaluated whether features extracted from a convolutional neural network trained for image classification, could also be used for tackling CV tasks such as scene recognition and domain adaptation. \citet{donahue2014decaf} showed that this was indeed the case, and publicly released the pre-trained model under the name of \texttt{DeCAF} with the goal of stimulating the CV community to further investigate the extent to which the features learned by this network were transferable to novel tasks. Almost concurrently, similar conclusions about the transferability of pre-trained convolutional networks were drawn by \citet{oquab2014learning}, who showed the benefits of using a pre-trained convolutional model as feature extractor when dealing with object and action localization problems, and by \citet{zeiler2014visualizing} who first showed that on many problems it was more beneficial to simply train the final classification layer of a pre-trained network, than to train a randomly initialized model from scratch. While definitely promising, all these works restricted their experimental analysis to a relatively small set of CV problems, and it was only with the seminal work of \citet{sharif2014cnn} that the deep learning community realized how powerful pre-trained networks could be. \citet{sharif2014cnn} used the OTS features of the pre-trained \texttt{OverFeat} network \cite{sermanet2013overfeat} for tackling numerous challenging CV problems, and consistently reported a final performance that was superior than the one of the state of the art algorithms of the time. Next to providing a wealth of empirical evidence in support of using off the shelf features, their work also established the very first training protocol for combining high dimensional OTS features with linear classifiers, such as SVMs, and dimensionality reduction techniques such as principal component analysis. It did not take long before the scientific community started to investigate whether off the shelf features could also be used for problems outside the typical CV benchmarks, and therefore fully realized the potential of this TL approach. Among the very first practical applications, we mention the work of \citet{van2015off} who used the previously mentioned \texttt{DeCAF} model for (successfully) tackling the challenging medical task of pulmonary nodule detection. Along the same line of research equally good results were obtained within the medical domain by \citet{hernandez2018periocular} who tackled the problem of periocular recognition, and by \citet{nguyen2017iris} who considered the similar task of iris recognition. Further successful applications of OTS classification which go beyond the medical domain are the ones reported by \citet{sharma2015adapting}, who considered the handwriting recognition task of work spotting, and the one of \citet{van2015deep} who studied the task of gender classification. While all these research solely relied on an OTS feature extraction approach when addressing a CV problem, it is also worth noting that OTS features can be used in combination with more traditional CV feature engineering techniques such as SIFT \cite{lowe2004distinctive} and HOG \cite{dalal2005histograms}. This research direction has been successfully explored by e.g., \citet{wang2014action} who examined the task of human action understanding, and by \citet{zhong2016face} who addressed the problem of face localization.


\paragraph{On the Benefits of Fine-Tuning}
Modern deep learning frameworks such as \texttt{TensorFlow} \cite{abadi2016tensorflow} and \texttt{PyTorch} \cite{paszke2017automatic}, provide high level and easy to use APIs, that make it possible to create and train deep learning models even without a necessarily strong machine learning background. Among the main reasons that have made deep learning so accessible, there is the fact that the aforementioned deep learning libraries provide easy access to models that have already been trained on a large variety of CV tasks \cite{russakovsky2015imagenet, lin2014microsoft, everingham2010pascal}. As a result, using pre-trained neural networks has become increasingly easy, which is among the reasons that allowed the deep learning community to explore whether fine-tuning pre-trained models could result into better performance than simply using them as OTS feature extractors. It is easy to see how this research question is of high practical interest and why it has therefore been heavily explored by practitioners working at the intersection of machine learning and fields such as e.g., medicine \cite{tajbakhsh2016convolutional,ho2021evaluation}. Among the first works exploring whether it is beneficial to fine-tune pre-trained models over using them as simple feature extractors there is the one of \citet{tajbakhsh2016convolutional}. The authors consistently show that fine-tuning a pre-trained network outperforms the OTS feature extraction approach when it comes to four distinct medical imaging tasks and that, similarly to what was predicted by \citet{zeiler2014visualizing}, pre-trained networks outperform models that are trained from scratch. A similar conclusion has also been reached by \citet{mormont2018comparison}, who analyzed the same research question under the lens of image classification problems coming from the digital pathology field. They also show that fine-tuning yields better performance than OTS feature extraction, but they do not provide an answer to whether this TL strategy works better than training a network from scratch. The question of whether to fine-tune or not to fine-tune a neural network has also been explored outside from the CV domain. Among the different works, we mention the one of \citet{peters2019tune}, who address this question from a Natural Language Processing (NLP) perspective. In line with what has been observed by the CV community, they also highlight the significant benefits that can come from fine-tuning popular NLP models such as ELMo \cite{peters2018deep} and BERT \cite{devlin2018bert} as long as the source task is carefully chosen. By now, studies investigating the benefits coming from fine-tuning pre-trained models are countless and range over a large variety of domains that do not necessarily strictly involve CV problems \cite{ackermann2018using,dominguez2019transfer,george2017deep,boulanger2013audio,deng2013new,kong2020panns,zarrella2016mitre,howard2018universal,houlsby2019parameter}.


\paragraph{On the Role of ImageNet as $\mathcal{T}_S$} Throughout this chapter we have constantly referred to the concept of source domain $\mathcal{D}_S$ and source task $\mathcal{T}_S$, two key elements without which the entire field of TL would not even exist. Albeit in the previous paragraphs we have mentioned the task of image classification as source task $\mathcal{T}_S$ that can be used for pre-training convolutional networks, we have not explicitly described what this task consists of in practice. When adopting TL strategies for CV problems the most common and, by far successful, approach is that of relying on models that have been trained for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) \cite{russakovsky2015imagenet}. The ILSRVC dataset, more commonly referred to as the ImageNet dataset, is a collection of over one million natural images that are categorized among one thousand different classes. Until 2017 it was largely considered to be the most complex and challenging problem of all CV, and is among the main reasons which has yielded the deep learning community to develop most of the neural architectures that we described in Chapter \ref{ch:supervised_learning}. Due to the large amount of samples constituting the dataset, and the complexity of the tackled task, networks trained for the ILSVRC challenge are regularly used as pre-trained networks even when the target task $\mathcal{T}_T$ does not involve the classification of natural images. Intuitively, the reasons behind this choice are very straightforward: on the one hand it is safe to assume that some of the features that are learned by a network that receives as input more than one million images will, at least in part, correspond to the features the same network would have to learn when getting trained from scratch on a desired target task. On the other hand, it is also highly unlikely that the target task $\mathcal{T}_T$ will be more complex than the source task $\mathcal{T}_S$, since it is not common to deal with classification problems that involve more than 1000 classes. In this sense it is reasonable to assume that if a network performs well on $\mathcal{T}_S$, it should also perform well on $\mathcal{T}_T$, since the latter task is essentially easier than the former \cite{mensink2021factors}. Despite these intuitive explanations a large body of work has studied why it is beneficial to transfer ImageNet pre-trained models and what the factors of influence of this dataset are for TL. This question was first tackled by \citet{huh2016makes} who studied, among the other questions, how many examples and classes of the ImageNet dataset should be used for pre-training a model. Perhaps surprisingly they found that already half of the ImageNet data yielded a well performing pre-trained network, and that among the different 1000 classes it was already enough to pre-train a network on a subset of 127 of them. \citet{kornblith2019better} empirically studied the benefits of using ImageNet pre-trained models for 12 different classification problems, and found that the better a model performs on ImageNet, the better it transfers to new unseen tasks. A similar study, which however yielded slightly different results, is the one of \citet{he2019rethinking} who showed that there might not be significant differences in terms of final performance between using an ImageNet pre-trained network and a randomly initialized one, but that the first ones consistently converge faster than the latter. Finally the recent work of \citet{mensink2021factors} shows that ImageNet pre-trained models always outperform models that are trained from scratch, but that this dataset might not necessarily always be the best possible source task for pre-training. It is worth noting that all the aforementioned examples revolve around the field of supervised learning within CV, it naturally follows that for e.g., NLP tasks different pre-training strategies, and therefore different source domains, need to be considered. Since discussing these techniques would go beyond the scope of this dissertation we do not present them here, however we refer the reader to \cite{mikolov2013efficient,rosset2020knowledge,brown2020language,devlin2018bert} for a discussion of pre-training strategies outside the CV and supervised learning domain.

\paragraph{The Deep Reinforcement Learning Setting}
While within the supervised learning setting the body of work studying the transfer learning properties of neural networks is very large, the same cannot be said when it comes to reinforcement learning. Although as presented in Sec. \ref{sec:reinforcement_learning_tl} there exist many different approaches for performing TL in RL, the integration of such techniques with e.g., convolutional neural networks is much rarer. Perhaps the work that studies the TL properties of Deep Q-Networks in a flavor which is the closest to the TL approaches used in supervised learning that we presented in Fig. \ref{fig:network_training_approaches} is that of \citet{farebrother2018generalization}. The authors study whether convolutional neural networks that are trained with the DQN algorithm \cite{mnih2015human}, are capable of learning features that are robust enough to allow the algorithm to generalize across different tasks. The results, obtained on four different Atari 2600 games do not provide a clear answer to this question: when Deep Q-Networks come as pre-trained on a certain game and simply get fine-tuned on a new different game, the authors fail in observing the benefits that this TL approach typically yields in the supervised learning context. However, if networks get pre-trained in combination with typical supervised learning regularization techniques such as dropout \cite{srivastava2014dropout} and $l_2$, then the authors observe that fine-tuning these models results into a final performance that is better than the one of models trained from scratch. While certainly encouraging, these results were only obtained on a very small set of RL environments, and it is unclear whether these conclusions would hold if a larger set of benchmarks, and algorithms, would be tested. A similar study, which arguably presents the same limitations, is the one performed by \citet{tyo2020transferable}. On the same line with \citet{farebrother2018generalization} the authors study the effect of fine-tuning different pre-trained Rainbow agents \cite{hessel2018rainbow} that use different weight initialization strategies. Results, reported for three different Atari games show that fine-tuning is beneficial only for one single game, but do not explain the TL properties of pre-trained Deep Q-Networks any further. A more thorough and successful study is the one presented by \citet{parisotto2015actor}, where the authors also investigate the effect of fine-tuning pre-trained DRL agents and show that this TL strategy can result into significant benefits. Their study however presents some significant differences when compared to the aforementioned works. First, the DRL algorithm under scrutiny is a policy gradient algorithm, which as discussed in Chapter \ref{ch:reinforcement_learning} is part of a family of techniques that is significantly different from the value based RL algorithms DQN and Rainbow are part of. Second, their proposed Actor-Mimic algorithm, does not come as pre-trained on a single Atari game anymore but is pre-trained in a multi-task learning setting instead. The algorithm therefore deals with different source-tasks $\mathcal{T}_S$ during the pre-training stage, which is a strategy that results into algorithms that are more robust and suitable for TL \cite{kirkpatrick2017overcoming}.  


\section{Relevance for this Dissertation}
\label{sec:relevance}

With all the concepts above and the ones presented in Chapters \ref{ch:supervised_learning} and \ref{ch:reinforcement_learning}, we are now ready to end this first part of this dissertation by describing how all the aforementioned content will play a role throughout the rest of this thesis. 

First, we start by noting that all the quantitative results that will be presented from now on will be defined with respect to the three TL benefits that we described in Sec. \ref{sec:rationale}. No matter which kind of task we will be training neural networks for, we will always seek to identify at least one of the three possible benefits that can arise from adapting TL. The only chapter where we will not do this is Chapter \ref{ch:dqv_family_of_algorithms}, which instead will serve for introducing some novel DRL algorithms that will be studied from a TL perspective only in Chapter \ref{ch:dqn_transfer}.

When it comes to all the research performed within the supervised learning setting it is important to note that we will only consider the inductive transfer learning scenario. More specifically, we will always study the extent to which neural networks pre-trained on natural images can generalize to non-natural datasets. The content of these datasets, and therefore the considered target tasks $\mathcal{T}_T$, will change from chapter to chapter, and so will the source task $\mathcal{T}_S$ that will be used for pre-training. For the research involving reinforcement learning we will instead only consider the TL setting that studies the transferability of parameters presented in Sec. \ref{sec:reinforcement_learning_tl}. As a result, no matter whether we will tackle supervised learning problems or reinforcement learning ones, the experimental protocol that we will adopt will always fall within at least one of the TL training strategies that we have described in Sec. \ref{sec:tl_general_framework} and presented in Fig. \ref{fig:network_training_approaches}. Thus, the coming results will all contribute to the development of the field that studies the transferability of neural networks that we have reviewed in Sec. \ref{sec:literature_review}.

We will now move to presenting the results of our studies investigating the TL properties of convolutional neural networks trained for supervised learning problems.
