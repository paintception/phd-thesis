\chapter{Transfer Learning}
\label{ch:transfer_learnimg}

\begin{remark}{Outline}
	We now present Transfer Learning (TL), a machine learning methodology that aims at creating algorithms that are capable of retaining and reusing previously learned knowledge when getting trained on new, unseen problems. Most of the contributions presented within this dissertation are motivated by TL, therefore, we now introduce the reader to this specific learning paradigm with the goal of providing him/her with all the preliminary knowledge that is necessary for fully understanding the research that will be presented in the coming chapters. We start with a gentle introduction to TL in Sec. \ref{sec:tl_introduction} where we present the main concepts underlying TL and explain why it is desirable to have machine learning models that are transferable. We then show in Sec. \ref{sec:rationale} some practical, high level, examples that visually represent the benefits that come from adopting TL strategies in machine learning. We will then provide more rigorous, mathematical definitions of TL in Sec. \ref{sec:definitions} where we will characterize TL both for supervised learning as for reinforcement learning. In Sec. \ref{sec:literature_review} we will thoroughly review how TL has already been studied by the machine learning community, we will again do this both for supervised learning as for reinforcement learning, with an additional focus on how TL is performed when neural networks are used. We end this chapter with Sec. \ref{sec:relevance} where we summarize the most relevant TL concepts that underpin the contributions that are presented in this dissertation.  
\end{remark}

\section{Introduction}
\label{sec:tl_introduction}


\section{Rationale of Transfer Learning}
\label{sec:rationale}

Before mathematically defining TL we start by building some intuition and visually represent how one can observe the benefits of TL in practice. We assume that we would like to solve a certain task, and that we can choose between two different models: a model that has never dealt with any kind of task before, and a second model, which is identical to the first one with the major difference being that it has already seen a similar task in the past. Because of their different nature, we refer to the first model as a model that will be trained from scratch, and to the second model as a pre-trained model. Ideally, as mentioned in the previous section, we would like the latter model to perform better on the considered task that the first one. Yet, how can we asses if one model is better than the other one? As initially presented by \citet{langley2006transfer} and later generalized by \citet{lazaric2012transfer} we would like the performance of a pre-trained model to result into three possible improvements. If at least one of these improvements is observed while training, we can then consider the pre-trained model to be better than the scratch model. These improvements are the following:
\begin{itemize}
	\item \textcolor{RoyalBlue}{Learning Speed Improvements}: in this scenario the performance between a pre-trained model and a model trained from scratch is identical by the time training is finished, however when this kind of improvement appears we observe that the pre-trained model converges faster than the model trained from scratch. An example of this TL improvement is presented in the first plot of Fig. \ref{fig:tl_examples}. The goal is to train a model such that by the end of training its performance reaches a value of $200$. We can clearly see that both models manage to converge to this desired performance value, but that the pre-trained model manages to converge already after $\approx 50$ training iterations, whereas the model trained from scratch requires more than $200$ training iterations to perform similarly. Also note that the performance of both models at the beginning of training is identical, with both models starting with an initial performance of $\approx20$.

	\item \textcolor{RoyalBlue}{Jumpstart Improvements}: similarly to the previous case, also in this scenario there are no significant differences between the performance of a pre-trained model and the performance of a random model by the end of training. However this changes when we consider the very first training iteration. If jumpstart improvements appear, we can usually observe that when both models start their training process, the performance of the pre-trained model is much closer to the one that will be obtained by the end of training than the one of the scratch model. We visually report an example of this scenario in the third plot of Fig. \ref{fig:tl_examples}. In this case the goal is to train a model such that by the end of training its performance will be of $\approx -100$. We can clearly see that by the end of training both models are able to successfully achieve this goal, but that at the very first training iteration, the performance of the pre-trained model is significantly closer to the desired final performance ($\approx -250$) than the one of the scratch model ($\approx -450$). 
	\item \textcolor{RoyalBlue}{Asymptotic Improvements}: when this TL improvement appears, the final performance of a pre-trained model is significantly higher than the one of a model trained from scratch. It is worth noting that similarly to what happens when learning speed improvements are observed, also in this case the performance of both models is identical when training begins, and that this TL improvement only presents itself after several training iterations. A visualization of this TL improvement can be observed in the last plot of Fig. \ref{fig:tl_examples} where we can observe that for the first $\approx 20$ training iterations there are no differences in terms of performance between a pre-trained model and a model trained from scratch. However the more training iterations are performed, the more the pre-trained model starts outperforming the model trained from scratch, reaching a final performance that is almost twice as good by the $100\text{th}$ training iteration.
\end{itemize}

\input{./Results/Chapter03/tex/tl_examples.tex}

While all the improvements presented in Fig. \ref{fig:tl_examples} are all highly desirable, it is  worth noting that some of them can be more preferable than others. In fact, the potential benefits of TL highly depend from the problem at hand. As an example, let us consider a training situation where the main goal is that of minimizing the overall training time of a model. In this particular case, jumpstart and learning improvements are more desirable than asymptotic improvements, since the latter improvement might not result into a model that converges to a desired solution faster. On the other hand, if the main objective is that of training a model which performs as best as possible, than evidently, asymptotic improvements are preferred. It is also worth noting that the examples presented in Fig. \ref{fig:tl_examples} are not mutually exclusive, and that the benefits of TL can present themselves as a combination of improvements, rather than in the form of a single, isolated, improvement. 

Now that we have introduced the key ideas behind TL, and presented why adopting TL training strategies is beneficial we move to formally characterizing this machine learning paradigm both from a supervised learning perspective as from a reinforcement learning one.  


\section{Mathematical Definitions}
\label{sec:definitions}

As is common throughout this thesis we start by focusing on the supervised learning perspective.

\subsection{Supervised Learning}



\begin{definition}
	A domain $\mathcal{D}$ is the combination between an input space $\mathcal{X}$ and a marginal distribution $P(X)$, $\mathcal{D} = \{\mathcal{X},P(X)\}$ where $X$ denotes an instance set defined as $X=\{\vec{x}|\vec{x_i}\in \mathcal{X}, i =1, \cdots, n \}$.
\end{definition}


\begin{definition}
	A task $\mathcal{T}$ consists of a label space $\mathcal{Y}$ and a decision function $f$, i.e., $\mathcal{T}=\{\mathcal{Y},f\}$. The decision function $f$ is implicit and can only be learned by sampling data from $\mathcal{X}$ which comes in the form of $\{x_i,y_i\}$ pairs where $x_i\inX$ and $y_i \in \mathcal{Y}$.
\end{definition}


\begin{definition}
Given one, or more, observations corresponding to $m^s \in \mathds{N}^{+}$ source domain(s) and tasks(s) (i.e., $\{(\mathcal{D}_{S}_{i}, \mathcal{T}_{S}_{i}|i=1\cdots,m^s)\})$ and some additional observation(s) about $m^T \in \mathds{N}^{+}$ target domain(s) and task(s) (i.e. $\{(\mathcal{D}_{T}_{j},\mathcal{T}_{T}_{j}|j=1,\cdots,m^T)\})$, transfer learning utilizes the knowledge implied in the source domains to improve the performance of the learned decision functions $f^{T}_j(j=1,\cdots,m^T)$ on the target domain(s).
\end{definition}

\paragraph{Inductive Transfer Learning}
\begin{definition}
	Given a source domain $\mathcal{D}_S$ and a learning task $\mathcal{T}_S$, and a target domain $\mathcal{D}_T$ and a learning task $\mathcal{T}_T$, inductive transfer learning aims to help improving the target predictive function $f_T(\cdot)$ in $\mathcal{D}_T$ by using the knowledge in $\mathcal{D}_S$ and $\mathcal{T}_S$, where $\mathcal{T}_S \neq \mathcal{T}_T$. 
\end{definition}



\paragraph{Transductive Transfer Learning}

\begin{definition}
	Given a source domain $\mathcal{D}_S$ and a learning task $\mathcal{T}_S$, and a target domain $\mathcal{D}_T$ and a learning task $\mathcal{T}_T$, transductive transfer learning aims to help improving the target predictive function $f_T(\cdot)$ in $\mathcal{D}_T$ by using the knowledge in $\mathcal{D}_S$ and $\mathcal{T}_S$, where $\mathcal{D}_S \neq \mathcal{D}_T$ and $\mathcal{T}_S = \mathcal{T}_T$. 
\end{definition}



\paragraph{Unsupervised Transfer Learning}

\begin{definition}
	Given a source domain $\mathcal{D}_S$ and a learning task $\mathcal{T}_S$, and a target domain $\mathcal{D}_T$ and a learning task $\mathcal{T}_T$, unsupervised transfer learning aims to help improving the target predictive function $f_T(\cdot)$ in $\mathcal{D}_T$ by using the knowledge in $\mathcal{D}_S$ and $\mathcal{T}_S$, where $\mathcal{T}_S \neq \mathcal{T}_T$ and $\mathcal{Y}_S$ and $\mathcal{Y}_T$ are not observable. 
\end{definition}




\subsection{Reinforcement Learning}

\begin{definition}
Other definition

\end{definition}


\paragraph{Transfer of Value Functions}
\paragraph{Transfer of Features}
\paragraph{Transfer of Policies}

\section{Literature Review}
\label{sec:literature_review}

\subsection{Transfer Learning in Supervised Learning}
\subsection{Transfer Learning in Reinforcement Learning}
\subsection{Deep Transfer Learning}

\section{Relevance for this Dissertation}
\label{sec:relevance}
