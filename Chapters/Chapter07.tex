%Chapter 7

\chapter{The Deep Quality-Value Learning Family of Algorithms} % Chapter title
\label{ch:dqv_family_of_algorithms} % For referencing the chapter elsewhere, use \autoref{ch:introduction} 


\begin{remark}{Contributions and Outline}
	This chapter is based on the following two publications: \citet{sabatelli2018deepqv} and \citet{sabatelli2020deep}
	\vspace{5mm}
\end{remark}

\section{Introduction}               
In value-based Reinforcement Learning (RL) the aim is to construct algorithms which learn \textit{value functions} that are either able to estimate how good or bad it is for an agent to be in a particular state, or how good it is for an agent to perform a particular action in a given state. Such functions are respectively denoted as the state-value function $V(s)$, and the state-action value function $Q(s,a)$ \cite{sutton2018reinforcement}. In Deep Reinforcement Learning (DRL) the aim is to approximate these value functions with e.g. deep convolutional neural networks \cite{lecun2015deep}, which can serve as universal function approximators and powerful feature extractors. Classic model-free RL algorithms like Q-Learning \cite{watkins1992q}, Double Q-Learning \cite{hasselt2010double} and SARSA \cite{rummery1994line} have all led to the development of a ``deep" version of themselves in which the original RL update rules are expressed as objective functions that can be minimized by gradient descent \cite{mnih2015human, van2016deep, zhao2016deep}. Despite their successful applications \cite{li2017deep}, the aforementioned algorithms only aim at approximating the $Q$ function, while completely ignoring the $V$ function. This approach, however, is prone to issues that go back to standard RL literature. As shown by \citet{van2016deep} the DQN algorithm \cite{mnih2015human} is known to overestimate the values of the $Q$ function and requires an additional target network to not diverge (which role is not yet fully understood \cite{achiam2019towards}). These overestimations can partially be corrected by the DDQN \cite{van2016deep} algorithm, which, despite yielding stability improvements, does not always prevent its $Q$ networks from diverging \cite{van2018deep} and sometimes even underestimating the $Q$ function. Furthermore, DRL algorithms are also extremely slow to train. In what follows, we introduce a new family of DRL algorithms based on the key idea of simultaneously learning the $V$ function alongside the $Q$ function with two separate neural networks. Our main insight is that by jointly approximating the $V$ function and the $Q$ function, the task of learning one of these value functions can be sped up if the model that is responsible for learning it, can rely on what is being learned by the model responsible for learning the remaining value function. We show that this simple, yet effective idea yields faster, more robust and better model-free Deep Reinforcement Learning.

\section{Preliminaries}
We formally define the RL setting as a Markov Decision Process (MDP) where the main components are a finite set of states $\mathcal{S}$ $=\{s^{1}, s^{2},...,s^{n}\}$, a finite set of actions $\mathcal{A}$ and a time-counter variable $t$. In each state $s_{t}\in \mathcal{S}$, the RL agent can perform an action $a_{t} \in$ $\mathcal{A}(s_t)$ and transit to the next state as defined by a transition probability distribution $p(s_{t+1} | s_{t}, a_{t})$.
 When moving from $s_t$ to a successor state $s_{t+1}$ the agent receives a reward signal $r_t$ coming from the reward function $\Re (s_{t}, a_{t}, s_{t+1})$. The actions of the agent are selected based on its policy $\pi:\mathcal{S} \rightarrow \mathcal{A}$ that maps each state to a particular action. For every state $s \in \mathcal{S}$, under policy $\pi$ its \textit{value function} $V^{\pi}$ is defined as:
\begin{align}
    V^{\pi}(s)=\mathds{E}\bigg[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k}\bigg| s_t = s, \pi \bigg],
\end{align}
which denotes the expected cumulative discounted reward that the agent will get when starting in state $s$ and by following policy $\pi$ thereafter. Similarly, we can also define the \textit{state-action} value function $Q$ for denoting the value of taking action $a$ in state $s$ based on policy $\pi$ as:
\begin{align}
    Q^{\pi}(s,a)=\mathds{E}\bigg[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k} \bigg| s_t = s, a_t=a, \pi\bigg].
\end{align}
Both functions are computed with respect to the discount factor $\gamma \in [0,1]$ which controls the trade-off between immediate and long term rewards. The goal of an RL agent is to find a policy $\pi^{*}$ that realizes the optimal expected return:
\begin{align}
 V^{*}(s)=\underset{\pi}{\max}\:V^{\pi}(s), \ \text{for all} \ s\in\mathcal{S}
\end{align}
and the optimal $Q$ value function:
\begin{align}
Q^{*}(s,a)= \underset{\pi}{\max}\:Q^{\pi}(s,a) \ \text{for all} \ s\in\mathcal{S} \ \text{and} \ a \in\mathcal{A}.
\end{align}
It is well-known that optimal value functions satisfy the Bellman optimality equation as given by
\begin{align}
    V^{*}(s_t) = \underset{a}{\max}\sum_{s_{t+1}}p(s_{t+1} | s_{t}, a) \bigg[\Re (s_{t}, a, s_{t+1}) + \gamma V^{*}(s_{t+1}) \bigg]
\end{align}
for the state-value function, and by
\begin{multline}
    Q^{*}(s_t,a_t)=\sum_{s_{t+1}}p(s_{t+1} | s_{t}, a_{t})  \bigg[\Re (s_{t}, a_{t}, s_{t+1}) + \gamma \: \underset{a}{\max} \: Q^{*}(s_{t+1}, a) \bigg],
\end{multline}
for the state-action value function. Both functions can either be learned via Monte Carlo methods or by Temporal-Difference (TD) learning \cite{sutton1988learning}, with the latter approach being so far the most popular choice among model-free RL algorithms \cite{watkins1992q, rummery1994line, hasselt2010double}. %We now present into detail some of the methods that have obtained state of the art performance on one of the most popular DRL benchmarks: the Atari Arcade Learning (ALE) \cite{bellemare2013arcade} environment.

\section{Related Work}
While RL algorithms have been successfully combined with shallow neural networks for over two decades \cite{tesauro1994td}, the use of these algorithms with deeper architectures is more recent. 
The contribution which has established the potential of DRL can certainly be identified with the Deep-Q-Network (DQN), the first algorithm which uses a convolutional neural network for successfully learning an approximation of the $Q$ function from high dimensional inputs \cite{mnih2015human}. This approximation is learned by reshaping the popular Q-Learning algorithm introduced by \citet{watkins1992q} to an objective function which can be minimized by gradient descent. The original Q-Learning algorithm learns the state-action value function as follows
\begin{multline}
Q(s_{t}, a_{t}) := Q(s_{t}, a_{t}) + \alpha \big[r_{t} + \gamma \max_{a \in \cal A} Q(s_{t+1}, a) - Q(s_{t}, a_{t})\big]
\label{eq:q_learning}
\end{multline}
where $\alpha$ corresponds to the learning rate. The DQN algorithm adapts this update rule to a differentiable loss function which can be used for training a neural network that is parametrized by $\theta$. This objective function comes in the following form: 
\begin{multline}
L(\theta) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}) - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg].
\label{eq:dqn}
\end{multline}
Within this loss there are two components which ensure stable training. The first one is the Experience-Replay memory buffer ($D$), a buffer coming in the form of a queue which stores RL experiences $\langle s_{t},a_{t},r_{t},s_{t+1}\rangle$. When it comes to the popular Atari Arcade Learning (ALE) \cite{bellemare2013arcade} benchmark, the DQN algorithm uniformly samples mini-batches of 32 experiences for minimizing Eq. \ref{eq:dqn}, a procedure which starts as soon as at least 50.000 experiences are stored within the queue.
Furthermore, there is a second component which ensures stable training denoted as the target-network. DQN learns an approximation of the $Q$ function via TD-Learning, meaning that the approximated Q-function is regressed towards TD-targets which are computed by the approximated $Q$ function itself. The TD-target, defined as $y^{DQN}_{t}$, is expressed as follows:
\begin{equation}
    y^{DQN}_{t} = r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}), 
\label{eq:dqn_td}
\end{equation}{}
and is computed by the target network $\theta^{-}$ instead from the online Q-network $\theta$. 
The online network, and its target counterpart, have the exact same structure, with the main difference being that the parameters of the latter do not get optimized each time a mini-batch of experiences is sampled from the memory buffer. On the contrary its weights are temporally frozen and are only periodically updated with the $\theta$ weights (as defined by an appropriate hyperparameter). Given a training iteration $i$, differentiating the objective function of Eq. \ref{eq:dqn} with respect to $\theta$ gives the following gradient: 
\begin{multline}
\nabla_{\theta_{i}}y^{DQN}_{t}(\theta_{i}) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \\ \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}_{i-1})  - Q(s_{t}, a_{t}; \theta_{i})\big)\nabla_{\theta_{i}} Q(s_{t}, a_{t}; \theta_{i})\bigg].
\label{eq:dqn_gradient}
\end{multline}
Despite yielding super-human performance on most games coming from the ALE, DQN has shown to be suffering from the same issues which characterize the Q-Learning algorithm \cite{hasselt2010double}. Among these issues we mention the overestimation bias of the Q-function which has been well characterized in the tabular setting by \citet{hasselt2010double} and later in the deep learning context by \citet{van2016deep}.
In short, DQN is prone to learn overestimated Q-values because the same values are used both for selecting an action ($\underset{a\in \mathcal{A}}{\max}$) and for evaluating it ($Q(s_{t+1},a;\theta^{-})$). As originally presented by \citet{van2016deep} this becomes clearer when re-writing Eq. \ref{eq:dqn_td} as:
\begin{equation}
    y^{DQN}_{t} = r_{t} + \gamma \: Q(s_{t+1}, \underset{a\in \mathcal{A}}{\argmax}\: Q(s_{t+1}, a; \theta); \theta^{-}). 
\end{equation}{}

As a result, DQN tends to approximate the expected maximum value of a state, instead of its maximum expected value. To solve this problem the DDQN algorithm untangles the selection of an action from its evaluation by taking advantage of the target network $\theta^{-}$. DDQN's target is the same as DQN's with the main difference being that the selection of an action, given by the online Q-network $\theta$, and the evaluation of the resulting policy, given by $\theta^{-}$, can get unbiased by symmetrically updating the two sets of weights ($\theta$ and $\theta^{-}$). This can be achieved by regularly switching their roles during training.

Several extensions of DQN and DDQN have been proposed over the years, to make these algorithms learn faster and more data-efficient. We refer the reader to \cite{li2017deep} for a more in-depth review of these contributions. Within this paper, we are only interested in synchronous DRL algorithms which learn an approximation of a value function. This is achieved by following the same experimental setups that have been used for DQN and DDQN, and that will be reviewed in Sec. \ref{sec:global_evaluation} of this paper. Therefore, in this contribution, we will aim at comparing our novel DRL algorithms to DQN and DDQN only, while leaving their potential integration within more sophisticated DRL techniques as future work.

\section{A Novel Family of Deep Reinforcement Learning Algorithms}
\label{sec:dqv_family}

Just as much as DQN and DDQN are based on two tabular RL algorithms, so are the main contributions presented in this chapter. More specifically we extend two RL algorithms which were first introduced by \citet{wiering2005qv} and then extended by \citet{wiering2009qv} to the use of deep neural networks that serve as function approximators. Training these algorithms robustly is done by taking advantage of some of the techniques which have been reviewed in the previous section.

\subsection{DQV-Learning}
Our first contribution is the Deep Quality-Value (DQV) Learning algorithm, a novel DRL algorithm which aims at jointly approximating the $V$ function alongside the $Q$ function in an \textit{on-policy} learning setting. This algorithm is based on the QV($\lambda$) algorithm \cite{wiering2005qv}, a tabular RL algorithm which learns the $V$ function via the simplest form of TD-Learning \cite{sutton1988learning}, and uses the estimates that are learned by this value function to update the $Q$ function in a Q-Learning resembling way. Specifically, after a transition $\langle$ $s_{t}$, $a_{t}$, $r_{t}$, $s_{t+1}$ $\rangle$, QV$(\lambda)$ uses the TD$(\lambda)$ learning rule \cite{sutton1988learning} to update the $V$ function for all states: 
\begin{equation}
V(s):= V(s) + \alpha \big[ r_{t} + \gamma V(s_{t+1}) - V(s_t) \big] e_{t}(s),
\label{eq:qv_lambda_v_update}
\end{equation}
where $\alpha$ stands again for the learning rate and $\gamma$ is the discount factor, while $e_t(s)$ are the elibility traces that are necessary for keeping track if a particular state has occurred before a certain time-step or not. These are updated for all states as follows:  
\begin{equation} 
e_{t}(s) = \gamma \lambda e_{t-1}(s) + \eta_t(s),
\end{equation}
where $\eta_t(s)$ is an indicator function that returns a value of $1$ whether a particular state occurred at time $t$ and $0$ otherwise. Before updating the $V$ function, QV$(\lambda)$ updates the $Q$ function first, and does this via the following update rule:
\begin{equation}
Q(s_{t}, a_{t}):= Q(s_{t}, a_{t}) + \alpha \big[r_{t} + \gamma V(s_{t+1}) - Q(s_{t}, a_{t}) \big].
\label{eq:qv_lambda_q_update}
\end{equation}

We take inspiration from this specific learning dynamic and aim at learning an approximation of both the $V$ function, and the $Q$ function, with two neural networks that are respectively parametrized by $\Phi$ and $\theta$. To do so, we follow the same principles which have led to the development of the DQN algorithm and that reshape Eq. \ref{eq:q_learning} to Eq. \ref{eq:dqn}. Therefore, starting from Eq.\ref{eq:qv_lambda_v_update} and after removing $e_{t}(s)$ for simplicity, we get the following objective function which is used by DQV for learning the state-value function:
\begin{multline}
L(\Phi) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \Phi^{-}) - V(s_{t}; \Phi)\big)^{2}\bigg],
\label{eq:dqv_v_update}
\end{multline}
while the following loss is minimized for learning the $Q$ function when starting from Eq. \ref{eq:qv_lambda_q_update}:
\begin{multline}
    L(\theta) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \Phi^{-}) - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg],
\label{eq:dqv_q_update}
\end{multline}
where $D$ is again the Experience-Replay memory buffer, used for uniformly sampling batches of RL trajectories $\langle s_{t},a_{t},r_{t},s_{t+1}\rangle$, and $\Phi^{-}$ is the target-network used for the construction of the TD-errors. Please note that the role of this target network is different from its role within the DQN algorithm. In DQV this network corresponds to a copy of the network which approximates the state-value function and not the state-action value function. It is also worth noting that both networks learn from the same TD-target which comes in the following form:
\begin{equation}
y_{t}^{DQV} = r_{t} + \gamma V(s_{t+1}; \Phi^{-}). 
\end{equation}
The gradient with respect to both loss functions can be easily expressed similarly as done in Eq. \ref{eq:dqn_gradient} for the DQN algorithm.

\subsection{DQV-Learning with Multilayer Perceptrons}
We start by exploring whether this learning dynamic of jointly approximating two value functions simultaneously, and let the $Q$ function bootstrap from the TD-targets that are learned from the $V$ network can yield successful results on a set of preliminary experiments. To do so, we use two classic control problems that are well known in the RL literature: \texttt{Acrobot} \cite{sutton1996generalization} and \texttt{Cartpole} \cite{barto1983neuronlike} with both environments being provided by the Open-AI Gym package \cite{brockman2016openai}. We approximate the $V$ function and the $Q$ function with a two hidden layer Multilayer Perceptron (MLP) that is activated by a ReLU non linearity ($f(x) = max (0,x)$) and compare the performance of DQV to the one of the DQN and the DDQN algorithms, which use the same MLP but for approximating the $Q$ function only. Given the simplicity of these two control problems we did not integrate DQV with the target network $\Phi^{-}$ yet. Our preliminary results reported in Fig. \ref{fig:mlp_dqv_results}, show the benefits that can come from training two separate networks with the update rules reported in Eq.\ref{eq:dqv_v_update} and Eq.\ref{eq:dqv_q_update}. We can in fact observe that on both control problems DQV-Learning outperforms DQN and DDQN, by converging significantly faster. 

\input{./Results/Chapter07/tex/dqv_mlp_results.tex}


\subsection{DQV-Max Learning}
Based on the successful results presented in Fig. \ref{fig:mlp_dqv_results} that highlight the potential benefits that could come from jointly approximating two value functions over one, we now introduce the Deep Quality-Value-Max (DQV-Max) algorithm, a novel DRL algorithm which builds on top of some of the ideas that characterize DQV. Similarly as done for DQV, we still aim at jointly learning an approximation of the $V$ function and the $Q$ function, but in this case, the goal is to do this with an \textit{off-policy} learning scheme. To construct this algorithm we take inspiration from the QV-Max RL algorithm introduced by \citet{wiering2009qv}. The key component of QV-Max is the use of the $\underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a)$ operator, which makes RL algorithms learn \textit{off-policy}. We use this operator when approximating the $V$ function and for computing TD-errors which correspond to the ones that are also used by the DQN algorithm. However, within DQV-Max, these TD-errors are used by the state-value network and not by the state-action value network. This results in the following loss which is used for learning the $V$ function:
\begin{multline}
L(\Phi) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}) - V(s_{t}; \Phi)\big)^{2}\bigg].
\label{eq:dqv_max_v}
\end{multline}
In this case the target network $\theta^{-}$ corresponds to the same target network that is used by DQN. The TD-error $r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-})$ is however only used for learning the $V$ function. When it comes to the $Q$ function we use the same update rule that is presented in Eq. \ref{eq:dqv_q_update} with the only difference being that in this case no $\Phi^{-}$ target network is used. Despite requiring the computation of two different targets for learning, we noticed that DQV-Max did not benefit from using two distinct target networks, therefore its loss function for approximating the $Q$ function is simply:
\begin{multline}
    L(\theta) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \Phi) - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg].
    \label{eq:dqv_max_q}
\end{multline}

The pseudocode of both DQV and DQV-Max is presented at the end of this paper in Algorithm \ref{alg: dqv_algorithms}. The pseudocode is an adaptation of a standard DRL training loop which corresponds to what is usually presented within the literature \cite{mnih2015human}. We just make explicit use of the hyperparameters \texttt{total\_a} and \texttt{c} which ensure that enough actions have been performed by the agent before updating the weights of the target network. We also ensure via the hyperparameter \texttt{total\_e}, that enough episodes are stored within the memory buffer (which has capacity $\mathcal{N}$) before starting to optimize the neural networks. 

\section{Results}
\label{sec:results}

\subsection{Global Evaluation}
\label{sec:global_evaluation}

We evaluate the performance of DQV and DQV-Max on a subset of 15 games coming from the popular \texttt{Atari-2600} benchmark \cite{bellemare2013arcade}. Our newly introduced algorithms are compared against DQN and DDQN. To keep all the comparisons as fair as possible we follow the same experimental setup and evaluation protocol which was used in \cite{mnih2015human} and \cite{van2016deep}. The only difference between DQV and DQV-Max, and DQN and DDQN is the exploration schedule which is used. Differently from the latter two algorithms, which use an epsilon-greedy strategy which has an $\epsilon$ starting value of 1.0, DQV and DQV-Max's exploration policy starts with an initial $\epsilon$ value of 0.5. All other hyperparameters, ranging from the size of the Experience-Replay memory buffer to the architectures of the neural networks, are kept the same among all algorithms. We refer the reader to the original DQN paper \cite{mnih2015human} for an in-depth overview of all these hyperparameters. The performance of the algorithms is tested based on the popular \texttt{no-op action} evaluation regime. At the end of the training, the learned policies are tested over a series of episodes for a total amount of 5 minutes of emulator time. All testing episodes start by executing a set of partially random actions to test the level of generalization of the learned policies. We present our results in Table \ref{tab:ch07_results.tex} where the best performing algorithm is reported in a green cell while the second-best performing algorithm is reported in a yellow cell. As is common within the DRL literature, the table also reports the scores which would be obtained by an expert human player and by a random policy. When the scores over games are equivalent, we report in the green and yellow cells the fastest and second fastest algorithm with respect to its convergence time. We can start by observing that DQV and DQV-Max successfully master all the environments on which they have been tested, with the only exception being the \texttt{Montezuma's Revenge} game. It is well-known that this game requires more sophisticated exploration strategies than the epsilon-greedy one \cite{fortunato2017noisy}, and was also not mastered by DQN and DDQN when these algorithms were introduced. We can also observe that there is no algorithm which performs best on all the tested environments even though, as highlighted by the green and yellow cells, the algorithms of the DQV-family seem to generally perform better than DQN and DDQN, with DQV-Max being the overall best performing algorithm in our set of experiments. When either DQV or DQV-Max are not the best performing algorithm (see for example the \texttt{Boxing} and \texttt{CrazyClimber} environments), we can still observe that our algorithms managed to converge to a policy which is not significantly worst than the one learned by DQN and DDQN.
There is however one exception being the \texttt{RoadRunner} environment. In fact, in this game, DDQN significantly outperforms DQV and DQV-Max. It is also worth noting the results on the \texttt{BankHeist} and \texttt{Enduro} environments. Both DQN and DDQN failed to achieve super-human performance on these games, while DQV and DQV-Max successfully managed to obtain a significantly higher score than the one obtained by a professional human player. On the \texttt{BankHeist} environment DQV and DQV-Max obtain $\approx 400$ points more than an expert human player, while on the \texttt{Enduro} environment their performance is almost three times better than the one obtained by DQN and DDQN.

\input{Tables/Chapter07/dqv_dqv_max_results.tex}


\subsection{Convergence Time}
\label{sec:convergence_time}

While DRL algorithms have certainly obtained impressive results on the \texttt{Atari-2600} benchmark, it is also true that the amount of training time which is required by these algorithms can be very long. Over the years, several techniques, ranging from Prioritized Experience Replay (PER) \cite{wang2016dueling} to the Rainbow extensions introduced by \citet{hessel2018rainbow}, have been proposed to reduce the training time of DRL algorithms. It is therefore natural to investigate whether jointly approximating the $V$ function alongside the $Q$ function can lead to significant benefits in this behalf. Unlike the $Q$ function, the state-value function is not dependent on the set of possible actions that the agent may take, and therefore requires fewer parameters to converge. Since DQV and DQV-Max use the estimates of the $V$ network to train the $Q$ function, it is possible that the $Q$ function could directly benefit from these estimates and as a result converge faster than when regressed towards itself (as happens in DQN).

We use two self-implemented versions of DQN and DDQN for comparing the convergence time that is required during training by all the tested algorithms on three increasingly complex \texttt{Atari} games: \texttt{Boxing, Pong} and \texttt{Enduro}. Our results, reported in Fig. \ref{fig:ch07_convergence_results}, show that DQV and DQV-Max converge significantly faster than DQN and DDQN, and highlight the benefits of jointly approximating two value functions instead of one when it comes to the overall convergence time that is required by the algorithms. Even though, as presented in Table \ref{tab:ch07_results}, DQV and DQV-Max do not always significantly outperform DQN and DDQN in terms of the final cumulative reward which is obtained, it is worth noting that these algorithms require significantly less training episodes to converge on all tested games. This benefit makes our two novel algorithms faster alternatives within model-free DRL.

\input{./Results/Chapter07/tex/dqv_family_cnn_convergence_results.tex}

\subsection{Quality of the Learned Value Functions}
\label{sec:quality_of_value_functions}

It is well-known that the combination of RL algorithms with function approximators can yield DRL algorithms that diverge. The popular Q-Learning algorithm is known to result in unstable learning both if linear \cite{tsitsiklis1997analysis} and non-linear functions are used when approximating the $Q$ function \cite{van2018deep}. This divergence according to \citet{sutton2018reinforcement} is caused by the interplay of three elements that are known as the \textit{`Deadly Triad'} of DRL. The elements of this triad are:
\begin{itemize}
    \item \textit{a function approximator}: which is used for learning an approximation of a value function that could not be learned in the tabular RL setting due to a too large state-action space.
    \item \textit{bootstrapping}: when the algorithms use a future estimated value for learning the same kind of estimate.
    \item \textit{off-policy learning}: when a future estimated value is different from the one which would be computed by the policy the agent is following.
\end{itemize}{}

\citet{van2018deep} have shown that the \textit{`Deadly Triad'} is responsible for enhancing one of the most popular biases that characterize the Q-Learning algorithm: the overestimation bias of the $Q$ function \cite{hasselt2010double}. It is therefore natural to study how DQV and DQV-Max relate to the \textit{`Deadly Triad'} of DRL, and to investigate up to what extent these algorithms suffer from the overestimation bias of the $Q$ function. To do this we monitor the estimates that are given by the network that is responsible for approximating the $Q$ function. More specifically, at training time, we compute the averaged $\underset{a \in \cal A}{\max}\:Q(s_{t+1}, a)$ over a set ($n$) of full evaluation episodes as defined by 
\begin{equation}
\frac{1}{n}\sum_{t=1}^{n}\underset{a \in \cal A}{\max}\:Q(s_{t+1}, a;\theta).
\end{equation}
As suggested by \citet{van2016deep} these estimates can then be compared to the averaged discounted return of all visited states that comes from an agent that has already concluded training. By analyzing whether the $Q$ values which are estimated while training differ from the ones which should be predicted by the end of it, it is possible to quantitatively characterize the level of divergence of DRL algorithms. We report our results in Figs. \ref{fig:overestimation_bias_results_dqn}, \ref{fig:overestimation_bias_results_ddqn}, \ref{fig:overestimation_bias_results_dqv} and \ref{fig:overestimation_bias_results_dqv_max} where the full lines correspond to the value estimates that come from each algorithm at training time, while the dashed lines correspond to the actual averaged discounted return that is given by an already trained agent.


We can start by observing that the values denoting the averaged discounted return obtained by each algorithm differ among agents. This is especially the case when it comes to the \texttt{Enduro} environment, and is a result which is in line with what has been presented in Table \ref{tab:results}: DQV and DQV-Max lead to better final policies than DQN and DDQN. Furthermore, when we compare these baseline values to the value estimates that are obtained during training, we can observe that the ones obtained by the DQN algorithm significantly diverge from the ones which should be predicted by the end of training. This behavior is known to be caused by the overestimation bias of the $Q$ function which can be corrected by the DDQN algorithm. By analyzing the value estimates of DQV and DQV-Max we can observe that both algorithms produce value estimates which are more similar to the ones computed by DDQN than to the ones given by DQN. This is especially the case for DQV, in fact its value estimates nicely correspond to the averaged discounted return baseline, both on the \texttt{Pong} environment and on the \texttt{Enduro} environment. The estimates coming from DQV-Max, however, seem to diverge more when compared to DQV and DDQN's ones. This is clearer on the \texttt{Enduro} environment, where the algorithm does show some divergence. However, we can also observe that this divergence is less strong when compared to DQN's one. The value estimates of the latter algorithm keep growing over time, while DQV-Max's ones get bounded while training progresses. This results in smaller estimated $Q$ values. We believe that there are mainly two reasons why our algorithms suffer less from the overestimation bias of the $Q$ function. When it comes to DQV, we believe that this algorithm suffers less from this bias since it is an \textit{on-policy} learning algorithm. Such algorithms are trained on exploration actions with lower $Q$ values. Because of its \textit{on-policy} learning scheme, DQV also does not present one element of the \textit{`Deadly Triad'}, which might help reducing divergence. When it comes to DQV-Max, we believe that the reason why this algorithm does not diverge as much as DQN can be found in the way it approximates the $Q$ function. One key component of the \textit{`Deadly Triad'}, is that divergence occurs if the $Q$ function is learned by regressing towards itself. As given by Eq. \ref{eq:dqv_max_q} we can see that this does not hold for DQV-Max, since the $Q$ function bootstraps with respect to estimates that come from the $V$ network. We believe that this specific learning dynamic, which also holds for the DQV algorithm, makes our algorithms less prone to estimate large $Q$ values.

% add overestimation bias results
\input{./Results/Chapter07/tex/dqn_overestimation.tex}
\input{./Results/Chapter07/tex/ddqn_overestimation.tex}
\input{./Results/Chapter07/tex/dqv_overestimation.tex}
\input{./Results/Chapter07/tex/dqv_max_overestimation.tex}


\section{Additional Studies}
As introduced in Sec. \ref{sec:dqv_family} DQV and DQV-Max use two separate neural networks for approximating the $Q$ function and the $V$ function. To verify whether two different architectures are needed for making both algorithms perform well, we have experimented with a series of variants of the DQV-Learning algorithm. The aim of these experiments is to investigate whether the performance of DQV gets harmed when reducing its capacity.
%that of reducing the number of trainable parameters that are required by the original version of DQV, and investigate whether its performance could get harmed when reducing the capacity of the algorithm.
The studied DQV's extensions are the following:

\begin{enumerate}
    \item \textit{Hard-DQV}: a version of DQV which uses one single common neural network for approximating both the $Q$ and the $V$ functions. An additional output node, needed for estimating the value of a state, is added next to the output nodes which estimate the different $Q$ values. The parameters of this algorithm are therefore `hardly-shared' among the agent, and provide the benefit of halving the total amount of trainable parameters of DQV. The different outputs of the network get then alternatively optimized according to Eq. \ref{eq:dqv_v_update} and \ref{eq:dqv_q_update}.

    \item \textit{Dueling-DQV}: a slightly more complicated version of Hard-DQV which adds one specific hidden layer before the output nodes that estimate the $Q$ and $V$ functions. In this case, the outputs of the neural network which learn one of the two value functions, partly benefit from some specific weights that are not shared within the neural network. This approach is similar to the one used by the `Dueling-Architecture' presented by \citet{wang2016dueling}, therefore the name Dueling-DQV. While it is well established that three convolutional layers are needed \cite{mnih2015human, van2016deep} for learning the $Q$ function, the same might not be true when it comes to learning the $V$ function. We thus report experiments with three different versions of Dueling-DQV: Dueling-1st, Dueling-2nd, and Dueling-3rd. The difference between these methods is simply the location of the hidden layer which precedes the output that learns the $V$ function. It can be positioned after the first convolutional layer, the second or the third one. Training this architecture is done as for Hard-DQV.

    \item \textit{Tiny-DQV}: the neural architectures used by DQV and DQV-Max that approximate the $V$ function and the $Q$ function follow the one which was initially introduced by the DQN algorithm \cite{mnih2015human}. This corresponds to a three-hidden layer convolutional neural network which is followed by a fully connected layer of 512 hidden units. The first convolutional layer has 32 channels while the last two layers have 64 channels. In Tiny-DQV we reduce the number of trainable parameters of DQV by reducing the number of channels at each convolution operation. Tiny-DQV only uses 8 channels after the first convolutional layer and 16 at the second and third convolutional layers. Furthermore, the size of the final fully connected layer is reduced to only 128 hidden units. The choice of this architecture is motivated by the work presented in \cite{van2018deep} which studies the role of the capacity of the DDQN algorithm. Unlike the Hard-DQV and Dueling-DQV extensions, the parameters of Tiny-DQV are not shared at all among the networks that are responsible for approximating the $V$ function and the $Q$ function.
\end{enumerate}

The results obtained by these alternative versions of DQV are presented in Figs. \ref{fig:hard_dqv_results}, \ref{fig:duelling_dqv_results} and \ref{fig:tiny_dqv_results} where we report the learning curves obtained by the tested algorithms on six different \texttt{Atari} games. Each DQV extension is directly compared to the original DQV algorithm. We can observe that all the extensions of DQV, which aim at reducing the number of trainable parameters of the algorithm, fail in performing as well as the original DQV algorithm. Starting from Fig.\ref{fig:hard_dqv_results} we can observe that \textit{Hard-DQV} does not only yield significantly lower rewards (see the results obtained on \texttt{Boxing}) but also presents more unstable training (as highlighted by the results obtained on the \texttt{Pong} environment). Lower rewards and unstable training also characterize the \textit{Tiny-DQV} algorithm (see results on \texttt{BankHeist} and \texttt{CrazyClimber} reported in Fig.\ref{fig:tiny_dqv_results}). Overall the most promising extensions of DQV are its \textit{Dueling} counterparts, we have observed in particular that the best performing architecture over most of our experiments was the \textit{Dueling-DQV-3rd} one. As can be seen by the results reported in Fig. \ref{fig:duelling_dqv_results} on the \texttt{Pong} environment we can observe that \textit{Dueling-DQV-3rd} has a comparable performance to DQV, even though it converges slower. Unfortunately, \textit{Dueling-DQV-3rd} still shows some limitations, in particular when tested on more complicated environments such as \texttt{Enduro}, we can observe that it under-performs DQV with $\approx$ 200 points. It is also worth mentioning that the idea of approximating the $V$ function before the $Q$ function explored by \textit{Dueling-DQV-1st} and \textit{Dueling-DQV-2nd} yielded negative results.

\input{./Results/Chapter07/tex/hard_dqv_results.tex}
\input{./Results/Chapter07/tex/duelling_dqv_results.tex}
\input{./Results/Chapter07/tex/tiny_dqv_results.tex}


\section{Discussion and Conclusion}
We have presented two novel model-free DRL algorithms which in addition to learning an approximation of the $Q$ function also aim at learning an approximation of the $V$ function. We have compared DQV and DQV-Max Learning to DRL algorithms which only learn an approximation of the $Q$ function, and showed the benefits which come from jointly approximating two value functions over one. Our newly introduced algorithms learn significantly faster than DQN and DDQN and show that approximating both the $V$ function and the $Q$ function can yield significant benefits both in an \textit{on-policy} learning setting as in an \textit{off-policy} learning one. This specific training dynamic allows for a better learned $Q$ function which makes DQV and DQV-Max less prone to estimate unrealistically large $Q$ values. All these benefits come however at a price: to successfully learn two value functions, two separate neural networks with enough capacity are required. We identify several directions for further research that focus on the following points: an integration of the algorithms of the DQV-family with all the extensions which have improved the DQN algorithm over the years; an integration of DQV and DQV-Max within an Actor-Critic framework which will allow us to tackle continuous-control problems, and lastly, a study of how the algorithms of the DQV-family will perform in a Batch-DRL setting. It has been shown that DRL algorithms fail when learning from a fixed data set of trajectories instead of dynamically interacting with the environment \cite{fujimoto2019benchmarking}. This is due to a phenomenon known as extrapolation error. We will investigate to what extent jointly learning two value functions instead of one will cope with this additional DRL bias.
