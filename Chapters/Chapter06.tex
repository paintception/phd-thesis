\chapter{Reinforcement Learning and Deep Neural Networks}
\label{ch:reinforcement_learning}


\begin{remark}{Outline}

\end{remark}

\section{Introduction}
\label{sec:rl_introduction}


\section{Markov Decision Processes}
\label{sec:mdps}

In order to develop and apply RL algorithms to optimal decision making problems, we need to formulate the problem in a specific framework: in RL this is done with Markov Decision Processes (MDPs) \cite{puterman1990markov,puterman2014markov}. Throughout this thesis we will characterize MDPs, and the resulting RL concepts, by using the mathematical notation that was used by \citet{sutton2018reinforcement} in their seminal book about RL, although it is worth noting that within the literature, different formulations can be found to express the same kind of concepts \cite{bertsekas1995neuro,busoniu2010reinforcement,bertsekas2000dynamic,bertsekas2019reinforcement}.

We start by introducing the following elements:
\begin{itemize}
	\item A set of possible states $\mathcal{S}$, that can be visited by an agent while it is interacting with the MDP, where $s_t \in \mathcal{S}$ denotes the state being visited at time-step $t$.
	\item A set of possible actions $\mathcal{A}$ that are available to the agent when it is in a certain state, where $a_t \in \mathcal{A}(s_t)$ denotes the action that is performed by the agent in state $s$ at time-step $t$.
\item A transition function $\mathcal{P}:\matchal{S}\times\matchal{A}\times\matchal{S}\Rightarrow [0,1]$ that defines the probability for an agent to visit state $s_{t+1}$, based on its current state and the action which will be performed thereafter.
\item A reward function $\Re:\matchal{S}\times\matchal{A}\times\matchal{S}\Rightarrow \mathbb{R}$ which returns a reward signal $r_{t+1}$ when an agent performs action $a_t$ in state $s_t$ and transits to $s_{t+1}$.
\item A discount factor denoted as $\gamma \in [0,1]$.

\end{itemize}

Based on these concepts a MDP is then defined by the following tuple $<\mathcal{S}, \matchcal{A}, \matchal{P}, r, \gamma>$ and is also commonly denoted in the RL literature as the \textcolor{RoyalBlue}{environment}. The way the agent interacts with this environment is given by its \textcolor{RoyalBlue}{policy} $\pi$, defined as a probability distribution over $a \in \matchal{A}(s)$ for each $s \in \mathcal{S}$:
\begin{equation}
	\pi(a|s) = \text{Pr}\; \{A_t = a | S_t = s\}, \; \text{for all}\; s \in \mathcal{S}\; \text{and}\; a\ \in \mathcal{A}. 
\end{equation}
A policy can be deterministic if $\forall s:\pi(s,a) = 1$ for exactly one $a \in \mathcal{A}(s)$ and $\pi(s,b)=0$ for all other $b \in \mathcal{A}(s)$, while it is stationary if it does not change over time. In both cases the goal of a policy is to map each state to an action and is defined as: $\pi:\mathcal{S}\Rightarrow\mathcal{A}$.

The elements of a MDP allow us to properly model the dynamics of an agent interacting with its environment, an interaction which can be summarized as follows: at each time-step $t$ the environment provides the agent with a certain state $s_t$, the agent then performs action $a_t$ which results into the reward signal $r_{t+1}$. After performing such action the agent will enter into a new state $s_{t+1}$. This continuous interaction with the environment is also known as the Reinforcement Learning loop, and can technically be infinite. This is however never the case in practice, since an agent will eventually visit a state which only transits to itself (denoted as terminal), which will therefore stop the agent-environment interaction. We visually represent the Reinforcement Learning loop in Fig. \ref{fig:rl_loop}.

\begin{figure}[ht!]
\centering
  \includegraphics[width=10cm]{./Images/Chapter06/rl_loop.png}
  \caption{A visual representation of how an agent interacts with an environment as modeled by a Markov decision process. Figure taken from page 48 of the \citet{sutton2018reinforcement} textbook.}
  \label{fig:rl_loop}
\end{figure}

Each interaction of the agent with the environment is defined as an \textcolor{RoyalBlue}{episode}, which consists of one, or several trajectories $\tau$, that come in the form of the following sequence:
\begin{align}
	\langle(s_t,a_t,r_t,s_{t+1})\rangle,t=0,\ldots,T-1
\end{align}
where $T$ is a random variable representing the length of the episode.

A key property of the environment is that it fulfills the Markov property which is defined as follows:
\begin{definition}
	A discrete stochastic process is Markovian if the conditional distribution of the next state of the process only depends from the current state of the process.
\end{definition}
This implies that the only information that is necessary for predicting to which state an agent will step next are $s_t$ and $a_t$, which can be expressed formally as:
\begin{align}
	p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = p(s_{t+1} | s_t, a_t).
\end{align}
Interestingly, a similar conclusion also holds for the reward that the agent will get, meaning that the reward that is obtained by an agent is only determined by its previous action, and not by the history of all previously taken actions, as defined by:
\begin{align}
	(r_t| s_t, a_t, \ldots, s_1, a_1) = p(r_t|s_t,a_t).
\end{align}


\section{Goals and Returns}
So far we have defined all the elements that model the interaction of an agent with an environment, whilst introducing some key properties that are key for the development of RL algorithms. While it is true that we have defined how the agent-environment interaction works, we yet do not know what the purpose of this interaction is. In RL the goal of an agent is defined with respect to the reward signal $r_t$ that is returned by the reward function $\Re$. Informally, the goal of an agent is that of maximizing the total amount of reward it receives while interacting with the environment. In the simplest case we can define this as:
\begin{align}
	G_t = r_t, r_{t+1}, r_{t+2}, \ldots, r_{T}.
\label{eq:goal}
\end{align}
While simple and intuitive this formulation has one major drawback: it treats each reward signal equally since it does not distinguish rewards that are obtained in the near future, i.e. $r_t$, from the ones that will be obtained in the more distant future, i.e. $r_{T-1}$. To deal with such issue we need an additional concept known as \textcolor{RoyalBlue}{discounting}, which can be governed by the discount rate parameter $\gamma$, also known as the discount factor. $\gamma$ allows us to weight the different reward signals based on how close or distant in the future these rewards are received by the agent. By introducing $\gamma$ in Eq. \ref{eq:goal} we can now define the expected discounted return as:
\begin{align}
	G_t & = r_t+\gamma r_{t+1}, \gamma^{2} r_{t+2} + ... \\
	    & = \sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1}.
\label{eq:discounted_return}
\end{align}
The role of $\gamma$ can be interpreted as follows: a reward obtained $k$ time steps in the future is only worth $\gamma^{k-1}$ times what it would be worth if received immediately. It is easy to see how different $\gamma$ values can result into different agent's behaviors. If $\gamma=0$ an agent will only take into account immediate rewards, therefore aiming to maximize $r_{t+1}$ only and resulting into having a ``myopic" behavior. If $\gamma$ approaches $1$ the agent will become more ``far-sighted", it will take future rewards into account more strongly and will therefore increase its chances of accessing future rewards that will result into a higher cumulative return. 
Please note that by defining $\gamma \leq 1$ we can make the infinite sum presented in Eq. \ref{eq:discounted_return} finite as long as the sequence of rewards $r_k$ is bounded.   

While the role of $\gamma$ is often taken for granted within the RL literature it is worth noting that as mentioned by \citet{van2011insights} and \citet{schmidhuber2019reinforcement}, $\gamma$ is an artificial concept which is not present in fields such as traditional control theory or engineering. The reason of this is that $\gamma$ corresponds to a concept that does not exists in the real world, and that in practice distorts the real value of $r_t$ in an exponentially shrinking fashion. Even if it is considered common practice to include a discount factor in the development of RL algorithms, it is worth noting that making $\gamma$ part of the RL framework corresponds to including a form of ``inductive bias" within the resulting algorithms. It is common knowledge that low discount factors result into poor performance, and that it is therefore as beneficial as possible to set $\gamma$ as close to $1$, yet choosing an appropriate $\gamma$ parameter can be more challenging than expected especially when RL algorithms are combined with function approximators. \citet{wiering2009qv} show that different algorithms prefer different discount factors, while \citet{franccois2015discount} show the relationship between low discount factors and the values of the learning rate. Finally \citet{van2019using} introduce a method that allows the use of low discount factors for approximate RL algorithms, while at the same time highlighting that the common perception of the role of $\gamma$ might need revision from the RL community.             

\section{Value Functions}
We are now ready to introduce the arguably most important concept that underlies many RL algorithms: the concept of \textcolor{RoyalBlue}{value}. We can define the value of a state $s$, as well as the value of a certain policy $\pi$ or of a certain action $a$, anyhow, independently from what we are considering the notion of value is always directly linked to the concept of expected discounted return defined in Eq. \ref{eq:discounted_return}. Given an MDP and a policy $\pi$ we can determine the value of a state $s$ as a function that measures the expected return that the agent will receive when starting in $s$ and following $\pi$ thereafter. 
\begin{align}
    V^{\pi}(s)=\mathds{E}\bigg[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k}\bigg| s_t = s, \pi \bigg].
    \label{eq:state_value_function}
\end{align}
$V^{\pi}(s)$ is also known as the \textit{state-value function} and intuitively tells us how good or how bad it is for an agent to be in a certain state. While this function is only conditioned on the state that is being visited by the agent, we can also condition it on the actions that are taken by the agent, which will quantify how good or bad it is for the agent to take a certain action $a$ in a certain state. This function comes with the name of \textit{state-action value function} and is defined as follows:
\begin{align}
     Q^{\pi}(s,a)=\mathds{E}\bigg[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k} \bigg| s_t = s, a_t=a, \pi\bigg].
 \end{align}
Both value functions are very powerful since they allow us to characterize the behavior of an agent by quantitatively assessing its interaction with the environment. They can be seen as the knowledge of the agent and represent its desirability of being in a specific state. As we will see in the coming sections, accurately modeling these value functions is one of the major goals of RL.  

A key property of $V^{\pi}(s)$ and $Q^{\pi}(s,a)$ is that both value functions satisfy a consistency condition that allows us to define both functions recursively. For example let us consider the state-value function $V^{\pi}(s)$ presented in Eq. \ref{eq:state_value_function}, we can rewrite it as:
\begin{align}
 V^{\pi}(s) & =\mathds{E}\big[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k}\big| s_t = s, \pi \big] \\ 
 & =\mathds{E}\big[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\ldots \big| s_t =s , \pi \big] \\ 
 & =\mathds{E}\big[r_{t+1}+\gamma(r_{t+2}+\gamma r_{t+3}+\ldots)\big| s_t =s , \pi \big] \\
 & =\mathds{E}\big[r_{t+1}+\gamma V^{\pi}(s_{t+1}) \big| s_t =s , \pi \big] \\
 & =\sum_a \pi(s,a) \sum_{s+1} p(s_{t+1}|s,a)\big[\Re(s_t, a, s_{t+1}) + \gamma V^{\pi}(s_{t+1}) \big].
\end{align}
Similar steps can be followed when considering $Q^{\pi}(s,a)$ which can then be recursively defined as:
\begin{equation}
	Q^{\pi}(s,a) = \sum_{s_{t+1}} p(s_{t+1}|s,a)\big(\Re(s_t, a, s_{t+1}) + \\ \gamma \sum_{a_{t+1}} \pi(s_{t+1},a_{t+1}) Q^{\pi}(s_{t+1}, a_{t+1}) \big).
\end{equation}

When it comes to sequential decision making, we are interested in maximizing the value of each state or of each state-action pair, since by doing so we will be finding a policy $\pi$ that is optimal. The \textcolor{RoyalBlue}{optimal policy} $\pi^{*}$ is a policy that realizes the optimal expected return defined as:
\begin{align}
 V^{*}(s)=\underset{\pi}{\max}\:V^{\pi}(s), \ \text{for all} \ s\in\mathcal{S}
\end{align}
and the optimal $Q$ value function:
\begin{align}
Q^{*}(s,a)= \underset{\pi}{\max}\:Q^{\pi}(s,a) \ \text{for all} \ s\in\mathcal{S} \ \text{and} \ a \in\mathcal{    A}.
\end{align}
When we recursively define both optimal value functions as we did for Eq. \ref{eq:state_value_function} we obtain:
\begin{align}
    V^{*}(s_t) = \underset{a}{\max}\sum_{s_{t+1}}p(s_{t+1} | s_{t}, a) \bigg[\Re (s_{t}, a, s_{t+1}) + \gamma V^{*}(s_{t+1}) \bigg]
    \label{eq:optimal_v}
\end{align}
and
\begin{multline}
    Q^{*}(s_t,a_t)=\sum_{s_{t+1}}p(s_{t+1} | s_{t}, a_{t})  \bigg[\Re (s_{t}, a_{t}, s_{t+1}) + \gamma \: \underset{a}{\max} \: Q^{*}(s_{t+1}, a) \bigg],
    \label{eq:optimal_q}
\end{multline}
which are well known to correspond to the Bellman \textcolor{RoyalBlue}{optimality} equations \cite{bellman1966dynamic}. 

If the optimal $Q$ function is learned it becomes as straightforward task to derive an optimal policy since one only needs to select the action which has the highest value in each state as defined by:
\begin{equation}
	\pi^{*}(s) = \underset{a\in\mathcal{A}}{\argmax} \ Q^{*}(s,a) \ \text{for all} \ s \in \mathcal{S}.
\end{equation}

It is also worth noting that the $Q$ function and the $V$ function satisfy the following equality
\begin{align}
	V^{*}(s) = \underset{a\in\mathcal{A}}{\max} \ Q^{*}(s,a) \ \text{for all} \ s \in \mathcal{S}
\end{align}
which comes from the fact $V^{*}(s) \leq \max_{a\in\mathcal{A}} \ \text{for all} \ s\in\mathcal{S}$. As we will later see throughout this thesis this equality is particularly important for the development of many RL algorithms.


\section{Learning Value Functions}
The $V$ function and the $Q$ function play a crucial role when it comes to the development of optimal decision making algorithms, and over the years several methods have been introduced to learn them. While the ultimate goal of all these algorithms is that of yielding an optimal policy, there exists cases for which learning these value functions are easier than others. The complexity of learning a value function depends from how many components of the MDP are known to the agent. If the agent has access to all five of the components of the MDP that we introduced in Sec. \ref{sec:mdps}, these algorithms are part of a collection of methods that comes with the name of \textcolor{RoyalBlue}{Dynamic Programming} (DP). DP algorithms such as \textit{value-iteration}, \textit{policy-iteration} and variants \cite{bertsekas2015value,wei2015value} learn an optimal value function or optimal policy by exploiting the fact that the transition function $\matchcal{P}$, and the reward function $\Re$ of the MDP are known. While DP methods can be considered as the progenitors of many RL algorithms we will not discuss them here since throughout this thesis we will be interested in scenarios for which $\matchal{P}$ and $\Re$ are unknown. Specifically, we will introduce novel methods that aim to learn an optimal value function without requiring to learn an approximation of the transition and reward functions ($\widehat{\matchal{P}}$ and $\widehat{\mathcal{\Re}}$) neither, therefore placing all contributions of this dissertation within the \textcolor{RoyalBlue}{model-free} RL literature. 

\subsection{Monte Carlo Methods}
The first family of methods that is able of learning optimal value functions when no complete knowledge of the environment is available comes with the name of Monte Carlo (MC) methods. MC algorithms only require RL trajectories in order to discover an optimal policy and achieve this by sampling and averaging the rewards that are obtained while the agent is interacting with the environment. While MC methods can be used both for learning $V^{*}(s)$ and for learning $Q^{*}(s,a)$, in this section we only present how one can learn the state-value function. The key idea of MC algorithms relies on computing the actual sum of discounted rewards that an agent obtains once an episode finishes, this corresponds to computing the quantity defined in Eq. \ref{eq:goal}. Once this value is computed it can be used for updating the current value of each state with the following update rule: 
\begin{equation}
	V(s_t) := V(s_t) + \alpha \big[G_t - V(s_t) \big]
\label{eq:mc_update}
\end{equation}
where $\alpha \in [0,1]$ is the learning rate controlling how much we want to change the value estimate of a state based on $G_t$. As a practical example let us consider the MDP represented in Fig. \ref{fig:mdp}. Let us assume that the starting state of the environment is $s_0$ while the terminal state of the environment is $s_2$, and that the agent follows a policy $\pi$ that results into actions $a_1, a_2$ and $a_3$. The rewards associated to each action are: $-1, +2$ and $+3$ respectively. If we set the discount factor to $0.99$ we know that the real discounted return that is obtained at the end of the agent-environment interaction when starting in state $s_0$ is $\sum_{k=0}^{\infty}\gamma^{k} r_{t+k+1} = -1+\gamma2+\gamma^{2}3 \approx 3.92$. If we assume that the current value of $s_0$ before is $0$, and that we set $\alpha=0.5$, the result of one MC update for $s_0$ will be $\approx 1.96$.    

When dealing with MC learning it can however be possible that a certain state is visited more than once before a terminal state is reached. This is the case for $s_0$ for the MDP represented in Fig. \ref{fig:mdp}. If that happens one must decide when to update $V(s)$ and which value to use as $G_t$ since different state visits result into different $G_t$ values. There are two typical ways to deal with this, one can either update $V(s)$ only once, or one can update $V(s)$ each time the state is visited by simply using as $G_t$ the average of all the different discounted returns. The first update strategy comes with the name of , while the latter is denoted as .

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[auto,node distance=8mm,>=latex,font=\small]

    \tikzstyle{round}=[thick,draw=black,circle]

    \node[round] (s0) {$s_0$};
    \node[round,above right=5mm and 20mm of s0] (s1) {$s_1$};
    \node[round,below right=5mm and 20mm of s0] (s2) {$s_2$};

    \draw[->] (s0) -- node [text width=0.5cm,above] {a1} (s1);
    \draw[] (s0) -- node [text width=.5cm,below] {\textcolor{Maroon}{-1}} (s1);

    \draw[->] (s0) -- node [text width=0.5cm,above]{a3} (s2);
    \draw[] (s0) -- node [text width=.5cm,below] {\textcolor{Maroon}{+3}} (s2);

    \draw[->] (s1) -- node [text width=0.5cm,above]{a2} (s0);
    \draw[] (s1) -- node [text width=.5cm,below] {\textcolor{Maroon}{-2}} (s0);

\end{tikzpicture}
\caption{A visual representation of a simple MDP.}
\label{fig:mdp}
\end{figure}

While several successful applications of MC methods exist \cite{jaakkola1995reinforcement,liu1998sequential,lazaric2007reinforcement}, a well known issue from this family of algorithms is that they suffer from highly biased updates. In fact one needs to compute the sum presented in Eq. \ref{eq:goal} over all visited states, which can result into returns with considerable variance. It is easy to see how this can become an issue especially when the length of the episodes increases since the larger the length of the episode, the larger the variance of the updates will be. Furthermore, an additional drawback of MC methods is that one must wait until the agent visits a terminal state before being able to perform an update that is based on Eq. \ref{eq:mc_update}, the latter drawback can result into slow learning and is addressed by the methods which will be presented hereafter.   

\subsection{Temporal Difference Learning}
\label{sec:td_learning}
Temporal Difference (TD) Learning \cite{sutton1984temporal,sutton1988learning} is a learning paradigm that allows to overcome the aforementioned issues that characterize MC learning based methods. The key idea of TD-Learning is to update the value of each state with respect to a single MC update, therefore overcoming the hurdle of having to wait for the end of an episode before being able to update the value of a state. Just as MC methods TD-Learning algorithms also learn an optimal value function simply based on the experience that is collected by the agent, however these algorithms base their updates only on the value of a single consecutive state rather than on the real discounted return that is dependent from the entire sequence of visited states. Updating the value of a state with respect to the value of its successor state only is a technique which comes with the name of \textcolor{RoyalBlue}{bootstrapping}, and is a very effective design choice that reduces the variance in the updates. Bootstrapping can be used both for learning the $V$ function and the $Q$ function and is at the core of the most popular model-free RL algorithms. The fist and simplest form of TD-Learning was introduced by \citet{sutton1988learning} for learning the state-value function with an algorithm that updates the value of a state based on the following learning rule:
\begin{equation}
	V(s_t):= V(s_t) + \alpha \big[r_t + \gamma V(s_{t+1}) - V(s_t)\big].
	\label{eq:td_learning_v}
\end{equation}
We can now clearly see that differently from what happens in the MC update presented in Eq. \ref{eq:mc_update} the update of a state now only depends from the reward and the value of the next state. This quantity is denoted as the TD-error $\delta_t$ and is defined as:
\begin{equation}
	\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t).
\end{equation}
where $r_t + \gamma V(s_{t+1})$ is also known as the TD-target.
If we again consider the simple MDP represented in Fig. \ref{fig:mdp} and assume that the value of each state of the process is set to $0$ while the discount factor $\gamma$ is again set to $0.5$, a TD update for $V(s_0)$ based on action $a_3$ will result into the new value estimate of $1.5$. 
TD-Learning is a very effective strategy for building algorithms that can learn in an online, fully incremental fashion, since one only needs to wait a single time-step before being able to update the considered value function. Due to its striking simplicity TD-Learning has been widely adopted by RL practitioners developing algorithms for learning the $Q$ function. We will present some of the most important algorithms hereafter. 

\paragraph{Q-Learning:} introduced by \citet{watkins1992q} is arguably the most popular model-free RL algorithm. It works by keeping track of an estimate of the state-action value function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \Re$ and updates each visited state-action pair with the following update rule:
\begin{equation}
Q(s_t,a_t):=Q(s_t,a_t) + \alpha\big[r_t + \gamma \underset{a\in \mathcal{A}}{\max} Q(s_{t+1},a_t) - Q(s_t, a_t) \big].
\label{eq:q_learning}
\end{equation}
The key component of Q-Learning's update rule is the $\max$ operator which characterizes its TD-error and that is necessary for constructing the TD-target. Since there are as many Q values as there are actions available to the agent, one must choose which Q value to use as a reference when updating the value of the state-action pair that is currently being visited by the agent. The $\max$ operator simply chooses the state-action pair with the largest Q value, a simple design choice that has the appealing property of making Q-Learning converge to $Q^{*}(s,a)$ with probability 1 as long as all state-action pairs are visited infinitely often. Interestingly this guarantee holds even if a random policy is followed by the agent. The $\max$ operator also defines Q-Learning as an \textcolor{RoyalBlue}{off-policy} learning algorithm, since the Q values chosen for the construction of the TD-target might not correspond to the ones that are associated to the state that will be visited by the agent after having updated its $Q$ function.

\paragraph{SARSA:} also known as `online Q-Learning" \cite{rummery1994line} can be seen as the most straightforward extension of the TD-Learning method presented in Eq. \ref{eq:td_learning_v}, and similarly to Q-Learning is an algorithm that aims at learning the state-action value function $Q$. The key idea of SARSA is to update a state-action value with respect to the Q value that is associated to the state that will be visited by the agent after a certain action is performed. SARSA does therefore not use the $\max$ operator within its TD-error and constructs TD-targets that are representative of the policy that is being followed by the agent, a characteristic that defines SARSA as an \textcolor{RoyalBlue}{on-policy} RL algorithm. The way SARSA learns the $Q$ function is given by the following update rule
\begin{equation}
	Q(s_t,a_t):=Q(s_t,a_t) + \alpha\big[r_t + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t, a_t) \big], 
	\label{eq:sarsa}
\end{equation}
where we can clearly see how the algorithm uses all the elements of the quintuple of events $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$ a property that gives rise to the name $sarsa$. Not using the $\max$ operator in Eq. \ref{eq:sarsa} results into an algorithm that differently from Q-Learning does not directly learn the optimal $Q$ function anymore, but rather learns to estimate $Q^{\pi}(s,a)$. This has the drawback of not guaranteeing convergence to $Q^{*}(s,a)$ for any random policy anymore. To overcome this SARSA needs an exploration policy that is greedy in the limit of infinite exploration \cite{singh2000convergence}. This can be achieved with the popular $\epsilon-\text{greedy}$ selection policy which defines the action that is taken by the agent as:
\begin{equation}
a_t = \begin{cases}
\underset{a\in\mathcal{A}}{\argmax} \ Q(s_t,a) &\text{with probability $1-\epsilon$}\\
a \sim \mathcal{U}(\mathcal{A}) &\text{with probability $\epsilon$}
\end{cases}
\label{eq:e_greedy}
\end{equation}
where $\epsilon$ is a hyperparameter that changes while training progresses. During early training iterations its value is close to $1$, while it approaches $0$ by the end of training. This allows the agent to take actions that are representative of a large set of policies when the learned $Q$ function does not yet correspond to $Q^{*}(s,a)$, while it will favor greedy actions at the end of training. This is a simple, yet effective strategy to deal with the \textcolor{RoyalBlue}{exploration-exploitation} dilemma and it is worth noting that its use is not limited to on-policy RL algorithms only. Furthermore the method presented in Eq. \ref{eq:e_greedy} represents only one possible way of balancing exploration and exploitation, and although it is arguably the most popular of such methods it is not the only existing one. We refer the reader to chapter 5 of \cite{wiering1999explorations} for a thorough analysis of different exploration algorithms.


\paragraph{Double Q-Learning:}


\paragraph{QV($\lambda$)-Learning:} first introduced by \citet{wiering2005qv} and further developed by \citet{wiering2009qv} is an on-policy RL algorithm which differently from the previously introduced methods keeps track of an estimate of the state-value function $V:\mathcal{S}\rightarrow\Re$ alongside the usual estimate of the state-action value function $Q:\mathcal{S}\times\mathcal{A}\rightarrow\Re$. Since the goal is that of jointly learning two value functions, QV($\lambda$)-Learning requires two separate update rules. The $V$ function is learned via the same form of TD-Learning that we introduced in Eq. \ref{eq:td_learning_v}, with the only difference being the addition of the eligibility traces $e_t(s)$ at the end of the update rule (a RL technique that we will not discuss in this dissertation). QV($\lambda$)-Learning therefore learns the $V$ function with the following update rule:
\begin{equation}
V(s):= V(s) + \alpha \big[ r_{t} + \gamma V(s_{t+1}) - V(s_t) \big] e_{t}(s).
\label{eq:qv_lambda_v_update}
\end{equation}
Since as discussed earlier only learning the $V$ function is not sufficient for deriving an optimal policy one needs to learn the $Q$ function as well. In QV($\lambda$)-Learning this is done as follows:
\begin{equation}
Q(s_{t}, a_{t}):= Q(s_{t}, a_{t}) + \alpha \big[r_{t} + \gamma V(s_{t+1}) - Q(s_{t}, a_{t}) \big].
\label{eq:qv_lambda_q_update}
\end{equation}
An interesting property of the algorithm is that it uses the same TD-target ($r_t + \gamma V(s_{t+1})$) for defining the two different TD-errors that are required for learning the state-value and the state-action value functions. Among the main insights which motivate learning two value functions over one \citet{wiering2005qv} mentions the possibility that the $V$ function, since it does not depend from the actions that are taken by the agent, might converge faster than the $Q$ function. In fact as described earlier, the $V$ function only depends from the state space of the MDP which by definition is smaller than the state-action space. For a more in-depth and formal presentation of the conditions that show the benefits of jointly learning the $V$ function alongside the $Q$ function we refer the reader to chapter 5 of \cite{van2011insights}.


\section{Function Approximators}
If it is true that model-free RL algorithms are very powerful methods for learning an optimal policy when parts of the MDP are unknown, it is also true that all the aforementioned algorithms suffer from the \textcolor{RoyalBlue}{curse of dimensionality}. Model-free algorithms are typically implemented in a tabular fashion, meaning that the state values, or state-action values, are stored within a table of sizes $|\matchal{S}|$ and $|\mathcal{S}\times\mathcal{A}|$ respectively. Albeit straightforward and easy to implement such an approach presents severe limitations. The first major drawback of the tabular representation approach is that it does not scale well with respect to the complexity of the MDP. If the state and action spaces of the environment become very large, storing a table quickly becomes unfeasible in terms of storage space. Furthermore tabular representations are also unable to deal with states that are continuous. A natural solution to this problem consists in discretizing the state space, which unfortunately, when done thoroughly, results into the same aforementioned storage space issues. A common solution that makes it possible to use RL techniques even when the state space of the MDP is large is based on parametrized function approximation. In this context the goal is to not learn the exact value function anymore, but to rather replace its tabular representation with a parametrized function. The parameters of this function can then be adjusted based on the RL algorithms that we introduced in Sec. \ref{sec:td_learning}. 


\subsection{Linear Functions}
\label{sec:linear_functions}

The most straightforward type of function approximator one can use is a linear function. Given a state-action tuple that gets represented as a feature vector $\vec{x}=[f_1(s,a), f_2(s,a), ..., f_i(s,a)] \in \mathds{R}^2$, and a function parametrized by a vector of parameters $\theta$, as shown in \cite{wiering2004convergence} we can redefine the value of a state-action pair as:
\begin{equation}
	Q(s,a) = \sum_i \theta_{i,a} \vec{x}_i(s).
\end{equation}
Given a trajectory $\langle s_t,a_t,r_t,s_{t+1}\rangle$ the Q-Learning algorithm presented in Eq. \ref{eq:dqn} can now be used for updating the parameters $\theta$ for all $i$ with the following update rule:
\begin{equation}
	\theta_{i,a_t} := \theta_{i,a_t} + \alpha(r_t +\gamma\underset{a\in \mathcal{A}}{\max} Q(s_{t+1},a_t) - Q(s_t, a_t)\vec{x}_i(s_t).
	\label{eq:q_learning_fa}
\end{equation}
We can observe that this update rule is equivalent to updating the parameter vector $\theta$ for minimizing the mean squared error loss between a given state-action tuple and Q-Learning's TD-target since  
\begin{align}
	& \mathcal{L}(\theta) = \frac{1}{2}\big(y_t - Q(s_t, a_t)\big)^2 \\ 
	& \frac{\partial\mathcal{L}}{\partial \theta_{i,a_t}}=-\big(y_t - Q(s_t, a_t)\big)\vec{x}_i(s_t)  \\ 
 	& \theta_{i,a_t} := \theta_{i,a_t} + \alpha(r_t +\gamma\underset{a\in \mathcal{A}}{\max} Q(s_{t+1},a_t) - Q(s_t, a_t)\vec{x}_i(s_t).
\end{align}

Similar steps can be used for adapting all the RL algorithms that we introduced in the previous section. As a representative example for the on-policy learning case let us consider the SARSA algorithm, which can be used for minimizing the parameters of a linear function as:
\begin{equation}
	\theta_{i,a_t} := \theta_{i,a_t} + \alpha(r_t +\gamma Q(s_{t+1},a_{t+1}) - Q(s_t, a_t)\vec{x}_i(s_t).
\end{equation}
Among the different functions one can use there are CMACs \cite{lane1992theory}, radial basis functions (RBFs) \cite{park1993approximation}, linear neural networks \cite{mcculloch1943logical} and linear support vector machines (SVMs). Although these techniques can be successfully used for dealing with the aforementioned curse of dimensionality problem, \textcolor{RoyalBlue}{non-linear} functions are usually preferred since their representational power is much larger than the one of linear methods. Throughout this dissertation we are interested in non-linear functions that come in the form of deep neural networks, which despite being able of learning rich representations from their inputs, are also particularly challenging to train when it comes to RL problems. We will now describe how one can successfully deal with some of the challenges that characterize the use of deep neural networks in RL by presenting some of the most important algorithms that have been introduced over the years.    

\subsection{Deep Neural Networks}
Before looking into how RL algorithms should be integrated within deep neural networks, it is important to mention that RL techniques have been successfully used in combination with (less powerful) neural networks for over three decades. In fact, the field known as \textcolor{RoyalBlue}{Connectionist Reinforcement Learning} (CRL) gave born to the very first algorithms that managed to outperform human experts on certain tasks. Among the possible multiple examples of this family of techniques we mention the TD-Gammon program introduced by \citet{tesauro1994td}. TD-Gammon successfully learns an approximation of the evaluation function of the popular Backgammon board-game through the same TD-Learning methods that we presented in Sec. \ref{sec:td_learning} and achieved a level of play which was comparable to the one of the top human Backgammon players of its time. For a more detailed presentation about the successful applications of CRL algorithms we refer the reader to \cite{bucsoniu2011approximate}.  

While certainly successful for a certain set of problems (see for example chapter $1$ of \cite{sabatelli2017learning}), CRL techniques also present severe limitations. Since they only use multi-layer perceptrons as function approximators, these algorithms cannot be used for tackling problems where the state representation of the MDP is highly dimensional. To overcome this, more complicated and powerful networks are required. \textcolor{RoyalBlue}{Deep Reinforcement Learning} (DRL) \cite{arulkumaran2017deep, li2017deep, franccois2018introduction} is a research field that combines RL algorithms with deeper and more complex neural architectures. In value based model-free DRL we are interested in learning an approximation of an optimal value function with a deep neural network that comes with parameters $\theta$

\noindent
\begin{tabularx}{\linewidth}{@{}XX@{}}
\begin{equation}
	  V(s;\theta)\approx V^{*}(s)
	  \label{eq:v_approx}
  \end{equation}
&
\begin{equation}  
	Q(s,a;\theta)\approx Q^{*}(s,a)
	\label{eq:q_approx}
  \end{equation}
\end{tabularx}
and that usually comes in the form of a convolutional neural network. 

\paragraph{Deep Q-Learning (DQN):} just like Q-Learning is arguably the most important tabular model-free RL algorithm, so is DQN when it comes to DRL. First introduced by \citet{mnih2013playing} and then made popular by the work presented in \cite{mnih2015human} this algorithm can certainly be considered as the very first successful example of a neural network that is able to learn an approximation of the optimal state-action value function just from high sensory inputs such as images. As the name suggests, DQN is based upon the Q-Learning algorithm, and aims at learning an approximation of the optimal state-action value function $Q$. This is done by reshaping Q-Learning's update rule, presented in Eq. \ref{eq:q_learning}, into a differentiable loss function that can be used for training a convolutional network with the following objective function:
\begin{multline}
	\mathcal{L}(\theta) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}) \\ - Q(s_{t}, a_{t}; \theta)\big)^{2}\bigg].
\label{eq:dqn}
\end{multline}

We can start by observing that the general principles that characterize the algorithm are the same ones which made it possible to generalize Q-Learning to the use of linear function approximators, although it is worth noting that differently from when a linear function is used the mapping between input and feature spaces is now not preserved anymore. Similarly to what we presented in Sec. \ref{sec:linear_functions}, we can see from Eq. \ref{eq:dqn} that learning $Q(s,a,\theta)$ is again achieved by minimizing the squared error loss between the $Q(s_t,a_t;\theta)$ estimates and the off-policy TD-target
\begin{equation}
    y^{DQN}_{t} = r_{t} + \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}).
\label{eq:dqn_td}
\end{equation}
Despite this similarity DQN requires some additional algorithmic design choices, without which, it would result impossible to successfully train a neural network with Eq. \ref{eq:dqn}. These additions, which significantly make DQN differ from the algorithm presented in Eq. \ref{eq:q_learning_fa} are the following:
\begin{itemize}
	\item Experience Replay: a memory buffer, $D$, represented as a queue which stores RL trajectories of the form $\langle s_{t}$, $a_{t}$, $r_{t}$, $s_{t+1} \rangle$. Once this memory buffer is filled with a large set of these quadruples, DQN uniformly samples batches of trajectories for training its network. This makes it possible to exploit past trajectories multiple times by reusing them while training, which makes the overall algorithm more sample efficient. Furthermore, using a memory buffer also improves the stability of the training procedure. Recall that each trajectory is representative of a certain episode, by repeatedly randomly sampling a different $\tau$ from the memory buffer, a resulting mini-batch of trajectories will be representative of different episodes and of different policies. As a consequence the correlation between trajectories within a mini-batch will be small. Although made popular by the DQN algorithm, the use of an experience replay buffer for tackling optimal decision making problems was already presented by \citet{lin1992self}.
		
	\item Target Network: We can observe from Eq. \ref{eq:dqn_td} that the TD-target used by DQN for bootstrapping is not computed by the $Q$ network that is being optimized ($\theta$), but rather from a second separate network that is parameterized with $\theta^{-}$. This second network has the same structure as the main $Q$ network, but its weights do not change each time RL experiences are sampled from $D$. On the contrary, its weights are temporally frozen, and only periodically get updated with the parameters of the main network $\theta$ as defined by an appropriate hyperparameter. Note that this is a design choice that is not motivated by the TD-Learning paradigm that we presented in Sec. \ref{sec:td_learning}, where we have seen that TD-Learning based methods learn in a fully online fashion by updating their value estimates based on their own future estimates. With a target-network, although the $\theta$ network still learns via the methods of temporal differences, it now requires an auxiliary, external model if it wants to successfully learn $\approx Q^{*}(s,a;\theta)$. Several research has studied the role of the target network with the aim of understanding why this design choice appears to be necessary for DRL, yet the role of $\theta^{-}$ is not fully understood by the DRL community. For more about this topic we refer the reader to .

\end{itemize}

Now that all components of Eq. \ref{eq:dqn} are introduced we can show that given a training iteration $i$, differentiating this objective function with respect to $\theta$ gives the following gradient: 
\begin{multline}
\nabla_{\theta_{i}}y^{DQN}_{t}(\theta_{i}) = \mathds{E}_{\langle s_{t},a_{t},r_{t},s_{t+1}\rangle\sim U(D)} \bigg[\big(r_{t} + \\ \gamma \: \underset{a\in \mathcal{A}}{\max}\: Q(s_{t+1}, a; \theta^{-}_{i-1})  - Q(s_{t}, a_{t}; \theta_{i})\big)\nabla_{\theta_{i}} Q(s_{t}, a_{t}; \theta_{i})\bigg].
\label{eq:dqn_gradient}
\end{multline}

The DQN algorithm showed its potential in \cite{mnih2015human} where it was shown how the same neural network, trained with Eq. \ref{eq:dqn}, could learn how to successfully play most of the \texttt{Atari} games that were part of the popular Atari Arcade Learning Environment (ALE) \cite{bellemare2013arcade}. The goal of the ALE platform is to provide a benchmark for testing the performance of DRL algorithms. This is done by measuring how well a DRL algorithm learns to play $57$ different emulations of \texttt{Atari} games that are specifically designed within a simulator. See Fig. \ref{fig:atari_games} for a visualization of some of the games that are part of the ALE suite. Remarkably DQN learned how to play most of the games of the platform by only using as input for the network the images representing an \texttt{Atari} game, therefore learning just from its own experience in a pure model-free RL fashion. Since then, DQN has been successfully used for a large variety of applications ranging from, to and .
However, despite all these remarkable applications the algorithm still comes with some drawbacks, some of which were solved by the follow-up algorithms presented below.

\begin{figure}[ht!]
\centering
  \includegraphics[width=10cm]{./Images/Chapter02/atari_games}
  \caption{A visual representation of some of the \texttt{Atari} games that are part of the Arcade Learning Environment (ALE) \cite{bellemare2013arcade}. From left to right \texttt{Breakout, Seaquest, Pong, MsPacman, Qbert, Kungfu-Master, Enduro} and \texttt{Space Invaders}. Most of these games will be of interest and used in chapters . Image courtesy of .}
  \label{fig:atari_games}
\end{figure}


\paragraph{Double Deep Q-Learning (DDQN)}
\paragraph{Prioritized Experience Replay}
\paragraph{Dueling Networks}
\paragraph{Rainbow}


\section{Deep Reinforcement Learning Challenges}
non stationarity


\begin{remark}{Conclusion}

\end{remark}



