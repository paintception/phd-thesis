% Table of Contents - List of Tables/Figures/Listings and Acronyms

\refstepcounter{dummy}

\pdfbookmark[1]{\contentsname}{tableofcontents} % Bookmark name visible in a PDF viewer

\setcounter{tocdepth}{2} % Depth of sections to include in the table of contents - currently up to subsections

\setcounter{secnumdepth}{3} % Depth of sections to number in the text itself - currently up to subsubsections

\manualmark
\markboth{\spacedlowsmallcaps{\contentsname}}{\spacedlowsmallcaps{\contentsname}}
\tableofcontents 
\automark[section]{chapter}
\renewcommand{\chaptermark}[1]{\markboth{\spacedlowsmallcaps{#1}}{\spacedlowsmallcaps{#1}}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\enspace\spacedlowsmallcaps{#1}}}

\clearpage

\begingroup 
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

%----------------------------------------------------------------------------------------
%	List of Figures
%----------------------------------------------------------------------------------------

\refstepcounter{dummy}
%\addcontentsline{toc}{chapter}{\listfigurename} % Uncomment if you would like the list of figures to appear in the table of contents
\pdfbookmark[1]{\listfigurename}{lof} % Bookmark name visible in a PDF viewer

%\listoffigures

\vspace{8ex}
\newpage

%----------------------------------------------------------------------------------------
%	List of Tables
%----------------------------------------------------------------------------------------

\refstepcounter{dummy}
%\addcontentsline{toc}{chapter}{\listtablename} % Uncomment if you would like the list of tables to appear in the table of contents
\pdfbookmark[1]{\listtablename}{lot} % Bookmark name visible in a PDF viewer

%\listoftables
        
\vspace{8ex}
\newpage
    
%----------------------------------------------------------------------------------------
%	List of Listings
%---------------------------------------------------------------------------------------- 

			       
%----------------------------------------------------------------------------------------
%	Acronyms
%----------------------------------------------------------------------------------------

\refstepcounter{dummy}
%\addcontentsline{toc}{chapter}{Acronyms} % Uncomment if you would like the acronyms to appear in the table of contents
\pdfbookmark[1]{Acronyms}{acronyms} % Bookmark name visible in a PDF viewer

\markboth{\spacedlowsmallcaps{Acronyms}}{\spacedlowsmallcaps{Acronyms}}

\chapter*{Symbols and Notation}

\begin{tabularx}{\textwidth}{ l X }

 $\mathcal{X}$ & input space \\
 $\mathcal{Y}$ & output space \\
 $P(X,Y)$ & joint probability distribution \\ 
 $\mathcal{F}$ & set of all possible functions \\ 
 $\ell$ & loss \\
 $\mathcal{L}$ & learning set \\ 
 $R(f)$ & expected risk \\ 
 $\hat{R}$ & empirical risk \\ 
 $f^{*}$ & optimal function \\ 
 $R_B$ & Bayes risk \\ 
 $f_B$ & Bayes model \\ 
 $\sigma$ & sigmoid function \\ 
 $\mathbf{w}$ & weight vector \\ 
 $\mathbf{W}$ & weight matrix \\ 
 $\theta$ & neural network parameters \\ 
 $\mathscr{L}$ & loss function \\ 
 $\eta$ & learning rate \\  
 $\rho$ & momentum \\ 
 $\circledast$ & cross convolution operator \\ 
 $\vec{o}$ & feature map \\
$\mathcal{M}$ & Markov Decision Process \\ 
$\mathcal{S}$ & state space \\ 
$\mathcal{A}$ & action space \\
$\mathcal{P}$ & transition function \\ 
$t$ & time-step \\
$\Re$ & reward function \\ 
$\gamma$ & discount factor \\ 
$\pi$ & policy \\ 
$\tau = \langle s_t,a_t,r_t,s_{t+1}\rangle$ & trajectory \\ 
$G$ & goal \\ 
$V^{\pi}(s)$ & state-value function \\
$Q^{\pi}(s,a)$ & state-action value function \\ 
$A^{\pi}(s,a)$ & advantage function \\
$\pi^{*}$ & optimal policy \\ 
$V^{*}(s)$ & optimal state-value function \\
$Q^{*}(s,a)$ & optimal state-action value function \\
	
\end{tabularx}
\clearpage


\begin{tabularx}{\textwidth}{ l X }
	$\delta_t$ & temporal-difference error \\ 
$y_t$ & temporal-difference target \\ 
$\epsilon$ & epislon-greedy exploration parameter \\
$e(t)$ & eligibility traces for state $s$ \\
$D$ & experience replay buffer \\
$h(s,a;\theta)$ & generic function approximator \\ 	
	$\mathcal{D}$ & domain \\ 
	$\mathcal{T}$ & task \\ 
	$\mathcal{D}_S$ & source domain \\ 
	$\mathcal{D}_T$ & target domain \\ 
	$\mathcal{T}_S$ & source task \\
	$\mathcal{T}_T$ & target task \\
	$f$ & decison function \\ 
	$f_T(\cdot)$ & target predictive function \\
	$\mathcal{K}$ & knowledge \\ 
	$\mathcal{K}_S$ & source knowledge \\
	$\mathcal{K}_T$ & target knowledge \\
	$\theta_S$ & source neural network parameters \\ 
	$\theta_T$ & target neural network parameters \\
	$\mathcal{X}_S$ & source input space \\
	$\mathcal{X}_T$ & target input space \\ 
	$\mathcal{Y}_S$ & source output space \\
	$\mathcal{Y}_T$ & target output space \\
	$N_T$ & number of samples in target dataset \\ 
	$Q_T$ & number of classes in target dataset \\	
	$\mathscr{X}$ & feature vector \\
	$\theta_i$ & ImageNet pre-trained parameters \\ 
	$\theta_r$ & Rijksmuseum pre-trained parameters \\
	$I_t$ & number of instruments in MINERVA \\ 
	$m$ & mask \\
	$k$ & late resetting epochs \\
	$f(x;m\odot\theta_0)$ & winning ticket \\
	$f(x;m\odot\theta_r)$ & random ticket \\ 
	$f(x;m\odot\theta_k)$ & winning ticket obtained through late resetting \\
	$\mathbf{T}$ & tensor \\ 
		
\end{tabularx}
\clearpage


\begin{tabularx}{\textwidth}{ l X }
	$\mathscr{D}$ & He-Uniform weight distribution \\
	$V(s;\Phi)$ & state-value network \\
	$V(s;\Phi^{-})$ & state-value target network \\ 
	$Q(s,a;\theta)$ & state-action value network \\ 
	$Q(s,a;\theta^{-})$ & state-action value target network \\
	$N$ & capacity of the memory buffer \\
	$\mathcal{M}_S$ & source Markov Decision Process \\ 
	$\mathcal{M}_T$ & target Markov Decision Process \\
	$\mathscr{R}$ & area ratio score \\
	
\end{tabularx}


\endgroup
