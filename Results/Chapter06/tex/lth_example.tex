\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}[scale = 0.5]

\begin{axis}[
	grid style={dashed,gray},
	grid = both, 
	tick style=black,
	x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
  	xlabel=Fraction of Weights Pruned,
  	ylabel= Accuracy ($\%$),
	title=MNIST,
	%width=1,
	xtick=data,
	%xticklabels from table={../logs/mnist_example.txt}{pruning_stages},
	label style={font=\scriptsize},
	xticklabels = {0.0,0.2,0.36,0.488,0.59,0.672,0.738,0.79,0.832,0.866,0.893,0.914,0.931,0.945,0.956,0.965,0.972,0.977,0.982,0.986,0.988,0.991,0.993,0.994,0.995,0.996,0.997,0.998,0.998,0.998,0.999},
	x=4.5mm,
	ymin=85,
    	ymax=99,
	scale only axis,
	xticklabel style={rotate=90},
        %log ticks with fixed point,
        scaled ticks=true,
	/pgf/number format/fixed,
        %log ticks with fixed point,
  	legend pos=outer north east,
]

	\addlegendentry{Baseline}
	\addlegendentry{Winning Ticket $f(x;m\odot\theta_0)$}
	\addlegendentry{Random Ticket $f(x;m\odot\theta_r)$}

\addplot [thick, black, mark=x] table [x expr=\coordindex, y=baseline]{./Results/Chapter06/logs/mnist_example.txt};

\addplot [thick, blue, mark=x] table [x expr=\coordindex, y=ticket]{./Results/Chapter06/logs/mnist_example.txt};
%\addplot [name path=upper,draw=none] table[x expr=\coordindex,y expr=\thisrow{ticket}+\thisrow{std-ticket}] {./Results/Chapter06/logs/mnist_example.txt};
%\addplot [name path=lower,draw=none] table[x expr=\coordindex,y expr=\thisrow{ticket}-\thisrow{std-ticket}] {./Results/Chapter06/logs/mnist_example.txt};
%\addplot [fill=blue!10] fill between[of=upper and lower];

\addplot [thick, orange, mark=x] table [x expr=\coordindex, y=random_weights]{./Results/Chapter06/logs/mnist_example.txt};

\end{axis}
    \end{tikzpicture}
  \begin{tikzpicture}[scale = 0.5]

\begin{axis}[
	grid style={dashed,gray},
	grid = both, 
	tick style=black,
	x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
  	xlabel=Fraction of Weights Pruned,
  	ylabel= Accuracy ($\%$),
	title=CIFAR-10,
	%width=1,
	xtick=data,
	label style={font=\scriptsize},
	xticklabels = {0.0,0.2,0.36,0.488,0.59,0.672,0.738,0.79,0.832,0.866,0.893,0.914,0.931,0.945,0.956,0.965,0.972,0.977,0.982,0.986,0.988,0.991,0.993,0.994,0.995,0.996,0.997,0.998,0.998,0.998,0.999},
	x=4.5mm,
	ymin=0,
    	ymax=90,
	scale only axis,
	xticklabel style={rotate=90},
        %log ticks with fixed point,
        scaled ticks=true,
	/pgf/number format/fixed,
        %log ticks with fixed point,
  	legend pos=outer north east,
]

	\addlegendentry{Baseline}
	\addlegendentry{Winning Ticket $f(x;m\odot\theta_k)$}
	\addlegendentry{Random Mask + Random $\theta$}

\addplot [thick, black, mark=x] table [x expr=\coordindex, y=baseline]{./Results/Chapter06/logs/cifar10_example.txt};
\addplot [thick, blue, mark=x] table [x expr=\coordindex, y=ticket]{./Results/Chapter06/logs/cifar10_example.txt};
\addplot [thick, red, mark=x] table [x expr=\coordindex, y=random_mask]{./Results/Chapter06/logs/cifar10_example.txt};

\end{axis}
    \end{tikzpicture}


    \caption{A visual representation of the performance of lottery winners that replicate the findings first presented by \citet{frankle2018lottery}. In the first plot we consider a multilayer perceptron that gets trained on the MNIST dataset. After the network gets trained from scratch it obtains a final accuracy of $\approx 97\%$ as reported by the black line. We can observe that winning tickets $f(x;m\odot\theta_0)$ only start performing worse than the network they have been extracted from once a large fraction of their weights gets pruned. We can also observe how crucial it is to re-initialize the weights of the pruned models with the same weights that were used when initializing the unpruned model from scratch ($\theta_0$). If random weights are used instead ($\theta_r$), the pruned masks appear to be less robust to pruning (orange curve). In the second plot we show how important it is for a pruned model to come in the form of $f(x;\odot\theta_k)$ after training a ResNet-50 architecture on the CIFAR-10 dataset, since we show that it is not possible to simply extract any random subset of weights from a deep convolutional network and obtain a performance that is robust to pruning after randomly initializing the parameters of the model (red curve).}
    \label{fig:original_lth_results}
\end{figure} 
