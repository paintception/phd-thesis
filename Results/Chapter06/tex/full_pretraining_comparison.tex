\begin{figure}[ht!]
\centering
	\begin{tikzpicture}[scale = 0.45]

\begin{axis}[
	name=ax1,
	grid style={dashed,gray},
	grid = both, 
	tick style=black,
  	x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
	xlabel=Fraction of Weights Pruned,
  	ylabel= Accuracy ($\%$),
	title=Human-LBA,
	%width=1,
	xtick=data,
	label style={font=\scriptsize},
	xticklabels = {0.0,0.2,0.36,0.488,0.59,0.672,0.738,0.79,0.832,0.866,0.893,0.914,0.931,0.945,0.956,0.965,0.972,0.977,0.982,0.986,0.988,0.991,0.993,0.994,0.995,0.996,0.997,0.998,0.998,0.998,0.999},
	x=4.5mm,
	ymin=40,
    	ymax=90,
	scale only axis,
	xticklabel style={rotate=90},
        %log ticks with fixed point,
        scaled ticks=true,
	/pgf/number format/fixed,
        %log ticks with fixed point,
  	legend pos= north east,
	%legend style={font=\small, at={(-0.8,-0.2,-0.2)},anchor=north west, legend columns = 1}
	]

	\addlegendentry{Baseline}
	\addlegendentry{Winning Ticket $f(x;m\odot\theta_k)$}
	\addlegendentry{CIFAR-10 $f(x;m\odot\theta_i)$}
	
\addplot [ultra thick, black, mark=x] table [x expr=\coordindex, y=baseline]{./Results/Chapter06/logs/human_lba_fine_tuning_study.txt};
\addplot [ultra thick, green, mark=x] table [x expr=\coordindex, y=winning_ticket]{./Results/Chapter06/logs/human_lba_fine_tuning_study.txt};
\addplot [ultra thick, green, mark=*] table [x expr=\coordindex, y=fine_tuned_pruned_model]{./Results/Chapter06/logs/human_lba_fine_tuning_study.txt};

\end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale = 0.45]
\begin{axis}[
	at={(ax1.south east)},
	xshift=2cm,
	grid style={dashed,gray},
	grid = both, 
	tick style=black,
	x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
  	xlabel=Fraction of Weights Pruned,
  	ylabel= Accuracy ($\%$),
	title=Human-LBA,
	%width=1,
	xtick=data,
	label style={font=\scriptsize},
	xticklabels = {0.0,0.2,0.36,0.488,0.59,0.672,0.738,0.79,0.832,0.866,0.893,0.914,0.931,0.945,0.956,0.965,0.972,0.977,0.982,0.986,0.988,0.991,0.993,0.994,0.995,0.996,0.997,0.998,0.998,0.998,0.999},
	x=4.5mm,
	ymin=40,
    	ymax=90,
	scale only axis,
	xticklabel style={rotate=90},
        %log ticks with fixed point,
        scaled ticks=true,
	/pgf/number format/fixed,
        %log ticks with fixed point,
  	legend p= north east,
	%legend style={font=\Large, at={(-0.8,-0.2,-0.2)},anchor=north west, legend columns = 2}
	]

	\addlegendentry{Baseline}
	\addlegendentry{Winning Ticket $f(x;m\odot\theta_k)$}
	\addlegendentry{CIFAR-100 $f(x;m\odot\theta_i)$}
	
\addplot [ultra thick, black, mark=x] table [x expr=\coordindex, y=baseline]{./Results/Chapter06/logs/human_lba_fine_tuning_study_cif.txt};
\addplot [ultra thick, red, mark=x] table [x expr=\coordindex, y=winning_ticket]{./Results/Chapter06/logs/human_lba_fine_tuning_study_cif.txt};
	\addplot [ultra thick, red, mark=*] table [x expr=\coordindex, y=fine_tuned_pruned_model]{./Results/Chapter06/logs/human_lba_fine_tuning_study_cif.txt};

\end{axis} 


    \end{tikzpicture}


    \caption{Our results showing the advantages of transferring lottery winners over pruned models that are fully fine-tuned on natural datasets (CIFAR-10/100). We can observe that their performance is overall inferior to the one of lottery tickets and that these models are significantly less robust to pruning. We believe that the reason behind their poor performance revolves around the fact that, once completely trained on a specific source task, and after having gone through the pruning stage, these models lose the necessary flexibility that is required for them to adapt to a new task.}
    \label{fig:full_fine_tuning_comparison}
\end{figure} 
