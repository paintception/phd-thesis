\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[scale = 0.8]

\begin{axis}[
	grid style={dashed,gray},
	grid = both, 
	tick style=black,
  	xlabel=Epochs,
  	ylabel= Accuracy ($\%$),
	title=Material Classification,
	%width=1,
    	xmin=0,
    	xmax=25,
    	ymin=0.80,
    	ymax=0.95,
  	legend pos=outer north east,
]

      \addlegendentry{Xception $\theta$}
      \addlegendentry{ResNet50 $\theta$}
      \addlegendentry{InceptionV3 $\theta$}
      \addlegendentry{VGG19 $\theta$}
      \addlegendentry{Scratch-V3 $\theta_{r}$}
      \addlegendentry{Xception $\theta^{-}$}
      \addlegendentry{ResNet50 $\theta^{-}$}
      \addlegendentry{InceptionV3 $\theta^{-}$}
      \addlegendentry{VGG19 $\theta^{-}$}


\addplot [thick, blue, mark=x] table [y=Xception, x=epochs]
{./Results/Chapter03/logs/res_1.txt};
\addplot [thick, red, mark=x] table [y=ResNet, x=epochs]{./Results/Chapter03/logs/res_1.txt};
\addplot [thick, black, mark=x] table [y=V3, x=epochs]{./Results/Chapter03/logs/res_1.txt};
\addplot [thick, green, mark=x] table [y=VGG19, x=epochs]{./Results/Chapter03/logs/res_1.txt};
\addplot [ultra thick, orange , solid] table [y=RandomV3, x=epochs]{./Results/Chapter03/logs/res_1.txt};


\addplot [ thick, blue, mark=halfcircle] table [y=Xception, x=epochs]{./Results/Chapter03/logs/res_2.txt};
\addplot [ thick, red, mark=halfcircle] table [y=ResNet, x=epochs]{./Results/Chapter03/logs/res_2.txt};
\addplot [thick, black, mark=halfcircle] table [y=V3, x=epochs]{./Results/Chapter03/logs/res_2.txt};
\addplot [thick, green, mark=halfcircle] table [y=VGG19, x=epochs]{./Results/Chapter03/logs/res_2.txt};



\end{axis}
    \end{tikzpicture}
    \caption{Comparison between the fine tuning approach versus the off the shelf one when classifying the material of the heritage objects of the Rijksmuseum dataset. We observe how the first approach (as reported by the the dashed lines) leads to significant improvements when compared to the latter one (reported by the dash-dotted lines) for three out of four neural architectures. Furthermore, we can also observe how training a DCNN from scratch leads to worse results when compared to fine-tuned architectures which have been pre-trained on ImageNet (solid orange line).}
    \label{fig:rijks_material}
\end{figure} 
